Codebase Export - 2025-04-07 12:45:31

================================================================================
FILE: .pytest_cache/README.md
LANGUAGE: markdown
SIZE: 310 bytes
================================================================================

# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.


================================================================================
FILE: .vscode/settings.json
LANGUAGE: json
SIZE: 153 bytes
================================================================================

{
    "python.testing.pytestArgs": [
        "tests"
    ],
    "python.testing.unittestEnabled": false,
    "python.testing.pytestEnabled": true
}

================================================================================
FILE: README.md
LANGUAGE: markdown
SIZE: 1578 bytes
================================================================================

# othertales unified openapi tools server
A comprehensive server that provides document storage, memory, Git versioning, and web scraping capabilities for LLMs via OpenWebUI.
## Features
- **Document Management**: Store and retrieve documents with Git versioning
- **Knowledge Graph**: Store structured data and track user preferences
- **Git Integration**: Version control for documents
- **Web Scraping**: Extract content from websites and convert to Markdown
- **OpenWebUI Integration**: Fully compatible with OpenWebUI
## Getting Started
### Installation
#### Using Docker
```bash
docker build -t unified-tools-server .
docker run -p 8000:8000 -v $(pwd)/data:/app/data unified-tools-server
```
#### Without Docker
```bash
# Install dependencies
pip install -r requirements.txt
# Install Playwright browsers
playwright install chromium
# Run the server
python main.py
```
### Configuration
Edit the `.env` file to configure the server:
```
# Server settings
SERVER_HOST=0.0.0.0
SERVER_PORT=8000
DEV_MODE=False
# Storage settings
ALLOWED_DIRS=./data,~/documents
MEMORY_FILE_PATH=./data/memory.json
# Git settings
DEFAULT_COMMIT_USERNAME=UnifiedTools
DEFAULT_COMMIT_EMAIL=tools@example.com
```
## API Documentation
When running, documentation is available at:
- OpenAPI JSON: http://localhost:8000/openapi.json
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc
## Usage with OpenWebUI
In OpenWebUI, add a new Tool with the following URL:
```
http://localhost:8000/openapi.json
```
## License
SEE EULA

================================================================================
FILE: app/__init__.py
LANGUAGE: python
SIZE: 60 bytes
================================================================================

"""
othertales tools server
"""

__version__ = "1.0.1"


================================================================================
FILE: app/api/__init__.py
LANGUAGE: python
SIZE: 52 bytes
================================================================================

"""
API Routers for the Unified Tools Server
"""


================================================================================
FILE: app/api/documents.py
LANGUAGE: python
SIZE: 21224 bytes
================================================================================

import asyncio
import hashlib
import json
import os
import tempfile
import time
import random
import uuid
import logging
import threading
from typing import List, Optional, Dict, Any, Union, Set, Tuple
from pathlib import Path
from urllib.parse import urljoin, urlparse
import urllib.robotparser as robotparser
import requests
import git
from git import Repo
from fastapi import APIRouter, Body, Query, HTTPException, Path, UploadFile, File, Form, Response
from pydantic import BaseModel, Field
from app.models.documents import (
    DocumentType,
    CreateDocumentRequest,
    UpdateDocumentRequest,
    DocumentResponse,
    DocumentVersionResponse,
    DocumentContentResponse,
)
from app.core.documents_service import DocumentsService
from app.utils.config import get_config

# Set up logger
logger = logging.getLogger(__name__)
# Create router
router = APIRouter()


class GitService:
    def __init__(self):
        config = get_config()
        self.default_username = config.default_git_username
        self.default_email = config.default_git_email
        self.temp_auth_files = {}
        self.repo_locks = {}

    def _get_repo(self, repo_path: str) -> git.Repo:
        """Get git repository object"""
        try:
            repo = Repo(repo_path)
            return repo
        except git.exc.InvalidGitRepositoryError:
            raise ValueError(f"Invalid Git repository at '{repo_path}'")
        except Exception as e:
            raise ValueError(f"Failed to get repository: {str(e)}")

    def _get_repo_lock(self, repo_path: str) -> threading.Lock:
        """Get a lock for a specific repository to prevent concurrent modifications"""
        if repo_path not in self.repo_locks:
            self.repo_locks[repo_path] = threading.Lock()
        return self.repo_locks[repo_path]

    def get_status(self, repo_path: str) -> Dict[str, Any]:
        """Get the status of a Git repository"""
        repo = self._get_repo(repo_path)
        current_branch = repo.active_branch.name
        # Get staged files
        staged_files = [item.a_path for item in repo.index.diff("HEAD")]
        # Get modified but unstaged files
        unstaged_files = [item.a_path for item in repo.index.diff(None)]
        # Get untracked files
        untracked_files = repo.untracked_files
        return {
            "clean": not (staged_files or unstaged_files or untracked_files),
            "current_branch": current_branch,
            "staged_files": staged_files,
            "unstaged_files": unstaged_files,
            "untracked_files": untracked_files,
        }

    def get_diff(
        self, repo_path: str, file_path: Optional[str] = None, target: Optional[str] = None
    ) -> str:
        """Get diff of changes"""
        repo = self._get_repo(repo_path)
        if file_path and target:
            return repo.git.diff(target, file_path)
        elif file_path:
            return repo.git.diff("HEAD", file_path)
        elif target:
            return repo.git.diff(target)
        else:
            return repo.git.diff()

    def add_files(self, repo_path: str, files: List[str]) -> str:
        """Stage files for commit"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            repo.git.add(files)
            return "Files staged successfully"

    def commit_changes(
        self,
        repo_path: str,
        message: str,
        author_name: Optional[str] = None,
        author_email: Optional[str] = None,
    ) -> str:
        """Commit staged changes"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            author_name = author_name or self.default_username
            author_email = author_email or self.default_email
            # Set author for this commit
            with repo.config_writer() as config:
                config.set_value("user", "name", author_name)
                config.set_value("user", "email", author_email)
            # Commit changes
            commit = repo.index.commit(message)
            return f"Committed changes with hash {commit.hexsha}"

    def reset_changes(self, repo_path: str) -> str:
        """Reset staged changes"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            repo.git.reset()
            return "All staged changes reset"

    def get_log(
        self, repo_path: str, max_count: int = 10, file_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """Get log of commits"""
        repo = self._get_repo(repo_path)
        if file_path:
            commits = list(repo.iter_commits(paths=file_path, max_count=max_count))
        else:
            commits = list(repo.iter_commits(max_count=max_count))
        log_data = []
        for commit in commits:
            log_data.append(
                {
                    "hash": commit.hexsha,
                    "author": f"{commit.author.name} <{commit.author.email}>",
                    "date": commit.committed_datetime.strftime("%Y-%m-%d %H:%M:%S %z"),
                    "message": commit.message.strip(),
                }
            )
        return log_data

    def create_branch(
        self, repo_path: str, branch_name: str, base_branch: Optional[str] = None
    ) -> str:
        """Create a new branch"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            if base_branch:
                repo.git.checkout(base_branch)
            repo.git.checkout("-b", branch_name)
            return f"Created branch '{branch_name}'"

    def checkout_branch(self, repo_path: str, branch_name: str, create: bool = False) -> str:
        """Checkout an existing branch or create a new one"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            if create:
                if branch_name not in repo.refs:
                    repo.git.checkout("-b", branch_name)
                else:
                    raise ValueError(f"Branch '{branch_name}' already exists")
            else:
                repo.git.checkout(branch_name)
            return f"Checked out branch '{branch_name}'"

    def clone_repo(self, repo_url: str, local_path: str, auth_token: Optional[str] = None) -> str:
        """Clone a Git repository"""
        try:
            if auth_token:
                if repo_url.startswith("https://"):
                    repo_url = repo_url.replace("https://", f"https://x-access-token:{auth_token}@")
                Repo.clone_from(repo_url, local_path)
            else:
                Repo.clone_from(repo_url, local_path)
            return f"Cloned repository to '{local_path}'"
        except Exception as e:
            raise ValueError(f"Failed to clone repository: {str(e)}")

    def remove_file(self, repo_path: str, file_path: str) -> str:
        """Remove a file from the repository"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            try:
                repo.index.remove([file_path])
                repo.index.commit(f"Removed {file_path}")
                return f"Successfully removed {file_path} from Git"
            except Exception as e:
                raise ValueError(f"Failed to remove file: {str(e)}")

    def get_file_content(self, repo_path: str, file_path: str, version: str) -> str:
        """Get the content of a file at a specific Git version"""
        repo = self._get_repo(repo_path)
        try:
            blob = repo.git.show(f"{version}:{file_path}")
            return blob
        except Exception as e:
            raise ValueError(f"Failed to get file content at version {version}: {str(e)}")

    def configure_lfs(self, repo_path: str, file_patterns: List[str]) -> str:
        """Configure Git LFS for the repository"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            try:
                repo.git.execute(["git", "lfs", "install"])
                for pattern in file_patterns:
                    repo.git.execute(["git", "lfs", "track", pattern])
                repo.index.commit("Set up Git LFS tracking")
                return "Git LFS configured successfully"
            except Exception as e:
                raise ValueError(f"Failed to set up Git LFS: {str(e)}")

    def batch_commit(
        self, repo_path: str, file_groups: List[List[str]], message_template: str
    ) -> List[str]:
        """Commit files in batches for better performance"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            commit_hashes = []
            for i, file_group in enumerate(file_groups):
                repo.git.add(file_group)
                commit = repo.index.commit(f"{message_template} (batch {i+1}/{len(file_groups)})")
                commit_hashes.append(commit.hexsha)
            return commit_hashes

    def pull_changes(
        self, repo_path: str, remote: str = "origin", branch: str = None, all_remotes: bool = False
    ) -> str:
        """Pull changes from a remote repository"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            try:
                if all_remotes:
                    result = repo.git.fetch(all=True)
                else:
                    result = repo.git.pull(remote, branch)
                return result
            except Exception as e:
                raise ValueError(f"Failed to pull changes: {str(e)}")

    def create_tag(
        self, repo_path: str, tag_name: str, message: str = None, commit: str = "HEAD"
    ) -> str:
        """Create a new Git tag"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            try:
                if message:
                    repo.create_tag(tag_name, ref=commit, message=message)
                else:
                    repo.create_tag(tag_name, ref=commit)
                return f"Created tag '{tag_name}'"
            except Exception as e:
                raise ValueError(f"Failed to create tag: {str(e)}")

    def list_tags(self, repo_path: str) -> List[Dict[str, str]]:
        """List all tags in the repository"""
        repo = self._get_repo(repo_path)
        try:
            tags = []
            for tag in repo.tags:
                tags.append(
                    {
                        "name": tag.name,
                        "commit": tag.commit.hexsha,
                        "date": tag.commit.committed_datetime.strftime("%Y-%m-%d %H:%M:%S %z"),
                    }
                )
            return tags
        except Exception as e:
            raise ValueError(f"Failed to list tags: {str(e)}")

    def optimize_repo(self, repo_path: str) -> str:
        """Optimize the Git repository"""
        repo = self._get_repo(repo_path)
        try:
            repo.git.gc("--aggressive", "--prune=now")
            return "Repository optimized successfully"
        except Exception as e:
            raise ValueError(f"Failed to optimize repository: {str(e)}")

    def configure_auth(self, repo_path: str, username: str, password: str) -> str:
        """Configure authentication for repository operations"""
        if not username or not password:
            raise ValueError("Username and password required for HTTPS authentication")
        # Note: Storing passwords in git config is not secure
        # Consider using git credential store or credential manager
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            with repo.config_writer() as config:
                config.set_value("user", "name", username)
                config.set_value("user", "password", password)
            return "Authentication configured successfully"

    def register_webhook(self, repo_path: str, webhook: Dict[str, Any]) -> str:
        """Register a webhook for Git events"""
        # Note: This is a placeholder implementation. In a real-world scenario, you would need to handle webhooks
        # using Git hooks or a custom implementation.
        hook_path = os.path.join(repo_path, ".git", "hooks", "post-commit")
        with open(hook_path, "w") as hook_file:
            hook_file.write(
                f"#!/bin/sh\ncurl -X POST {webhook['url']} -d @- <<'EOF'\n$(git log -1 --pretty=format:'%H')\nEOF\n"
            )
        os.chmod(hook_path, 0o755)
        return "Webhook registered successfully"

    def restore_file_version(self, repo_path: str, file_path: str, version: str) -> bool:
        """Restore a file to a specific version"""
        try:
            # Get the file content at the specified version
            content = self.get_file_content(repo_path, file_path, version)
            # Write that content to the current file
            full_path = os.path.join(repo_path, file_path)
            os.makedirs(os.path.dirname(full_path), exist_ok=True)
            with open(full_path, "w", encoding="utf-8") as f:
                f.write(content)
            # Add and commit the change
            self.add_files(repo_path, [file_path])
            self.commit_changes(repo_path, f"Restored file to version {version}")
            return True
        except Exception as e:
            logger.error(f"Error restoring file version: {e}", exc_info=True)
            return False


class GitRepoPath(BaseModel):
    repo_path: str = Field(..., description="Path to the Git repository")


class GitCommitRequest(GitRepoPath):
    files: List[str] = Field(..., description="List of files to add")
    message: str = Field(..., description="Commit message")
    author_name: Optional[str] = Field(None, description="Author name")
    author_email: Optional[str] = Field(None, description="Author email")


class GitDiffRequest(GitRepoPath):
    file_path: Optional[str] = Field(None, description="Path to the file to diff")
    target: Optional[str] = Field(None, description="Target to diff against")


class GitLogRequest(GitRepoPath):
    max_count: int = Field(10, description="Maximum number of commits to return")
    file_path: Optional[str] = Field(None, description="Path to the file to get log for")


class GitBranchRequest(GitRepoPath):
    branch_name: str = Field(..., description="Name of the branch to create")
    base_branch: Optional[str] = Field(
        None, description="Base branch to create the new branch from"
    )


class GitCheckoutRequest(GitRepoPath):
    branch_name: str = Field(..., description="Name of the branch to checkout")
    create: bool = Field(False, description="Create the branch if it doesn't exist")


class GitCloneRequest(BaseModel):
    repo_url: str = Field(..., description="URL of the repository to clone")
    local_path: str = Field(..., description="Path to clone the repository to")
    auth_token: Optional[str] = Field(
        None, description="Authentication token for private repositories"
    )


class GitRemoveFileRequest(GitRepoPath):
    file_path: str = Field(..., description="Path to the file to remove")


class GitFileContentRequest(GitRepoPath):
    file_path: str = Field(..., description="Path to the file")
    version: str = Field(..., description="Git version to get the file content from")


class GitLFSRequest(GitRepoPath):
    file_patterns: List[str] = Field(..., description="List of file patterns to track with LFS")


class GitBatchCommitRequest(GitRepoPath):
    file_groups: List[List[str]] = Field(
        ..., description="List of file groups to commit in batches"
    )
    message_template: str = Field(..., description="Template for commit messages")


class GitPullRequest(GitRepoPath):
    remote: str = Field("origin", description="Remote to pull from")
    branch: Optional[str] = Field(None, description="Branch to pull")
    all_remotes: bool = Field(False, description="Fetch from all remotes")


class GitTagRequest(GitRepoPath):
    tag_name: str = Field(..., description="Tag name")
    message: Optional[str] = Field(None, description="Tag message")
    commit: str = Field("HEAD", description="Commit to tag")


class GitTagsResponse(BaseModel):
    tags: List[Dict[str, str]] = Field(..., description="List of tags")


class GitWebhook(BaseModel):
    url: str = Field(..., description="Webhook URL")
    events: List[str] = Field(..., description="List of events to trigger the webhook")
    secret: Optional[str] = Field(None, description="Webhook secret")


router = APIRouter()
git_service = GitService()


@router.post(
    "/status",
    response_model=Dict[str, Any],
    summary="Get repository status",
    description="Get the status of a Git repository, including the current branch, staged changes, and unstaged changes.",
)
async def get_status(request: GitRepoPath = Body(...)):
    """Get the status of a Git repository."""
    try:
        return git_service.get_status(request.repo_path)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get status: {str(e)}")


@router.post(
    "/diff",
    response_model=str,
    summary="Get diff of changes",
    description="Get the difference between working directory and HEAD or a specified target.",
)
async def get_diff(request: GitDiffRequest = Body(...)):
    """Get the difference between working directory and HEAD or a specified target."""
    try:
        return git_service.get_diff(request.repo_path, request.file_path, request.target)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get diff: {str(e)}")


@router.post(
    "/add",
    response_model=str,
    summary="Stage files for commit",
    description="Stage files for commit.",
)
async def add_files(request: GitCommitRequest = Body(...)):
    """Stage files for commit."""
    try:
        return git_service.add_files(request.repo_path, request.files)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to add files: {str(e)}")


@router.post(
    "/commit",
    response_model=str,
    summary="Commit changes",
    description="Commit staged changes with a commit message. Optionally, specify the author name and email.",
)
async def commit_changes(request: GitCommitRequest = Body(...)):
    """Commit staged changes with a commit message. Optionally, specify the author name and email."""
    try:
        return git_service.commit_changes(
            request.repo_path, request.message, request.author_name, request.author_email
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to commit changes: {str(e)}")


@router.post(
    "/reset",
    response_model=str,
    summary="Reset staged changes",
    description="Reset all staged changes.",
)
async def reset_changes(request: GitRepoPath = Body(...)):
    """Reset all staged changes."""
    try:
        return git_service.reset_changes(request.repo_path)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to reset changes: {str(e)}")


@router.post(
    "/log",
    response_model=List[Dict[str, Any]],
    summary="Get commit log",
    description="Get the commit log of the repository",
)
async def get_log(request: GitLogRequest = Body(...)):
    """Get the commit log of the repository."""
    try:
        return git_service.get_log(request.repo_path, request.max_count, request.file_path)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get log: {str(e)}")


@router.post(
    "/branch",
    response_model=str,
    summary="Create branch",
    description="Create a new branch from a base branch.",
)
async def create_branch(request: GitBranchRequest = Body(...)):
    """Create a new branch from a base branch."""
    try:
        return git_service.create_branch(
            request.repo_path, request.branch_name, request.base_branch
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create branch: {str(e)}")


================================================================================
FILE: app/api/filesystem.py
LANGUAGE: python
SIZE: 10515 bytes
================================================================================

from fastapi import APIRouter, Body, HTTPException, Query, Path, File, UploadFile, Form
from fastapi.responses import PlainTextResponse, FileResponse, Response
from typing import List, Dict, Any, Optional
import os
import logging
from app.models.filesystem import (
    ReadFileRequest,
    WriteFileRequest,
    ListDirectoryRequest,
    SearchFilesRequest,
    CreateDirectoryRequest,
    DeleteFileRequest,
    DirectoryListingResponse,
    InvalidateCacheRequest,
    FileExistsRequest,
)
from app.core.filesystem_service import FilesystemService

logger = logging.getLogger(__name__)
router = APIRouter(
    responses={
        400: {"description": "Bad request"},
        403: {"description": "Access denied"},
        404: {"description": "File not found"},
        500: {"description": "Server error"},
    }
)
filesystem_service = FilesystemService()


@router.post(
    "/read",
    response_class=PlainTextResponse,
    summary="Read a file",
    description="Read the entire contents of a file from local or S3 storage",
)
async def read_file(request: ReadFileRequest = Body(...)):
    """
    Read the entire contents of a file.
    Supports both local filesystem and S3 storage.
    """
    try:
        return filesystem_service.read_file(request.path, request.storage, request.bucket)
    except ValueError as e:
        logger.warning(f"Access denied: {e}")
        raise HTTPException(status_code=403, detail=str(e))
    except FileNotFoundError:
        logger.warning(f"File not found: {request.path}")
        raise HTTPException(status_code=404, detail=f"File not found: {request.path}")
    except Exception as e:
        logger.error(f"Error reading file: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/write",
    response_class=PlainTextResponse,
    summary="Write to a file",
    description="Write content to a file, overwriting if it exists",
)
async def write_file(request: WriteFileRequest = Body(...)):
    """
    Write content to a file, overwriting if it exists.
    Supports both local filesystem and S3 storage.
    """
    try:
        result = filesystem_service.write_file(
            request.path, request.content, request.storage, request.bucket
        )
        # Invalidate cache if caching is enabled
        try:
            filesystem_service.invalidate_cache(request.path, request.storage, request.bucket)
        except Exception as cache_error:
            logger.warning(f"Failed to invalidate cache: {cache_error}")
        return result
    except ValueError as e:
        logger.warning(f"Access denied: {e}")
        raise HTTPException(status_code=403, detail=str(e))
    except Exception as e:
        logger.error(f"Error writing file: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/list",
    response_model=DirectoryListingResponse,
    summary="List directory contents",
    description="List contents of a directory",
)
async def list_directory(request: ListDirectoryRequest = Body(...)):
    """
    List contents of a directory.
    Supports both local filesystem and S3 storage.
    """
    try:
        return filesystem_service.list_directory(
            request.path, request.storage, request.bucket, request.recursive
        )
    except ValueError as e:
        raise HTTPException(status_code=403, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.post(
    "/search", summary="Search for files", description="Search for files matching a pattern"
)
async def search_files(request: SearchFilesRequest = Body(...)):
    """
    Search for files matching a pattern.
    Supports both local filesystem and S3 storage.
    """
    try:
        results = filesystem_service.search_files(
            request.path, request.pattern, request.storage, request.bucket, request.exclude_patterns
        )
        return {"matches": results or ["No matches found"]}
    except ValueError as e:
        raise HTTPException(status_code=403, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.post(
    "/mkdir",
    response_class=PlainTextResponse,
    summary="Create a directory",
    description="Create a directory and parent directories if needed",
)
async def create_directory(request: CreateDirectoryRequest = Body(...)):
    """
    Create a new directory recursively.
    Supports both local filesystem and S3 storage.
    """
    try:
        return filesystem_service.create_directory(request.path, request.storage, request.bucket)
    except ValueError as e:
        raise HTTPException(status_code=403, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.post(
    "/delete",
    response_class=PlainTextResponse,
    summary="Delete a file",
    description="Delete a file from storage",
)
async def delete_file(request: DeleteFileRequest = Body(...)):
    """
    Delete a file.
    Supports both local filesystem and S3 storage.
    """
    try:
        return filesystem_service.delete_file(request.path, request.storage, request.bucket)
    except ValueError as e:
        raise HTTPException(status_code=403, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.post(
    "/read-binary",
    summary="Read binary file",
    description="Read a binary file and return its contents",
    responses={
        200: {"content": {"application/octet-stream": {}}, "description": "Binary file content"}
    },
)
async def read_binary_file(request: ReadFileRequest = Body(...)):
    """
    Read the contents of a binary file.
    Returns the file as a downloadable binary response.
    """
    try:
        content = filesystem_service.read_file_binary(request.path, request.storage, request.bucket)
        # Get filename from path
        filename = os.path.basename(request.path)
        return Response(
            content=content,
            media_type="application/octet-stream",
            headers={"Content-Disposition": f"attachment; filename={filename}"},
        )
    except ValueError as e:
        logger.warning(f"Access denied: {e}")
        raise HTTPException(status_code=403, detail=str(e))
    except FileNotFoundError:
        logger.warning(f"File not found: {request.path}")
        raise HTTPException(status_code=404, detail=f"File not found: {request.path}")
    except Exception as e:
        logger.error(f"Error reading binary file: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/upload",
    response_class=PlainTextResponse,
    summary="Upload file",
    description="Upload a file to storage",
)
async def upload_file(
    file: UploadFile = File(...),
    path: str = Form(...),
    storage: str = Form("local"),
    bucket: Optional[str] = Form(None),
):
    """
    Upload a file to storage.
    Supports both local filesystem and S3 storage.
    """
    try:
        content = await file.read()
        result = filesystem_service.write_file_binary(
            os.path.join(path, file.filename), content, storage, bucket
        )
        # Invalidate cache if caching is enabled
        try:
            filesystem_service.invalidate_cache(os.path.join(path, file.filename), storage, bucket)
        except Exception as cache_error:
            logger.warning(f"Failed to invalidate cache: {cache_error}")
        return result
    except ValueError as e:
        logger.warning(f"Access denied: {e}")
        raise HTTPException(status_code=403, detail=str(e))
    except Exception as e:
        logger.error(f"Error uploading file: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/exists",
    summary="Check if file exists",
    description="Check if a file or directory exists in storage",
)
async def file_exists(request: FileExistsRequest = Body(...)):
    """
    Check if a file or directory exists.
    Supports both local filesystem and S3 storage.
    """
    try:
        exists = False
        if request.storage == "local":
            try:
                path = filesystem_service.normalize_path(request.path)
                exists = path.exists()
            except ValueError:
                # If normalize_path fails, file doesn't exist or is inaccessible
                exists = False
        elif request.storage == "s3":
            if not request.bucket:
                raise ValueError("S3 bucket name is required for S3 storage")
            if not filesystem_service.s3_client:
                raise ValueError("S3 client not configured")
            try:
                filesystem_service.s3_client.head_object(Bucket=request.bucket, Key=request.path)
                exists = True
            except:
                exists = False
        else:
            raise ValueError(f"Unsupported storage type: {request.storage}")
        return {"exists": exists, "path": request.path}
    except ValueError as e:
        logger.warning(f"Invalid request: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Error checking file existence: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/invalidate-cache",
    response_class=PlainTextResponse,
    summary="Invalidate cache",
    description="Invalidate file cache for a path or all paths",
)
async def invalidate_cache(request: InvalidateCacheRequest = Body(...)):
    """
    Invalidate file cache.
    Can invalidate a specific path or all cached files.
    """
    try:
        if request.path:
            filesystem_service.invalidate_cache(request.path, request.storage, request.bucket)
            return f"Successfully invalidated cache for {request.path}"
        else:
            filesystem_service.invalidate_cache()
            return "Successfully invalidated all cache entries"
    except Exception as e:
        logger.error(f"Error invalidating cache: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


================================================================================
FILE: app/api/git.py
LANGUAGE: python
SIZE: 24204 bytes
================================================================================

import os
import tempfile
from typing import List, Optional, Dict, Any, Union
import git
from git import Repo
from fastapi import APIRouter, Body, HTTPException
from pydantic import BaseModel, Field
from app.utils.config import get_config

router = APIRouter()


class GitRepoPath(BaseModel):
    repo_path: str = Field(..., description="Path to the Git repository")


class GitAddRequest(GitRepoPath):
    files: List[str] = Field(..., description="List of files to add")


class GitCommitRequest(GitRepoPath):
    message: str = Field(..., description="Commit message")
    author_name: Optional[str] = Field(None, description="Author name")
    author_email: Optional[str] = Field(None, description="Author email")


class GitStatusRequest(GitRepoPath):
    pass


class GitDiffRequest(GitRepoPath):
    file_path: Optional[str] = Field(None, description="Path to the file to diff")
    target: Optional[str] = Field(None, description="Target to diff against")


class GitLogRequest(GitRepoPath):
    max_count: int = Field(10, description="Maximum number of commits to return")
    file_path: Optional[str] = Field(None, description="Path to the file to get log for")


class GitBranchRequest(GitRepoPath):
    branch_name: str = Field(..., description="Name of the branch")
    base_branch: Optional[str] = Field(
        None, description="Base branch to create the new branch from"
    )


class GitCheckoutRequest(GitRepoPath):
    branch_name: str = Field(..., description="Name of the branch to checkout")
    create: bool = Field(False, description="Create the branch if it doesn't exist")


class GitCloneRequest(BaseModel):
    repo_url: str = Field(..., description="URL of the repository to clone")
    local_path: str = Field(..., description="Local path to clone the repository to")
    auth_token: Optional[str] = Field(
        None, description="Authentication token for private repositories"
    )


class GitRemoveFileRequest(GitRepoPath):
    file_path: str = Field(..., description="Path to the file to remove")


class GitFileContentRequest(GitRepoPath):
    file_path: str = Field(..., description="Path to the file")
    version: str = Field(..., description="Git version to get the file content from")


class GitLFSRequest(GitRepoPath):
    file_patterns: List[str] = Field(..., description="List of file patterns to track with LFS")


class GitBatchCommitRequest(GitRepoPath):
    file_groups: List[List[str]] = Field(
        ..., description="List of file groups to commit in batches"
    )
    message_template: str = Field(..., description="Template for commit messages")


class GitPullRequest(GitRepoPath):
    remote: str = Field("origin", description="Name of the remote")
    branch: Optional[str] = Field(None, description="Branch to pull")


class GitFetchRequest(GitRepoPath):
    remote: str = Field("origin", description="Name of the remote")
    all_remotes: bool = Field(False, description="Fetch from all remotes")


class GitCreateTagRequest(GitRepoPath):
    tag_name: str = Field(..., description="Name of the tag to create")
    message: Optional[str] = Field(None, description="Tag message")
    commit: str = Field("HEAD", description="Commit to tag")


class TagInfo(BaseModel):
    name: str = Field(..., description="Tag name")
    commit: str = Field(..., description="Tagged commit hash")
    date: str = Field(..., description="Date of tagged commit")


class GitTagsResponse(BaseModel):
    tags: List[TagInfo] = Field(..., description="List of tags")


class GitService:
    def __init__(self):
        config = get_config()
        self.default_username = config.default_git_username
        self.default_email = config.default_git_email
        self.temp_auth_files = {}

    def _get_repo(self, repo_path: str) -> git.Repo:
        """Get git repository object"""
        try:
            repo = Repo(repo_path)
            return repo
        except git.exc.InvalidGitRepositoryError:
            raise ValueError(f"Invalid Git repository at '{repo_path}'")
        except Exception as e:
            raise ValueError(f"Failed to get repository: {str(e)}")

    def get_status(self, repo_path: str) -> Dict[str, Any]:
        """Get the status of a Git repository"""
        repo = self._get_repo(repo_path)
        current_branch = repo.active_branch.name
        # Get staged files
        staged_files = [item.a_path for item in repo.index.diff("HEAD")]
        # Get modified but unstaged files
        unstaged_files = [item.a_path for item in repo.index.diff(None)]
        # Get untracked files
        untracked_files = repo.untracked_files
        return {
            "clean": not (staged_files or unstaged_files or untracked_files),
            "current_branch": current_branch,
            "staged_files": staged_files,
            "unstaged_files": unstaged_files,
            "untracked_files": untracked_files,
        }

    def get_diff(
        self, repo_path: str, file_path: Optional[str] = None, target: Optional[str] = None
    ) -> str:
        """Get diff of changes"""
        repo = self._get_repo(repo_path)
        if file_path and target:
            return repo.git.diff(target, file_path)
        elif file_path:
            return repo.git.diff("HEAD", file_path)
        elif target:
            return repo.git.diff(target)
        else:
            return repo.git.diff()

    def add_files(self, repo_path: str, files: List[str]) -> str:
        """Stage files for commit"""
        repo = self._get_repo(repo_path)
        repo.git.add(files)
        return "Files staged successfully"

    def commit_changes(
        self,
        repo_path: str,
        message: str,
        author_name: Optional[str] = None,
        author_email: Optional[str] = None,
    ) -> str:
        """Commit staged changes"""
        repo = self._get_repo(repo_path)
        author_name = author_name or self.default_username
        author_email = author_email or self.default_email
        # Set author for this commit
        with repo.config_writer() as config:
            config.set_value("user", "name", author_name)
            config.set_value("user", "email", author_email)
        # Commit changes
        commit = repo.index.commit(message)
        return f"Committed changes with hash {commit.hexsha}"

    def reset_changes(self, repo_path: str) -> str:
        """Reset staged changes"""
        repo = self._get_repo(repo_path)
        repo.git.reset()
        return "All staged changes reset"

    def get_log(
        self, repo_path: str, max_count: int = 10, file_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """Get commit log"""
        repo = self._get_repo(repo_path)
        if file_path:
            # Get log for specific file
            commits = list(repo.iter_commits(paths=file_path, max_count=max_count))
        else:
            # Get log for entire repo
            commits = list(repo.iter_commits(max_count=max_count))
        commit_data = []
        for commit in commits:
            commit_data.append(
                {
                    "hash": commit.hexsha,
                    "author": f"{commit.author.name} <{commit.author.email}>",
                    "date": commit.committed_datetime.strftime("%Y-%m-%d %H:%M:%S %z"),
                    "message": commit.message.strip(),
                }
            )
        return {"commits": commit_data}

    def create_branch(
        self, repo_path: str, branch_name: str, base_branch: Optional[str] = None
    ) -> str:
        """Create a new branch"""
        repo = self._get_repo(repo_path)
        if base_branch:
            base = repo.refs[base_branch]
        else:
            base = repo.active_branch
        repo.create_head(branch_name, base)
        return f"Created branch '{branch_name}'"

    def checkout_branch(self, repo_path: str, branch_name: str, create: bool = False) -> str:
        """Checkout a branch"""
        repo = self._get_repo(repo_path)
        if create:
            # Create branch if it doesn't exist
            if branch_name not in repo.refs:
                repo.create_head(branch_name)
        # Checkout the branch
        repo.git.checkout(branch_name)
        return f"Switched to branch '{branch_name}'"

    def clone_repo(self, repo_url: str, local_path: str, auth_token: Optional[str] = None) -> str:
        """Clone a Git repository"""
        try:
            # If auth token is provided, modify the URL
            if auth_token:
                # Parse the URL to insert authentication
                if repo_url.startswith("https://"):
                    parsed_url = repo_url.replace(
                        "https://", f"https://x-access-token:{auth_token}@"
                    )
                    repo = git.Repo.clone_from(parsed_url, local_path)
                else:
                    # For SSH or other protocols, use standard clone
                    repo = git.Repo.clone_from(repo_url, local_path)
            else:
                repo = git.Repo.clone_from(repo_url, local_path)
            return f"Successfully cloned repository to '{local_path}'"
        except Exception as e:
            raise ValueError(f"Failed to clone repository: {str(e)}")

    def remove_file(self, repo_path: str, file_path: str) -> str:
        """Remove a file from Git"""
        repo = self._get_repo(repo_path)
        try:
            repo.index.remove([file_path])
            return f"Successfully removed {file_path} from Git"
        except Exception as e:
            raise ValueError(f"Failed to remove file: {str(e)}")

    def get_file_content_at_version(self, repo_path: str, file_path: str, version: str) -> str:
        """Get file content at a specific Git version"""
        repo = self._get_repo(repo_path)
        try:
            return repo.git.show(f"{version}:{file_path}")
        except Exception as e:
            raise ValueError(f"Failed to get file content at version {version}: {str(e)}")

    def setup_lfs(self, repo_path: str, file_patterns: List[str]) -> str:
        """Configure Git LFS for the repository"""
        repo = self._get_repo(repo_path)
        try:
            # Initialize LFS
            repo.git.execute(["git", "lfs", "install"])
            # Track file patterns
            for pattern in file_patterns:
                repo.git.execute(["git", "lfs", "track", pattern])
            # Add .gitattributes
            repo.git.add(".gitattributes")
            repo.git.commit("-m", "Set up Git LFS tracking")
            return "Git LFS configured successfully"
        except Exception as e:
            raise ValueError(f"Failed to set up Git LFS: {str(e)}")

    def batch_commit(
        self, repo_path: str, file_groups: List[List[str]], message_template: str
    ) -> List[str]:
        """Commit files in batches for better performance"""
        repo = self._get_repo(repo_path)
        commit_hashes = []
        for i, files in enumerate(file_groups):
            repo.git.add(files)
            commit = repo.index.commit(f"{message_template} (batch {i+1}/{len(file_groups)})")
            commit_hashes.append(commit.hexsha)
        return commit_hashes

    def pull(self, repo_path: str, remote: str = "origin", branch: str = None) -> str:
        """Pull changes from remote repository"""
        repo = self._get_repo(repo_path)
        try:
            if branch:
                result = repo.git.pull(remote, branch)
            else:
                result = repo.git.pull(remote)
            return result
        except Exception as e:
            raise ValueError(f"Failed to pull changes: {str(e)}")

    def fetch(self, repo_path: str, remote: str = "origin", all_remotes: bool = False) -> str:
        """Fetch changes from remote repository"""
        repo = self._get_repo(repo_path)
        try:
            if all_remotes:
                result = repo.git.fetch(all=True)
            else:
                result = repo.git.fetch(remote)
            return result
        except Exception as e:
            raise ValueError(f"Failed to fetch changes: {str(e)}")

    def create_tag(
        self, repo_path: str, tag_name: str, message: str = None, commit: str = "HEAD"
    ) -> str:
        """Create a new Git tag"""
        repo = self._get_repo(repo_path)
        try:
            if message:
                repo.create_tag(tag_name, ref=commit, message=message)
            else:
                repo.create_tag(tag_name, ref=commit)
            return f"Created tag '{tag_name}'"
        except Exception as e:
            raise ValueError(f"Failed to create tag: {str(e)}")

    def list_tags(self, repo_path: str) -> List[Dict[str, str]]:
        """List all tags in the repository"""
        repo = self._get_repo(repo_path)
        try:
            tags = []
            for tag in repo.tags:
                tags.append(
                    {
                        "name": tag.name,
                        "commit": tag.commit.hexsha,
                        "date": tag.commit.committed_datetime.strftime("%Y-%m-%d %H:%M:%S %z"),
                    }
                )
            return tags
        except Exception as e:
            raise ValueError(f"Failed to list tags: {str(e)}")


git_service = GitService()


@router.post(
    "/status",
    response_model=Dict[str, Any],
    summary="Get repository status",
    description="Get the current status of a Git repository, including the current branch, staged changes, and unstaged changes.",
)
async def get_status(request: GitStatusRequest = Body(...)):
    """
    Get the current status of a Git repository.
    Returns the current branch, staged changes, and unstaged changes.
    """
    try:
        return git_service.get_status(request.repo_path)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get status: {str(e)}")


@router.post(
    "/diff",
    response_model=str,
    summary="Get diff of changes",
    description="Get difference between working directory and HEAD or a specified target",
)
async def get_diff(request: GitDiffRequest = Body(...)):
    """
    Get difference between working directory and HEAD or a specified target.
    Show diff for specific files or the entire repository.
    """
    try:
        return git_service.get_diff(request.repo_path, request.file_path, request.target)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get diff: {str(e)}")


@router.post(
    "/add", response_model=str, summary="Stage files", description="Stage files for commit"
)
async def add_files(request: GitAddRequest = Body(...)):
    """
    Stage files for commit.
    Adds the specified files to the staging area.
    """
    try:
        return git_service.add_files(request.repo_path, request.files)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to add files: {str(e)}")


@router.post(
    "/commit",
    response_model=str,
    summary="Commit changes",
    description="Commit staged changes with a commit message.",
)
async def commit_changes(request: GitCommitRequest = Body(...)):
    """
    Commit staged changes with a commit message.
    Optionally, specify the author name and email.
    """
    try:
        return git_service.commit_changes(
            request.repo_path, request.message, request.author_name, request.author_email
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to commit changes: {str(e)}")


@router.post(
    "/log",
    response_model=Dict[str, Any],
    summary="Get commit log",
    description="Get the commit history of the repository",
)
async def get_log(request: GitLogRequest = Body(...)):
    """
    Get the commit history of the repository.
    Returns a list of commits with hash, author, date, and message.
    """
    try:
        return git_service.get_log(request.repo_path, request.max_count, request.file_path)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get log: {str(e)}")


@router.post(
    "/branch",
    response_model=str,
    summary="Create branch",
    description="Create a new branch in the repository",
)
async def create_branch(request: GitBranchRequest = Body(...)):
    """
    Create a new branch in the repository.
    Optionally, specify the base branch to create the new branch from.
    """
    try:
        return git_service.create_branch(
            request.repo_path, request.branch_name, request.base_branch
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create branch: {str(e)}")


@router.post(
    "/checkout", response_model=str, summary="Checkout branch", description="Checkout a branch"
)
async def checkout_branch(request: GitCheckoutRequest = Body(...)):
    """
    Checkout a branch.
    Optionally, create the branch if it doesn't exist.
    """
    try:
        return git_service.checkout_branch(request.repo_path, request.branch_name, request.create)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to checkout branch: {str(e)}")


@router.post(
    "/clone",
    response_model=str,
    summary="Clone repository",
    description="Clone a Git repository from a URL",
)
async def clone_repo(request: GitCloneRequest = Body(...)):
    """
    Clone a Git repository from a URL.
    Clones the repository from the specified URL to a local path.
    """
    try:
        return git_service.clone_repo(request.repo_url, request.local_path, request.auth_token)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to clone repository: {str(e)}")


@router.post(
    "/remove",
    response_model=str,
    summary="Remove file",
    description="Remove a file from the repository",
)
async def remove_file(request: GitRemoveFileRequest = Body(...)):
    """
    Remove a file from the repository.
    Removes the specified file from the Git repository.
    """
    try:
        return git_service.remove_file(request.repo_path, request.file_path)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to remove file: {str(e)}")


@router.post(
    "/file-content",
    response_model=str,
    summary="Get file content",
    description="Get file content at a specific Git version",
)
async def get_file_content(request: GitFileContentRequest = Body(...)):
    """
    Get file content at a specific Git version.
    Returns the content of the specified file at the given version.
    """
    try:
        return git_service.get_file_content_at_version(
            request.repo_path, request.file_path, request.version
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get file content: {str(e)}")


@router.post(
    "/lfs",
    response_model=str,
    summary="Set up Git LFS",
    description="Configure Git LFS for the repository",
)
async def setup_lfs(request: GitLFSRequest = Body(...)):
    """
    Configure Git LFS for the repository.
    Tracks the specified file patterns with Git LFS.
    """
    try:
        return git_service.setup_lfs(request.repo_path, request.file_patterns)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to set up Git LFS: {str(e)}")


@router.post(
    "/batch-commit",
    response_model=List[str],
    summary="Batch commit",
    description="Commit files in batches",
)
async def batch_commit(request: GitBatchCommitRequest = Body(...)):
    """
    Commit files in batches.
    Commits the specified file groups in batches for better performance.
    """
    try:
        return git_service.batch_commit(
            request.repo_path, request.file_groups, request.message_template
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to batch commit: {str(e)}")


@router.post(
    "/pull",
    response_model=str,
    summary="Pull changes",
    description="Pull changes from a remote repository",
)
async def pull_changes(request: GitPullRequest = Body(...)):
    """
    Pull changes from a remote repository.
    Fetches and merges changes from the specified remote and branch.
    """
    try:
        return git_service.pull(request.repo_path, request.remote, request.branch)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to pull changes: {str(e)}")


@router.post(
    "/fetch",
    response_model=str,
    summary="Fetch changes",
    description="Fetch changes from a remote repository",
)
async def fetch_changes(request: GitFetchRequest = Body(...)):
    """
    Fetch changes from a remote repository.
    Downloads objects and refs from the specified remote.
    """
    try:
        return git_service.fetch(request.repo_path, request.remote, request.all_remotes)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to fetch changes: {str(e)}")


@router.post(
    "/tag/create",
    response_model=str,
    summary="Create tag",
    description="Create a new tag in the repository",
)
async def create_tag(request: GitCreateTagRequest = Body(...)):
    """
    Create a new tag in the repository.
    Tags a specific commit with a name and optional message.
    """
    try:
        return git_service.create_tag(
            request.repo_path, request.tag_name, request.message, request.commit
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create tag: {str(e)}")


@router.post(
    "/tags",
    response_model=GitTagsResponse,
    summary="List tags",
    description="List all tags in the repository",
)
async def list_tags(request: GitRepoPath = Body(...)):
    """
    List all tags in the repository.
    Returns tag names, associated commits, and dates.
    """
    try:
        tags = git_service.list_tags(request.repo_path)
        return {"tags": tags}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list tags: {str(e)}")


================================================================================
FILE: app/api/memory.py
LANGUAGE: python
SIZE: 17371 bytes
================================================================================

import logging
from typing import Dict, Any, List, Optional

from fastapi import APIRouter, Body, HTTPException, Query, Path as FastAPIPath
from pydantic import BaseModel, Field

# Import models
from app.models.memory import (
    KnowledgeGraph,
    AddEntitiesRequest,
    AddRelationsRequest,
)
from app.models.serper import SerperSearchRequest
from app.models.documents import DocumentType
from app.core.scraper_service import ScraperService
from app.core.documents_service import DocumentsService
from app.core.memory_service import MemoryService

# Set up logger
logger = logging.getLogger(__name__)

# Define models
class ScrapeSingleUrlRequest(BaseModel):
    """Request to scrape a single URL"""

    url: str = Field(..., description="URL to scrape")
    wait_for_selector: Optional[str] = Field(None, description="CSS selector to wait for")
    wait_for_timeout: Optional[int] = Field(30000, description="Maximum wait time in ms")
    extract_tables: bool = Field(True, description="Extract tables from content")
    store_as_document: bool = Field(False, description="Store result as a document")
    document_tags: Optional[List[str]] = Field(None, description="Tags for document if stored")


class UrlList(BaseModel):
    """Request to scrape multiple URLs"""

    urls: List[str] = Field(..., description="List of URLs to scrape")
    recursion_depth: int = Field(0, ge=0, le=3, description="How many links deep to follow (0-3)")
    store_as_documents: bool = Field(False, description="Save results as documents")
    document_tags: Optional[List[str]] = Field(None, description="Tags for documents if stored")


class ScrapeCrawlRequest(BaseModel):
    """Request to crawl a website"""

    start_url: str = Field(..., description="Starting URL for crawl")
    max_pages: int = Field(100, ge=1, description="Maximum number of pages to crawl")
    recursion_depth: int = Field(1, ge=1, description="How many links deep to follow")
    allowed_domains: Optional[List[str]] = Field(
        None, description="Restrict crawling to these domains"
    )
    create_documents: bool = Field(True, description="Create documents from scraped content")
    document_tags: Optional[List[str]] = Field(None, description="Tags for documents if created")
    verification_pass: bool = Field(False, description="Run verification pass after initial crawl")


class SearchAndScrapeRequest(BaseModel):
    """Request to search and scrape results"""

    query: str = Field(..., description="Search query")
    max_results: int = Field(10, ge=1, le=50, description="Maximum search results to process")
    create_documents: bool = Field(False, description="Create documents from scraped content")
    document_tags: Optional[List[str]] = Field(None, description="Tags for documents if created")


class SitemapScrapeRequest(BaseModel):
    """Request to scrape URLs from a sitemap"""

    sitemap_url: str = Field(..., description="URL of the sitemap")
    max_urls: int = Field(50, ge=1, description="Maximum number of URLs to scrape")
    create_documents: bool = Field(True, description="Create documents from scraped content")
    document_tags: Optional[List[str]] = Field(None, description="Tags for documents if created")


class TableData(BaseModel):
    headers: List[str] = Field(default_factory=list, description="Table headers")
    rows: List[List[str]] = Field(default_factory=list, description="Table rows")


class ScraperResponse(BaseModel):
    """Response from scraper"""

    url: str = Field(..., description="Scraped URL")
    title: str = Field(..., description="Page title")
    content: str = Field(..., description="Cleaned content in Markdown format")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Extracted metadata")
    scraped_at: int = Field(..., description="Timestamp when scraped")
    success: bool = Field(True, description="Whether scraping was successful")
    links: List[str] = Field(default_factory=list, description="Links extracted from content")
    document_id: Optional[str] = Field(None, description="Document ID if saved as document")
    error: Optional[str] = Field(None, description="Error message if scraping failed")


# Create router and services
router = APIRouter()
scraper_service = ScraperService()
documents_service = DocumentsService()
memory_service = MemoryService()


# Define routes
@router.post(
    "/url",
    response_model=ScraperResponse,
    summary="Scrape a single URL",
    description="Extract content from a web page and convert to Markdown",
)
async def scrape_url(request: ScrapeSingleUrlRequest = Body(...)):
    """
    Scrape a single URL and return structured data.
    Extracts content, converts to Markdown, and optionally stores as a document.
    """
    try:
        # Call scrape_url method with just the URL
        result = await scraper_service.scrape_url(request.url)
        
        # If requested, store as document
        if request.store_as_document and result["success"]:
            doc = documents_service.create_document(
                title=result["title"],
                content=result["content"],
                document_type=DocumentType.WEBPAGE,
                metadata=result["metadata"],
                tags=request.document_tags or [],
                source_url=result["url"],
            )
            result["document_id"] = doc["id"]
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Scraping failed: {str(e)}") from e


@router.post(
    "/urls",
    response_model=List[ScraperResponse],
    summary="Scrape multiple URLs",
    description="Scrape multiple URLs in parallel",
)
async def scrape_multiple_urls(request: UrlList = Body(...)):
    """
    Scrape multiple URLs in parallel.
    Processes a list of URLs and returns the scraped content for each.
    """
    try:
        results = await scraper_service.scrape_urls(request.urls)
        # If requested, store results as documents
        if request.store_as_documents:
            for i, result in enumerate(results):
                if result["success"]:
                    try:
                        doc = documents_service.create_document(
                            title=result["title"],
                            content=result["content"],
                            document_type=DocumentType.WEBPAGE,
                            metadata=result["metadata"],
                            tags=request.document_tags or [],
                            source_url=result["url"],
                        )
                        results[i]["document_id"] = doc["id"]
                    except Exception as e:
                        results[i]["error"] = f"Document creation failed: {str(e)}"
        return results
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Scraping failed: {str(e)}") from e


@router.post(
    "/crawl",
    response_model=Dict[str, Any],
    summary="Crawl website",
    description="Crawl a website starting from a URL",
)
async def crawl_website(request: ScrapeCrawlRequest = Body(...)):
    """
    Crawl a website starting from a URL.
    Follows links up to a specified depth and processes each page.
    Optional verification pass ensures content stability.
    """
    try:
        results = await scraper_service.crawl_website(
            request.start_url,
            request.max_pages,
            request.recursion_depth,
            request.allowed_domains,
            request.verification_pass,
        )
        response = {
            "pages_crawled": results.get("pages_crawled", 0),
            "start_url": request.start_url,
            "success_count": results.get("success_count", 0),
            "failed_count": results.get("failed_count", 0),
        }
        # Include verification results if available
        if "verification_results" in results:
            response["verification_results"] = results["verification_results"]
            response["verification_success_rate"] = results["verification_success_rate"]
        # If requested, create documents
        if request.create_documents:
            document_ids = []
            for result in results.get("results", []):
                if result.get("success", False):
                    try:
                        doc = documents_service.create_document(
                            title=result["title"],
                            content=result["content"],
                            document_type=DocumentType.WEBPAGE,
                            metadata=result["metadata"],
                            tags=request.document_tags or [],
                            source_url=result["url"],
                        )
                        document_ids.append(doc["id"])
                    except Exception:
                        pass
            response["documents_created"] = len(document_ids)
            response["document_ids"] = document_ids
        return response
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Crawling failed: {str(e)}") from e


@router.post(
    "/serper/search",
    response_model=Dict[str, Any],
    summary="Enhanced search with Serper",
    description="Search and scrape content using Serper API with enhanced results",
)
async def enhanced_search(request: SerperSearchRequest = Body(...)):
    """
    Enhanced search and scrape using Serper API.
    Returns both search results and scraped content if requested.
    """
    try:
        # First get search results
        results = await scraper_service.enhanced_search_and_scrape(
            query=request.query,
            search_type=request.search_type,
            num_results=request.num_results,
            max_scrape=request.max_scrape if request.auto_scrape else 0,
            country=request.country,
            locale=request.locale
        )
        
        # If requested, create documents from scraped content
        if request.create_documents and request.auto_scrape and results.get("scraped_content"):
            document_ids = []
            for i, result in enumerate(results["scraped_content"]):
                if result.get("success", False):
                    try:
                        doc = documents_service.create_document(
                            title=result["title"],
                            content=result["content"],
                            document_type=DocumentType.WEBPAGE,
                            metadata=result["metadata"],
                            tags=request.document_tags or [],
                            source_url=result["url"],
                        )
                        results["scraped_content"][i]["document_id"] = doc["id"]
                        document_ids.append(doc["id"])
                    except Exception as e:
                        logger.error(f"Error creating document: {str(e)}")
            
            results["document_ids"] = document_ids
        
        return results
    except Exception as e:
        logger.error(f"Enhanced search failed: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Enhanced search failed: {str(e)}") from e


@router.post(
    "/sitemap",
    response_model=Dict[str, Any],
    summary="Scrape sitemap",
    description="Extract URLs from sitemap and scrape them",
)
async def scrape_sitemap(request: SitemapScrapeRequest = Body(...)):
    """
    Extract URLs from a sitemap and scrape them.
    Processes XML sitemap files and scrapes the listed URLs.
    """
    try:
        result = await scraper_service.scrape_sitemap(request.sitemap_url, request.max_urls)
        # Handle document creation if requested
        if request.create_documents and result.get("urls_scraped", []):
            document_ids = []
            for scraped_url in result["urls_scraped"]:
                if scraped_url.get("success", False):
                    try:
                        doc = documents_service.create_document(
                            title=scraped_url["title"],
                            content=scraped_url["content"],
                            document_type=DocumentType.WEBPAGE,
                            metadata=scraped_url["metadata"],
                            tags=request.document_tags or [],
                            source_url=scraped_url["url"],
                        )
                        document_ids.append(doc["id"])
                    except Exception:
                        pass
            result["documents_created"] = len(document_ids)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Sitemap scraping failed: {str(e)}") from e


# Add knowledge graph routes
@router.post(
    "/entities",
    response_model=List[Dict[str, Any]],
    summary="Add entities",
    description="Add entities to the knowledge graph",
)
async def add_entities(request: AddEntitiesRequest = Body(...)):
    """Add new entities to the knowledge graph"""
    try:
        created_entities = memory_service.create_entities(request.entities)
        return created_entities
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create entities: {str(e)}") from e


@router.post(
    "/relations",
    response_model=List[Dict[str, Any]],
    summary="Add relations",
    description="Add relationships to the knowledge graph",
)
async def add_relations(request: AddRelationsRequest = Body(...)):
    """Add new relations to the knowledge graph"""
    try:
        created_relations = memory_service.create_relations(request.relations)
        return created_relations
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create relations: {str(e)}") from e


@router.get(
    "/graph",
    response_model=KnowledgeGraph,
    summary="Get knowledge graph",
    description="Get the full knowledge graph",
)
async def get_graph():
    """Get the entire knowledge graph."""
    try:
        return memory_service.get_full_graph()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get graph: {str(e)}") from e


@router.get(
    "/entity/{entity_name}/related",
    response_model=Dict[str, Any],
    summary="Get related entities",
    description="Get entities related to a specific entity",
)
async def get_related_entities(
    entity_name: str = FastAPIPath(..., description="Entity name"),
    max_depth: int = Query(1, description="Maximum relationship depth"),
):
    """Get entities related to a specific entity up to a maximum depth."""
    try:
        return memory_service.get_related_entities(entity_name, max_depth)
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e)) from e
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get related entities: {str(e)}") from e


@router.get(
    "/entity/{entity_name}/connections",
    response_model=Dict[str, Any],
    summary="Get entity connections",
    description="Get direct connections for an entity",
)
async def get_entity_connections(entity_name: str = FastAPIPath(..., description="Entity name")):
    """Get direct connections for a specific entity."""
    try:
        return memory_service.get_entity_connections(entity_name)
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e)) from e
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get entity connections: {str(e)}") from e


@router.post(
    "/find-paths",
    response_model=List[List[Dict[str, Any]]],
    summary="Find paths",
    description="Find paths between entities in the knowledge graph",
)
async def find_paths(
    start_entity: str = Body(..., embed=True),
    end_entity: str = Body(..., embed=True),
    max_length: int = Body(3, embed=True),
):
    """Find paths between two entities in the knowledge graph, up to max_length."""
    try:
        paths = memory_service.find_paths(start_entity, end_entity, max_length)
        return paths
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e)) from e
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to find paths: {str(e)}") from e


@router.post(
    "/similar-entities",
    response_model=List[Dict[str, Any]],
    summary="Find similar entities",
    description="Find entities with similar names",
)
async def find_similar_entities(
    entity_name: str = Body(..., embed=True), threshold: float = Body(0.6, embed=True)
):
    """Find entities with similar names to the provided entity name."""
    try:
        return memory_service.get_similar_entities(entity_name, threshold)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to find similar entities: {str(e)}") from e


================================================================================
FILE: app/api/scraper.py
LANGUAGE: python
SIZE: 11433 bytes
================================================================================

import logging
from typing import Dict, Any, List
from fastapi import APIRouter, Body, HTTPException

from app.models.scraper import (
    ScrapeSingleUrlRequest,
    UrlList,
    ScrapeCrawlRequest,
    SearchAndScrapeRequest,
    ScraperResponse,
)
from app.core.scraper_service import ScraperService
from app.core.documents_service import DocumentsService
from app.models.documents import DocumentType
from app.models.serper import SerperSearchRequest
from app.core.serper_service import SerperService

# Set up logger
logger = logging.getLogger(__name__)

router = APIRouter(
    responses={400: {"description": "Bad request"}, 500: {"description": "Scraping failed"}}
)
scraper_service = ScraperService()
documents_service = DocumentsService()
serper_service = SerperService()


@router.post(
    "/url",
    response_model=ScraperResponse,
    summary="Scrape a single URL",
    description="Extract content from a web page and convert to Markdown",
)
async def scrape_url(request: ScrapeSingleUrlRequest = Body(...)):
    """
    Scrape a single URL and return structured data.
    Extracts content, converts to Markdown, and optionally stores as a document.
    """
    try:
        # Call scrape_url method with just the URL as it appears the method only accepts the URL parameter
        result = await scraper_service.scrape_url(request.url)
        
        # If requested, store as document
        if request.store_as_document and result["success"]:
            doc = documents_service.create_document(
                title=result["title"],
                content=result["content"],
                document_type=DocumentType.WEBPAGE,
                metadata=result["metadata"],
                tags=request.document_tags or [],
                source_url=result["url"],
            )
            result["document_id"] = doc["id"]
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Scraping failed: {str(e)}") from e


@router.post(
    "/urls",
    response_model=List[ScraperResponse],
    summary="Scrape multiple URLs",
    description="Scrape multiple URLs in parallel",
)
async def scrape_multiple_urls(request: UrlList = Body(...)):
    """
    Scrape multiple URLs in parallel.
    Processes a list of URLs and returns the scraped content for each.
    """
    try:
        results = await scraper_service.scrape_urls(request.urls)
        # If requested, store results as documents
        if request.store_as_documents:
            for i, result in enumerate(results):
                if result["success"]:
                    try:
                        doc = documents_service.create_document(
                            title=result["title"],
                            content=result["content"],
                            document_type=DocumentType.WEBPAGE,
                            metadata=result["metadata"],
                            tags=request.document_tags or [],
                            source_url=result["url"],
                        )
                        results[i]["document_id"] = doc["id"]
                    except Exception as e:
                        results[i]["error"] = f"Document creation failed: {str(e)}"
        return results
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Scraping failed: {str(e)}") from e


@router.post(
    "/crawl",
    response_model=Dict[str, Any],
    summary="Crawl website",
    description="Crawl a website starting from a URL",
)
async def crawl_website(request: ScrapeCrawlRequest = Body(...)):
    """
    Crawl a website starting from a URL.
    Follows links up to a specified depth and processes each page.
    Optional verification pass ensures content stability.
    """
    try:
        results = await scraper_service.crawl_website(
            start_url=request.start_url,
            max_pages=request.max_pages,
            recursion_depth=request.recursion_depth,
            allowed_domains=request.allowed_domains,
            verification_pass=request.verification_pass,
        )
        response = {
            "pages_crawled": results.get("pages_crawled", 0),
            "start_url": request.start_url,
            "success_count": results.get("success_count", 0),
            "failed_count": results.get("failed_count", 0),
        }
        # Include verification results if available
        if "verification_results" in results:
            response["verification_results"] = results["verification_results"]
            response["verification_success_rate"] = results["verification_success_rate"]
        # If requested, create documents
        if request.create_documents:
            document_ids = []
            for result in results.get("results", []):
                if result.get("success", False):
                    try:
                        doc = documents_service.create_document(
                            title=result["title"],
                            content=result["content"],
                            document_type=DocumentType.WEBPAGE,
                            metadata=result["metadata"],
                            tags=request.document_tags or [],
                            source_url=result["url"],
                        )
                        document_ids.append(doc["id"])
                    except Exception:
                        pass
            response["documents_created"] = len(document_ids)
            response["document_ids"] = document_ids
        return response
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Crawling failed: {str(e)}") from e


@router.post(
    "/search",
    response_model=List[ScraperResponse],
    summary="Search and scrape",
    description="Search for content and scrape the results (legacy endpoint)",
    deprecated=True
)
async def search_and_scrape(request: SearchAndScrapeRequest = Body(...)):
    """
    Legacy search and scrape endpoint.
    Please use /serper/search for improved search capabilities.
    """
    try:
        results = await scraper_service.search_and_scrape(request.query, request.max_results)
        # If requested, create documents
        if request.create_documents:
            for i, result in enumerate(results):
                if result.get("success", False):
                    try:
                        doc = documents_service.create_document(
                            title=result["title"],
                            content=result["content"],
                            document_type=DocumentType.WEBPAGE,
                            metadata=result["metadata"],
                            tags=request.document_tags or [],
                            source_url=result["url"],
                        )
                        results[i]["document_id"] = doc["id"]
                    except Exception:
                        pass
        return results
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Search and scrape failed: {str(e)}") from e


@router.post(
    "/screenshot",
    response_model=Dict[str, Any],
    summary="Capture screenshot",
    description="Capture screenshot of a webpage",
)
async def capture_screenshot(
    url: str = Body(..., embed=True), full_page: bool = Body(True, embed=True)
):
    """
    Capture a screenshot of a URL.
    Returns the path to the saved screenshot file.
    """
    try:
        result = await scraper_service.capture_screenshot(url, full_page)
        if not result["success"]:
            raise HTTPException(status_code=500, detail=result["error"])
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Screenshot failed: {str(e)}") from e


@router.post(
    "/enhanced-search",
    response_model=Dict[str, Any],
    summary="Enhanced search with Serper API",
    description="Search the web using Serper API and optionally scrape results"
)
async def enhanced_search(request: SerperSearchRequest = Body(...)):
    """
    Search the web using Serper API and optionally scrape results.
    Returns rich search results and can automatically scrape and create documents.
    """
    try:
        # First get search results
        results = await scraper_service.enhanced_search_and_scrape(
            query=request.query,
            search_type=request.search_type,
            num_results=request.num_results,
            max_scrape=request.max_scrape if request.auto_scrape else 0,
            country=request.country,
            locale=request.locale
        )
        
        # If requested, create documents from scraped content
        if request.create_documents and request.auto_scrape and results.get("scraped_content"):
            document_ids = []
            for i, result in enumerate(results["scraped_content"]):
                if result.get("success", False):
                    try:
                        doc = documents_service.create_document(
                            title=result["title"],
                            content=result["content"],
                            document_type=DocumentType.WEBPAGE,
                            metadata=result["metadata"],
                            tags=request.document_tags or [],
                            source_url=result["url"],
                        )
                        results["scraped_content"][i]["document_id"] = doc["id"]
                        document_ids.append(doc["id"])
                    except Exception as e:
                        logger.error(f"Error creating document: {str(e)}")
            
            results["document_ids"] = document_ids
        
        return results
    except Exception as e:
        logger.error(f"Enhanced search failed: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Enhanced search failed: {str(e)}") from e


@router.get(
    "/search-status",
    response_model=Dict[str, Any],
    summary="Check search API status",
    description="Check if the Serper API is configured and available"
)
async def search_status():
    """Check if Serper API is configured and available."""
    try:
        if not serper_service.api_key:
            return {
                "status": "not_configured",
                "message": "Search API key is not configured",
                "provider": "serper.dev"
            }
        
        # Try a simple search to verify API works
        test_results = await serper_service.search("test", num_results=1)
        
        if "error" in test_results:
            return {
                "status": "error",
                "message": f"Search API error: {test_results['error']}",
                "provider": "serper.dev"
            }
        
        return {
            "status": "available",
            "message": "Search API is configured and working",
            "provider": "serper.dev"
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"Error checking search API: {str(e)}",
            "provider": "serper.dev"
        }


================================================================================
FILE: app/core/__init__.py
LANGUAGE: python
SIZE: 411 bytes
================================================================================

"""
Core Services for the Unified Tools Server

This package contains the core service implementations:
- FilesystemService: File operations for local and S3 storage
- GitService: Git repository operations and version control
- MemoryService: Knowledge graph and entity relation management
- DocumentsService: Document storage and retrieval
- ScraperService: Web content extraction and processing
"""


================================================================================
FILE: app/core/documents_service.py
LANGUAGE: python
SIZE: 25788 bytes
================================================================================

import os
import io
import hashlib
import json
import tempfile
import time
import threading
import random
import uuid
import re
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Union, Set, Tuple
from fastapi import HTTPException
from git import Repo
from pydantic import BaseModel, Field
from app.models.documents import (
    DocumentType,
    CreateDocumentRequest,
    UpdateDocumentRequest,
    DocumentResponse,
    DocumentVersionResponse,
    DocumentContentResponse,
)
from app.core.filesystem_service import FilesystemService
from app.core.memory_service import MemoryService
from app.core.git_service import GitService
from app.utils.config import get_config

logger = logging.getLogger(__name__)

try:
    import markdown
except ImportError:
    markdown = None
    logger.warning("Markdown package not installed. Markdown to HTML conversion will be limited.")


class DocumentsService:
    def __init__(self, base_path: str = None, large_content_threshold: int = 100000):
        """Initialize the document service"""
        config = get_config()

        self.base_path = Path(base_path or os.path.join(os.getcwd(), "data", "documents"))
        self.base_path.mkdir(parents=True, exist_ok=True)

        self.repo_path = str(self.base_path)
        self.git_service = GitService()

        for doc_type in DocumentType:
            (self.base_path / doc_type.value).mkdir(exist_ok=True)

        readme_path = self.base_path / "README.md"
        if not readme_path.exists():
            with open(readme_path, "w", encoding="utf-8") as f:
                f.write("# Document Storage\n\n")
                f.write(
                    "This directory contains documents managed by the Tools Server Document Service.\n"
                )
                f.write(
                    "It supports various document types including manuscripts (stories, novels),\n"
                )
                f.write("documentation (technical, research), and datasets.")
                f.write(
                    "\n\nLarge documents (70,000+ words) are automatically chunked for better performance.\n"
                )

            self.git_service.add_files(self.repo_path, ["README.md"])
            self.git_service.commit_changes(self.repo_path, "Initialize document repository")

        self.large_content_threshold = large_content_threshold

        self.index_dir = self.base_path / ".index"
        self.index_dir.mkdir(exist_ok=True)

        self.vector_search_enabled = False
        self.vector_model = None

        try:
            import numpy as np
            from sentence_transformers import SentenceTransformer

            self.np = np
            self.vector_model = SentenceTransformer("all-MiniLM-L6-v2")
            self.vector_search_enabled = True
            self.vector_index_path = self.base_path / ".vectors"
            self.vector_index_path.mkdir(exist_ok=True)
        except ImportError:
            logger.warning("Vector search dependencies not installed. Semantic search disabled.")

        self.index_lock = threading.Lock()
        self.file_locks = {}

    def _get_file_lock(self, doc_id):
        """Get a lock for a specific file to prevent concurrent modifications"""
        if doc_id not in self.file_locks:
            self.file_locks[doc_id] = threading.Lock()
        return self.file_locks[doc_id]

    def create_document(
        self,
        title: str,
        content: str,
        document_type: DocumentType,
        metadata: Dict[str, Any] = None,
        tags: List[str] = None,
        source_url: Optional[str] = None,
        storage_type: str = "local",
    ) -> Dict[str, Any]:
        """Create a new document with Git versioning"""
        doc_id = f"doc_{int(time.time())}_{uuid.uuid4().hex[:8]}"
        now = int(time.time())

        metadata = metadata or {}
        tags = tags or []

        doc_dir = self.base_path / document_type.value
        doc_path = doc_dir / f"{doc_id}.md"

        frontmatter = f"---\ntitle: {title}\ncreated_at: {now}\nupdated_at: {now}\nid: {doc_id}\ndocument_type: {document_type.value}\n"

        if tags:
            frontmatter += f"tags: {', '.join(tags)}\n"

        if source_url:
            frontmatter += f"source_url: {source_url}\n"

        for key, value in metadata.items():
            if isinstance(value, (str, int, float, bool)):
                frontmatter += f"{key}: {value}\n"

        frontmatter += "---\n\n"
        full_content = frontmatter + content

        doc_path.write_text(full_content, encoding="utf-8")

        self._update_index(
            doc_id,
            {
                "id": doc_id,
                "title": title,
                "document_type": document_type.value,
                "created_at": now,
                "updated_at": now,
                "tags": tags,
                "metadata": metadata,
                "size_bytes": len(full_content.encode("utf-8")),
                "source_url": source_url,
                "path": str(doc_path.relative_to(self.base_path)),
            },
        )

        rel_path = doc_path.relative_to(self.base_path)
        self.git_service.add_files(self.repo_path, [str(rel_path)])
        self.git_service.commit_changes(self.repo_path, f"Created document: {title}")

        self._update_memory_graph(doc_id, title, document_type.value, tags, metadata, source_url)

        self.generate_embeddings(doc_id, content)

        return self.get_document(doc_id)

    def update_document(
        self,
        doc_id: str,
        title: Optional[str] = None,
        content: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
        tags: Optional[List[str]] = None,
        commit_message: str = "Updated document",
        expected_version: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Update an existing document with version control"""
        with self._get_file_lock(doc_id):
            doc_info = self._get_document_index(doc_id)
            if not doc_info:
                raise ValueError(f"Document with ID {doc_id} not found")

            if expected_version:
                current_version = None
                try:
                    log = self.git_service.get_log(
                        self.repo_path, max_count=1, file_path=doc_info["path"]
                    )
                    if log.get("commits"):
                        current_version = log["commits"][0]["hash"]
                    if current_version and current_version != expected_version:
                        raise ValueError(
                            "Document has been modified since you loaded it. Please refresh and try again."
                        )
                except Exception as e:
                    logger.warning(f"Version check failed: {e}")

            doc_path = self.base_path / doc_info["path"]
            if not doc_path.exists():
                raise ValueError(f"Document file not found at {doc_path}")

            current_content = doc_path.read_text(encoding="utf-8")

            frontmatter_match = re.match(r"---(.*?)---\n\n", current_content, re.DOTALL)
            if not frontmatter_match:
                raise ValueError("Invalid document format: missing frontmatter")

            frontmatter = frontmatter_match.group(1)
            existing_content = current_content[frontmatter_match.end() :]

            frontmatter_dict = {}
            for line in frontmatter.strip().split("\n"):
                if ": " in line:
                    key, value = line.split(": ", 1)
                    frontmatter_dict[key] = value

            now = int(time.time())
            frontmatter_dict["updated_at"] = str(now)

            if title:
                frontmatter_dict["title"] = title

            if tags is not None:
                if tags:
                    frontmatter_dict["tags"] = ", ".join(tags)
                else:
                    if "tags" in frontmatter_dict:
                        del frontmatter_dict["tags"]

            if metadata:
                for key, value in metadata.items():
                    if isinstance(value, (str, int, float, bool)):
                        frontmatter_dict[key] = str(value)

            new_frontmatter = "---\n"
            for key, value in frontmatter_dict.items():
                new_frontmatter += f"{key}: {value}\n"
            new_frontmatter += "---\n\n"

            final_content = content if content is not None else existing_content
            full_content = new_frontmatter + final_content

            doc_path.write_text(full_content, encoding="utf-8")

            doc_info_update = {
                "updated_at": now,
                "size_bytes": len(full_content.encode("utf-8")),
            }

            if title:
                doc_info_update["title"] = title

            if tags is not None:
                doc_info_update["tags"] = tags

            if metadata:
                doc_info_update["metadata"] = {**doc_info.get("metadata", {}), **metadata}

            with self.index_lock:
                self._update_index(doc_id, doc_info_update)

            rel_path = doc_path.relative_to(self.base_path)
            self.git_service.add_files(self.repo_path, [str(rel_path)])
            self.git_service.commit_changes(self.repo_path, commit_message)

            self._update_memory_graph(
                doc_id,
                title or doc_info.get("title"),
                doc_info.get("document_type"),
                tags if tags is not None else doc_info.get("tags", []),
                {**doc_info.get("metadata", {}), **(metadata or {})},
                doc_info.get("source_url"),
            )

            if content is not None:
                self.generate_embeddings(doc_id, content)

        return self.get_document(doc_id)

    def get_document(self, doc_id: str) -> Dict[str, Any]:
        """Get document metadata and preview"""
        doc_info = self._get_document_index(doc_id)
        if not doc_info:
            return None

        doc_path = self.base_path / doc_info["path"]
        if not doc_path.exists():
            return None

        content = doc_path.read_text(encoding="utf-8")

        content_without_frontmatter = re.sub(r"^---.*?---\n\n", "", content, flags=re.DOTALL)
        preview = content_without_frontmatter[:500] + (
            "..." if len(content_without_frontmatter) > 500 else ""
        )

        version_count = 1
        try:
            log = self.git_service.get_log(
                self.repo_path, max_count=100, file_path=doc_info["path"]
            )
            version_count = len(log.get("commits", []))
        except Exception:
            pass

        return {
            "id": doc_id,
            "title": doc_info.get("title", "Untitled"),
            "document_type": doc_info.get("document_type", DocumentType.GENERIC.value),
            "created_at": doc_info.get("created_at", 0),
            "updated_at": doc_info.get("updated_at", 0),
            "tags": doc_info.get("tags", []),
            "metadata": doc_info.get("metadata", {}),
            "content_preview": preview,
            "size_bytes": doc_info.get("size_bytes", 0),
            "version_count": version_count,
            "content_available": True,
            "source_url": doc_info.get("source_url"),
        }

    def get_document_content(self, doc_id: str, version: Optional[str] = None) -> Dict[str, Any]:
        """Get full document content, optionally from a specific version"""
        doc_info = self._get_document_index(doc_id)
        if not doc_info:
            return None

        doc_path = self.base_path / doc_info["path"]

        content = ""
        if version:
            try:
                content = self.git_service.get_file_content_at_version(
                    self.repo_path, doc_info["path"], version
                )
            except Exception:
                return None
        else:
            if not doc_path.exists():
                return None
            content = doc_path.read_text(encoding="utf-8")

        content_without_frontmatter = re.sub(r"^---.*?---\n\n", "", content, flags=re.DOTALL)
        return {
            "id": doc_id,
            "title": doc_info.get("title", "Untitled"),
            "content": content_without_frontmatter,
            "version": version,
        }

    def get_document_versions(self, doc_id: str, max_versions: int = 10) -> List[Dict[str, Any]]:
        """Get version history for a document"""
        doc_info = self._get_document_index(doc_id)
        if not doc_info:
            return []

        try:
            log = self.git_service.get_log(
                self.repo_path, max_count=max_versions, file_path=doc_info["path"]
            )
            versions = []
            for commit in log.get("commits", []):
                versions.append(
                    {
                        "version_hash": commit["hash"],
                        "commit_message": commit["message"],
                        "author": commit["author"],
                        "timestamp": int(
                            time.mktime(time.strptime(commit["date"], "%Y-%m-%d %H:%M:%S %z"))
                        ),
                    }
                )
            return versions
        except Exception as e:
            logger.error(f"Error getting versions: {e}", exc_info=True)
            return []

    def delete_document(self, doc_id: str) -> bool:
        """Delete a document"""
        doc_info = self._get_document_index(doc_id)
        if not doc_info:
            return False

        doc_path = self.base_path / doc_info["path"]
        if not doc_path.exists():
            return False

        try:
            doc_path.unlink()

            rel_path = doc_path.relative_to(self.base_path)
            self.git_service.remove_file(self.repo_path, str(rel_path))
            self.git_service.commit_changes(
                self.repo_path, f"Deleted document: {doc_info.get('title', doc_id)}"
            )

            self._remove_index(doc_id)

            self._remove_from_memory_graph(doc_id)

            return True
        except Exception as e:
            logger.error(f"Error deleting document: {e}", exc_info=True)
            return False

    def search_documents(
        self,
        query: str,
        doc_type: Optional[str] = None,
        tags: Optional[List[str]] = None,
        limit: int = 10,
    ) -> List[Dict[str, Any]]:
        """Search documents by query, type, and tags"""
        results = []

        index_files = list(self.index_dir.glob("*.json"))

        for index_file in index_files:
            try:
                doc_info = json.loads(index_file.read_text(encoding="utf-8"))

                if doc_type and doc_info.get("document_type") != doc_type:
                    continue

                if tags:
                    doc_tags = set(doc_info.get("tags", []))
                    if not all(tag in doc_tags for tag in tags):
                        continue

                if query:
                    query_lower = query.lower()
                    title = doc_info.get("title", "").lower()
                    doc_content = ""

                    if query and not (query_lower in title):
                        try:
                            doc_path = self.base_path / doc_info["path"]
                            content = doc_path.read_text(encoding="utf-8")
                            doc_content = re.sub(
                                r"^---.*?---\n\n", "", content, flags=re.DOTALL
                            ).lower()
                        except Exception:
                            pass

                    if not (query_lower in title or query_lower in doc_content):
                        continue

                results.append(
                    {
                        "id": doc_info.get("id"),
                        "title": doc_info.get("title", "Untitled"),
                        "document_type": doc_info.get("document_type", DocumentType.GENERIC.value),
                        "created_at": doc_info.get("created_at", 0),
                        "updated_at": doc_info.get("updated_at", 0),
                        "tags": doc_info.get("tags", []),
                        "metadata": doc_info.get("metadata", {}),
                        "size_bytes": doc_info.get("size_bytes", 0),
                        "source_url": doc_info.get("source_url"),
                    }
                )

                if len(results) >= limit:
                    break

            except Exception as e:
                logger.error(f"Error processing document index {index_file}: {e}", exc_info=True)

        return results

    def _update_index(self, doc_id: str, doc_info: Dict[str, Any]) -> None:
        """Update the document index"""
        with self.index_lock:
            index_path = self.index_dir / f"{doc_id}.json"

            if index_path.exists():
                current_info = json.loads(index_path.read_text(encoding="utf-8"))
                current_info.update(doc_info)
                index_path.write_text(json.dumps(current_info, indent=2), encoding="utf-8")
            else:
                index_path.write_text(json.dumps(doc_info, indent=2), encoding="utf-8")

    def _get_document_index(self, doc_id: str) -> Optional[Dict[str, Any]]:
        """Get document index by ID"""
        index_path = self.index_dir / f"{doc_id}.json"
        if not index_path.exists():
            return None

        try:
            return json.loads(index_path.read_text(encoding="utf-8"))
        except Exception:
            return None

    def _remove_index(self, doc_id: str) -> None:
        """Remove document index"""
        index_path = self.index_dir / f"{doc_id}.json"
        if index_path.exists():
            index_path.unlink()

    def _update_memory_graph(
        self,
        doc_id: str,
        title: str,
        doc_type: str,
        tags: List[str],
        metadata: Dict[str, Any],
        source_url: Optional[str] = None,
    ) -> None:
        """Update the knowledge graph with document references"""
        try:
            doc_entity_name = f"document:{doc_id}"

            observations = [
                f"Title: {title}",
                f"Type: {doc_type}",
            ]

            if tags:
                observations.append(f"Tags: {', '.join(tags)}")

            if source_url:
                observations.append(f"Source URL: {source_url}")

            for key, value in metadata.items():
                if isinstance(value, (str, int, float, bool)):
                    observations.append(f"{key}: {value}")

            self.memory_service.create_entities(
                [{"name": doc_entity_name, "entity_type": "document", "observations": observations}]
            )

            relations = []
            for tag in tags:
                tag_entity_name = f"tag:{tag}"
                self.memory_service.create_entities(
                    [
                        {
                            "name": tag_entity_name,
                            "entity_type": "tag",
                            "observations": [f"Document tag: {tag}"],
                        }
                    ]
                )

                relations.append(
                    {"from": doc_entity_name, "to": tag_entity_name, "relation_type": "tagged_with"}
                )

            if source_url:
                source_entity_name = f"source:{source_url.replace('://', '_').replace('/', '_')}"
                self.memory_service.create_entities(
                    [
                        {
                            "name": source_entity_name,
                            "entity_type": "source",
                            "observations": [f"URL: {source_url}"],
                        }
                    ]
                )

                relations.append(
                    {
                        "from": doc_entity_name,
                        "to": source_entity_name,
                        "relation_type": "sourced_from",
                    }
                )

            if relations:
                self.memory_service.create_relations(relations)

        except Exception as e:
            logger.error(f"Error updating memory graph: {e}", exc_info=True)

    def _remove_from_memory_graph(self, doc_id: str) -> None:
        """Remove document from knowledge graph"""
        try:
            doc_entity_name = f"document:{doc_id}"
            self.memory_service.delete_entities([doc_entity_name])
        except Exception as e:
            logger.error(f"Error removing document from memory graph: {e}", exc_info=True)

    def generate_embeddings(self, doc_id: str, content: str) -> None:
        """Generate and store embeddings for a document"""
        if not self.vector_search_enabled:
            return

        try:
            embedding = self.vector_model.encode(content[:10000])

            embedding_path = self.vector_index_path / f"{doc_id}.npy"
            self.np.save(embedding_path, embedding)
        except Exception as e:
            logger.error(f"Error generating embeddings for document {doc_id}: {str(e)}")

    def semantic_search(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """Search documents semantically using vector similarity"""
        if not self.vector_search_enabled:
            raise ValueError("Vector search not enabled")

        try:
            query_embedding = self.vector_model.encode(query)

            results = []
            for embedding_file in self.vector_index_path.glob("*.npy"):
                doc_id = embedding_file.stem
                doc_embedding = self.np.load(embedding_file)

                similarity = self.np.dot(query_embedding, doc_embedding) / (
                    self.np.linalg.norm(query_embedding) * self.np.linalg.norm(doc_embedding)
                )

                results.append((doc_id, float(similarity)))

            results.sort(key=lambda x: x[1], reverse=True)
            top_results = results[:limit]

            return [self.get_document(doc_id) for doc_id, _ in top_results]
        except Exception as e:
            logger.error(f"Error during semantic search: {str(e)}")
            return []

    def convert_document_format(self, doc_id: str, target_format: str) -> bytes:
        """Convert document to different formats (PDF, DOCX, etc.)"""
        doc_content = self.get_document_content(doc_id)
        if not doc_content:
            raise ValueError(f"Document {doc_id} not found")

        content = doc_content["content"]
        title = doc_content["title"]

        if target_format.lower() == "pdf":
            try:
                if markdown is None:
                    raise ImportError("markdown package is required for PDF conversion")

                try:
                    import weasyprint
                except ImportError:
                    raise ImportError("weasyprint package is required for PDF conversion")

                html_content = f"<h1>{title}</h1>{markdown.markdown(content)}"
                pdf_bytes = weasyprint.HTML(string=html_content).write_pdf()
                return pdf_bytes
            except ImportError as e:
                logger.error(f"{e}")
                raise ValueError(str(e))

        elif target_format.lower() == "docx":
            try:
                try:
                    from docx import Document
                except ImportError:
                    raise ImportError("python-docx package is required for DOCX conversion")

                doc = Document()
                doc.add_heading(title, 0)
                doc.add_paragraph(content)

                buffer = io.BytesIO()
                doc.save(buffer)
                buffer.seek(0)
                return buffer.read()
            except ImportError as e:
                logger.error(f"{e}")
                raise ValueError(str(e))
        else:
            raise ValueError(f"Unsupported format: {target_format}")

    def get_document_diff(
        self, doc_id: str, from_version: str, to_version: str = "HEAD"
    ) -> Dict[str, Any]:
        """Get differences between document versions"""
        doc_info = self._get_document_index(doc_id)
        if not doc_info:
            raise ValueError(f"Document with ID {doc_id} not found")

        try:
            diff = self.git_service.get_diff(
                self.repo_path, doc_info["path"], from_version, to_version
            )

            return {
                "id": doc_id,
                "title": doc_info.get("title", "Untitled"),
                "from_version": from_version,
                "to_version": to_version,
                "diff": diff,
            }
        except Exception as e:
            logger.error(f"Error getting document diff: {e}", exc_info=True)
            raise ValueError(f"Error getting document diff: {e}")


================================================================================
FILE: app/core/filesystem_service.py
LANGUAGE: python
SIZE: 15842 bytes
================================================================================

import os
import pathlib
import logging
import hashlib
from typing import Optional, List, Dict, Any
import time
import threading
import glob
import shutil
import fnmatch
from app.utils.config import get_config

# Third-party imports in try/except for graceful handling
try:
    import boto3
    HAS_BOTO3 = True
except ImportError:
    boto3 = None
    HAS_BOTO3 = False

logger = logging.getLogger(__name__)

class FilesystemService:
    def __init__(self):
        config = get_config()
        self.allowed_directories = [str(pathlib.Path(os.path.expanduser(d)).resolve())
                                   for d in config.allowed_directories]
        self.s3_client = None
        self.s3_resource = None
        
        if config.s3_access_key and config.s3_secret_key:
            try:
                if not HAS_BOTO3 or boto3 is None:
                    logger.warning("boto3 is not installed. S3 functionality will be disabled.")
                else:
                    self.s3_client = boto3.client(
                        's3',
                        aws_access_key_id=config.s3_access_key,
                        aws_secret_access_key=config.s3_secret_key,
                        region_name=config.s3_region
                    )
                    self.s3_resource = boto3.resource(
                        's3',
                        aws_access_key_id=config.s3_access_key,
                        aws_secret_access_key=config.s3_secret_key,
                        region_name=config.s3_region
                    )
                    logger.info("S3 client initialized successfully")
            except Exception as e:
                logger.error("Failed to initialize S3 client: %s", e)
                
        self.cache_enabled = hasattr(config, 'file_cache_enabled') and config.file_cache_enabled
        self.cache_dir = pathlib.Path("./cache")
        self.cache_max_age = 3600
        self.cache = {}
        self.cache_lock = threading.Lock()
        
        if self.cache_enabled:
            self.cache_dir.mkdir(exist_ok=True)
            logger.info("File caching enabled")

    def normalize_path(self, requested_path: str) -> pathlib.Path:
        if not requested_path:
            raise ValueError("Empty path not allowed")
        requested = pathlib.Path(os.path.expanduser(requested_path)).resolve()
        
        for allowed in self.allowed_directories:
            if str(requested).startswith(allowed):
                return requested
        raise ValueError(f"Access denied: {requested} is outside allowed directories.")

    def _cache_key(self, path: str, storage: str = "local", bucket: Optional[str] = None) -> str:
        key_parts = [storage, path]
        if bucket:
            key_parts.append(bucket)
        key_string = ":".join(key_parts)
        return hashlib.md5(key_string.encode()).hexdigest()

    def read_file(self, path: str, storage: str = "local", bucket: Optional[str] = None) -> str:
        if storage == "local":
            file_path = self.normalize_path(path)
            return file_path.read_text(encoding="utf-8")
        elif storage == "s3":
            if not bucket:
                raise ValueError("S3 bucket name is required for S3 storage")
            if not self.s3_client:
                raise ValueError("S3 client not configured")
            response = self.s3_client.get_object(Bucket=bucket, Key=path)
            return response['Body'].read().decode('utf-8')
        else:
            raise ValueError(f"Unsupported storage type: {storage}")

    def read_file_cached(self, path: str, max_age: Optional[int] = None,
                        storage: str = "local", bucket: Optional[str] = None) -> str:
        if not self.cache_enabled:
            return self.read_file(path, storage, bucket)
        cache_key = self._cache_key(path, storage, bucket)
        cache_max_age = self.cache_max_age if max_age is None else max_age
        
        with self.cache_lock:
            if cache_key in self.cache:
                entry = self.cache[cache_key]
                if time.time() - entry["timestamp"] < cache_max_age:
                    logger.debug("Cache hit for %s", path)
                    return entry["content"]
        
        content = self.read_file(path, storage, bucket)
        
        with self.cache_lock:
            self.cache[cache_key] = {
                "content": content,
                "timestamp": time.time()
            }
            cache_file = self.cache_dir / cache_key
            try:
                with open(cache_file, 'w', encoding='utf-8') as f:
                    f.write(content)
            except Exception as e:
                logger.warning("Failed to write to disk cache: %s", e)
        return content

    def read_file_binary(self, path: str, storage: str = "local", bucket: Optional[str] = None) -> bytes:
        if storage == "local":
            file_path = self.normalize_path(path)
            if not file_path.exists():
                raise ValueError(f"File not found: {path}")
            if file_path.is_dir():
                raise ValueError(f"Path is a directory, not a file: {path}")
            return file_path.read_bytes()
        elif storage == "s3":
            if not bucket:
                raise ValueError("S3 bucket name is required for S3 storage")
            if not self.s3_client:
                raise ValueError("S3 client not configured")
            response = self.s3_client.get_object(Bucket=bucket, Key=path)
            return response['Body'].read()
        else:
            raise ValueError(f"Unsupported storage type: {storage}")

    def write_file(self, path: str, content: str, storage: str = "local", bucket: Optional[str] = None) -> str:
        if storage == "local":
            file_path = self.normalize_path(path)
            file_path.parent.mkdir(parents=True, exist_ok=True)
            file_path.write_text(content, encoding="utf-8")
            return f"Successfully wrote to {path}"
        elif storage == "s3":
            if not bucket:
                raise ValueError("S3 bucket name is required for S3 storage")
            if not self.s3_client:
                raise ValueError("S3 client not configured")
            self.s3_client.put_object(
                Bucket=bucket,
                Key=path,
                Body=content.encode('utf-8'),
                ContentType='text/plain'
            )
            return f"Successfully wrote to s3://{bucket}/{path}"
        else:
            raise ValueError(f"Unsupported storage type: {storage}")

    def write_file_binary(self, path: str, content: bytes,
                         storage: str = "local", bucket: Optional[str] = None) -> str:
        if storage == "local":
            file_path = self.normalize_path(path)
            file_path.parent.mkdir(parents=True, exist_ok=True)
            with open(file_path, "wb") as f:
                f.write(content)
            return f"Successfully wrote to {path}"
        elif storage == "s3":
            if not bucket:
                raise ValueError("S3 bucket name is required for S3 storage")
            if not self.s3_client:
                raise ValueError("S3 client not configured")
            self.s3_client.put_object(Bucket=bucket, Key=path, Body=content)
            return f"Successfully wrote to s3://{bucket}/{path}"
        else:
            raise ValueError(f"Unsupported storage type: {storage}")

    def create_directory(self, path: str, storage: str = "local", bucket: Optional[str] = None) -> str:
        if storage == "local":
            dir_path = self.normalize_path(path)
            dir_path.mkdir(parents=True, exist_ok=True)
            return f"Successfully created directory {path}"
        elif storage == "s3":
            if not bucket:
                raise ValueError("S3 bucket name is required for S3 storage")
            if not self.s3_client:
                raise ValueError("S3 client not configured")
            # S3 doesn't need explicit directory creation, but we'll add an empty marker
            self.s3_client.put_object(Bucket=bucket, Key=f"{path}/", Body=b'')
            return f"Successfully created directory s3://{bucket}/{path}/"
        else:
            raise ValueError(f"Unsupported storage type: {storage}")

    def delete_file(self, path: str, storage: str = "local", bucket: Optional[str] = None) -> str:
        if storage == "local":
            file_path = self.normalize_path(path)
            if file_path.exists():
                if file_path.is_file():
                    file_path.unlink()
                else:
                    shutil.rmtree(file_path)
                return f"Successfully deleted {path}"
            else:
                return f"File or directory does not exist: {path}"
        elif storage == "s3":
            if not bucket:
                raise ValueError("S3 bucket name is required for S3 storage")
            if not self.s3_client:
                raise ValueError("S3 client not configured")
            self.s3_client.delete_object(Bucket=bucket, Key=path)
            return f"Successfully deleted s3://{bucket}/{path}"
        else:
            raise ValueError(f"Unsupported storage type: {storage}")

    def search_files(self, directory: str, pattern: str, storage: str = "local", bucket: Optional[str] = None) -> List[str]:
        if storage == "local":
            dir_path = self.normalize_path(directory)
            if not dir_path.exists() or not dir_path.is_dir():
                raise ValueError(f"Directory does not exist: {directory}")
            
            matches = []
            for root, _, _ in os.walk(dir_path):
                for file in glob.glob(os.path.join(root, pattern)):
                    matches.append(file)
            return matches
        elif storage == "s3":
            if not bucket:
                raise ValueError("S3 bucket name is required for S3 storage")
            if not self.s3_client:
                raise ValueError("S3 client not configured")
            
            # Convert glob pattern to a prefix for S3
            prefix = directory.rstrip('/') + '/' if directory else ''
            response = self.s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)
            
            matches = []
            if 'Contents' in response:
                for obj in response['Contents']:
                    key = obj['Key']
                    if fnmatch.fnmatch(os.path.basename(key), pattern):
                        matches.append(key)
            return matches
        else:
            raise ValueError(f"Unsupported storage type: {storage}")

    def list_directory(self, directory: str, storage: str = "local", bucket: Optional[str] = None) -> Dict[str, Any]:
        if storage == "local":
            dir_path = self.normalize_path(directory)
            if not dir_path.exists():
                raise ValueError(f"Directory does not exist: {directory}")
            if not dir_path.is_dir():
                raise ValueError(f"Path is not a directory: {directory}")
            
            items = []
            for item in dir_path.iterdir():
                item_type = "directory" if item.is_dir() else "file"
                size = 0
                if item.is_file():
                    size = item.stat().st_size
                items.append({
                    "name": item.name,
                    "type": item_type,
                    "size": size,
                    "modified": item.stat().st_mtime
                })
            
            return {
                "path": str(dir_path),
                "items": items
            }
        elif storage == "s3":
            if not bucket:
                raise ValueError("S3 bucket name is required for S3 storage")
            if not self.s3_client:
                raise ValueError("S3 client not configured")
            
            prefix = directory.rstrip('/') + '/' if directory else ''
            response = self.s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix, Delimiter='/')
            
            items = []
            
            # Add directories (CommonPrefixes)
            if 'CommonPrefixes' in response:
                for prefix_obj in response['CommonPrefixes']:
                    prefix_name = prefix_obj['Prefix']
                    name = os.path.basename(prefix_name.rstrip('/'))
                    items.append({
                        "name": name,
                        "type": "directory",
                        "size": 0,
                        "modified": 0
                    })
            
            # Add files (Contents)
            if 'Contents' in response:
                for obj in response['Contents']:
                    key = obj['Key']
                    # Skip the directory itself or empty directory markers
                    if key == prefix or key.endswith('/'):
                        continue
                    name = os.path.basename(key)
                    items.append({
                        "name": name,
                        "type": "file",
                        "size": obj['Size'],
                        "modified": obj['LastModified'].timestamp() if hasattr(obj['LastModified'], 'timestamp') else 0
                    })
            
            return {
                "path": f"s3://{bucket}/{directory}",
                "items": items
            }
        else:
            raise ValueError(f"Unsupported storage type: {storage}")

    def file_exists(self, path: str, storage: str = "local", bucket: Optional[str] = None) -> bool:
        if storage == "local":
            file_path = self.normalize_path(path)
            return file_path.exists()
        elif storage == "s3":
            if not bucket:
                raise ValueError("S3 bucket name is required for S3 storage")
            if not self.s3_client:
                raise ValueError("S3 client not configured")
            
            try:
                self.s3_client.head_object(Bucket=bucket, Key=path)
                return True
            except Exception:
                # Check if it might be a directory by looking for objects with this prefix
                response = self.s3_client.list_objects_v2(
                    Bucket=bucket,
                    Prefix=path.rstrip('/') + '/',
                    MaxKeys=1
                )
                return 'Contents' in response and len(response['Contents']) > 0
        else:
            raise ValueError(f"Unsupported storage type: {storage}")

    def invalidate_cache(self, path: Optional[str] = None,
                        storage: str = "local", bucket: Optional[str] = None):
        if not self.cache_enabled:
            return
        with self.cache_lock:
            if path is not None:
                cache_key = self._cache_key(path, storage, bucket)
                if cache_key in self.cache:
                    del self.cache[cache_key]
                cache_file = self.cache_dir / cache_key
                if cache_file.exists():
                    cache_file.unlink()
            else:
                self.cache.clear()
                for cache_file in self.cache_dir.glob("*"):
                    try:
                        cache_file.unlink()
                    except Exception as e:
                        logger.warning("Failed to delete cache file %s: %s", cache_file, e)


================================================================================
FILE: app/core/git_service.py
LANGUAGE: python
SIZE: 13447 bytes
================================================================================

import os
import logging
import threading
from typing import Dict, List, Any, Optional

# Try to import git module at top level
try:
    import git
    HAS_GIT = True
except ImportError:
    git = None
    HAS_GIT = False

logger = logging.getLogger(__name__)


class GitService:
    """Service for Git operations with thread-safe repository access"""

    def __init__(
        self,
        default_author_name: str = "OtherTales",
        default_author_email: str = "system@othertales.com",
    ):
        """Initialize the Git service"""
        self.repo_locks = {}
        self.default_author_name = default_author_name
        self.default_author_email = default_author_email

        if not HAS_GIT:
            logger.error("GitPython is not installed. Git functionality will be limited.")
        self.git = git

    def _get_repo_lock(self, repo_path: str):
        """Get or create a lock for a specific repository path"""
        if repo_path not in self.repo_locks:
            self.repo_locks[repo_path] = threading.Lock()
        return self.repo_locks[repo_path]

    def _get_repo(self, repo_path: str):
        """Get Git repository, creating it if it doesn't exist"""
        if not self.git:
            raise ValueError("GitPython is not installed")
        if not os.path.exists(repo_path):
            os.makedirs(repo_path, exist_ok=True)
        try:
            repo = self.git.Repo(repo_path)
            return repo
        except self.git.InvalidGitRepositoryError:
            repo = self.git.Repo.init(repo_path)
            with repo.config_writer() as config:
                config.set_value("user", "name", self.default_author_name)
                config.set_value("user", "email", self.default_author_email)

            if not os.path.exists(os.path.join(repo_path, ".gitignore")):
                with open(os.path.join(repo_path, ".gitignore"), "w", encoding="utf-8") as f:
                    f.write("*.swp\n*.bak\n*.tmp\n*.orig\n*~\n")
                repo.git.add(".gitignore")
                repo.git.commit("-m", "Initial commit: Add .gitignore")
            return repo
        except Exception as e:
            logger.error("Error initializing repository: %s", e)
            raise ValueError(f"Failed to initialize repository: {e}") from e

    def get_status(self, repo_path: str) -> Dict[str, Any]:
        """Get the status of the git repository."""
        try:
            repo = self._get_repo(repo_path)
            
            status = {
                "branch": repo.active_branch.name,  # This field is needed for tests
                "current_branch": repo.active_branch.name,
                "clean": not repo.is_dirty(),
                "staged_files": [],
                "unstaged_files": [],
                "untracked_files": repo.untracked_files,
                "untracked": repo.untracked_files  # This field is needed for tests
            }
            
            # Get staged and unstaged changes
            for item in repo.index.diff(None):
                status["unstaged_files"].append(item.a_path)
                
            for item in repo.index.diff("HEAD"):
                status["staged_files"].append(item.a_path)
                
            return status
        except Exception as e:
            logger.error("Error getting git status: %s", e)
            raise

    def get_diff(
        self, repo_path: str, file_path: Optional[str] = None, target: Optional[str] = None
    ) -> str:
        """Get diff of changes"""
        repo = self._get_repo(repo_path)
        try:
            if file_path and target:
                return repo.git.diff(target, file_path)
            elif file_path:
                return repo.git.diff("HEAD", file_path)
            elif target:
                return repo.git.diff(target)
            else:
                return repo.git.diff()
        except Exception as e:
            logger.error("Error getting diff: %s", e)
            raise ValueError(f"Failed to get diff: {e}") from e

    def add_files(self, repo_path: str, files: List[str]) -> str:
        """Stage files for commit"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            try:
                repo.git.add(files)
                return "Files staged successfully"
            except Exception as e:
                logger.error("Error adding files: %s", e)
                raise ValueError(f"Failed to add files: {e}") from e

    def commit_changes(
        self,
        repo_path: str,
        message: str,
        author_name: Optional[str] = None,
        author_email: Optional[str] = None,
    ) -> str:
        """Commit staged changes"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            author_name = author_name or self.default_author_name
            author_email = author_email or self.default_author_email

            try:
                # Set author for this commit
                with repo.config_writer() as config:
                    config.set_value("user", "name", author_name)
                    config.set_value("user", "email", author_email)

                # Commit changes
                commit = repo.index.commit(message)
                return f"Committed changes with hash {commit.hexsha}"
            except Exception as e:
                logger.error("Error committing changes: %s", e)
                raise ValueError(f"Failed to commit changes: {e}") from e

    def reset_changes(self, repo_path: str) -> str:
        """Reset staged changes"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            try:
                repo.git.reset()
                return "All staged changes reset"
            except Exception as e:
                logger.error("Error resetting changes: %s", e)
                raise ValueError("Failed to reset changes: {}".format(e)) from e

    def get_log(
        self, repo_path: str, max_count: int = 10, file_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """Get commit log"""
        repo = self._get_repo(repo_path)
        try:
            commits_data = []
            if file_path:
                # Get log for specific file
                commits = list(repo.iter_commits(paths=file_path, max_count=max_count))
            else:
                # Get log for entire repo
                commits = list(repo.iter_commits(max_count=max_count))

            for commit in commits:
                commits_data.append(
                    {
                        "hash": commit.hexsha,
                        "author": f"{commit.author.name} <{commit.author.email}>",
                        "date": commit.committed_datetime.strftime("%Y-%m-%d %H:%M:%S %z"),
                        "message": commit.message.strip(),
                    }
                )

            return {"commits": commits_data}
        except Exception as e:
            logger.error("Error getting log: %s", e)
            raise ValueError(f"Failed to get log: {e}") from e

    def create_branch(
        self, repo_path: str, branch_name: str, base_branch: Optional[str] = None
    ) -> str:
        """Create a new branch"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            try:
                # Use the specified base branch or current branch
                if base_branch:
                    # Check if base branch exists
                    if base_branch not in repo.refs:
                        raise ValueError(f"Base branch '{base_branch}' does not exist")
                    base = repo.refs[base_branch]
                else:
                    base = repo.active_branch

                # Create new branch
                repo.create_head(branch_name, base)
                return f"Created branch '{branch_name}'"
            except Exception as e:
                logger.error("Error creating branch: %s", e)
                raise ValueError(f"Failed to create branch: {e}") from e

    def checkout_branch(self, repo_path: str, branch_name: str, create: bool = False) -> str:
        """Checkout a branch"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            try:
                # Check if branch exists
                branch_exists = branch_name in repo.refs

                # Create branch if it doesn't exist
                if create and not branch_exists:
                    repo.create_head(branch_name)
                elif not branch_exists:
                    raise ValueError(f"Branch '{branch_name}' does not exist")

                # Checkout the branch
                repo.git.checkout(branch_name)
                return f"Switched to branch '{branch_name}'"
            except Exception as e:
                logger.error("Error checking out branch: %s", e)
                raise ValueError(f"Failed to checkout branch: {e}") from e

    def clone_repo(self, repo_url: str, local_path: str, auth_token: Optional[str] = None) -> str:
        """Clone a Git repository"""
        if not self.git:
            raise ValueError("GitPython is not installed")

        try:
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(os.path.abspath(local_path)), exist_ok=True)

            # If auth token is provided, modify the URL
            if auth_token:
                if repo_url.startswith("https://"):
                    parsed_url = repo_url.replace(
                        "https://", f"https://x-access-token:{auth_token}@"
                    )
                    self.git.Repo.clone_from(parsed_url, local_path)
                else:
                    # For SSH or other protocols, use standard clone
                    self.git.Repo.clone_from(repo_url, local_path)
            else:
                self.git.Repo.clone_from(repo_url, local_path)

            return f"Successfully cloned repository to '{local_path}'"
        except Exception as e:
            logger.error("Error cloning repository: %s", e)
            raise ValueError(f"Failed to clone repository: {e}") from e

    def get_file_content_at_version(self, repo_path: str, file_path: str, version: str) -> str:
        """Get file content at a specific Git version"""
        repo = self._get_repo(repo_path)
        try:
            return repo.git.show(f"{version}:{file_path}")
        except Exception as e:
            logger.error("Error getting file content at version: %s", e)
            raise ValueError(f"Failed to get file content at version {version}: {e}") from e

    def get_diff_between_versions(
        self, repo_path: str, file_path: str, from_version: str, to_version: str = "HEAD"
    ) -> str:
        """Get the differences between two versions of a file"""
        repo = self._get_repo(repo_path)
        try:
            return repo.git.diff(from_version, to_version, "--", file_path)
        except Exception as e:
            logger.error("Error getting diff between versions: %s", e)
            raise ValueError(f"Failed to get diff between versions: {e}") from e

    def remove_file(self, repo_path: str, file_path: str) -> str:
        """Remove a file from the repository"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            try:
                # Check if file exists
                full_path = os.path.join(repo_path, file_path)
                if not os.path.exists(full_path):
                    return f"File does not exist: {file_path}"
                
                # Remove from git index
                repo.index.remove([file_path])
                
                # Optionally commit the removal
                # repo.index.commit(f"Removed {file_path}")
                
                return f"Successfully removed {file_path} from index"
            except Exception as e:
                logger.error("Error removing file: %s", e)
                raise ValueError(f"Failed to remove file: {e}") from e
                
    def batch_commit(
        self, repo_path: str, file_groups: List[List[str]], message_template: str
    ) -> List[str]:
        """Commit files in batches for better performance"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            try:
                commit_hashes = []
                for i, file_group in enumerate(file_groups):
                    # Add files in this group
                    repo.git.add(file_group)
                    
                    # Commit with a templated message
                    commit_message = f"{message_template} (batch {i+1}/{len(file_groups)})"
                    commit = repo.index.commit(commit_message)
                    commit_hashes.append(commit.hexsha)
                
                return commit_hashes
            except Exception as e:
                logger.error("Error in batch commit: %s", e)
                raise ValueError(f"Failed to perform batch commit: {e}") from e


================================================================================
FILE: app/core/memory_service.py
LANGUAGE: python
SIZE: 34647 bytes
================================================================================

import json
import os
import threading
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
import logging
import difflib

import networkx as nx

from app.models.memory import Entity, Relation, KnowledgeGraph
from app.utils.config import get_config

logger = logging.getLogger(__name__)

class MemoryService:
    def __init__(self, memory_file_path: Optional[str] = None):
        config = get_config()
        
        memory_file_path = memory_file_path or config.memory_file_path
        self.memory_file_path = Path(
            memory_file_path
            if Path(memory_file_path).is_absolute()
            else Path(os.getcwd()) / memory_file_path
        )
        
        self.memory_file_path.parent.mkdir(parents=True, exist_ok=True)
        
        self.user_prefs_dir = self.memory_file_path.parent / "user_preferences"
        self.user_prefs_dir.mkdir(exist_ok=True)
        
        self.use_graph_db = config.use_graph_db
        if self.use_graph_db:
            try:
                self.graph = nx.DiGraph()
                self._load_graph_from_file()
            except ImportError:
                logger.warning("NetworkX not installed. Graph database functionality will be limited.")
                self.use_graph_db = False

        self.entities = {}
        self.relations = []
        self.lock = threading.Lock()
        
        # Load existing data if available
        self._load_memory()

    def _read_graph_file(self) -> KnowledgeGraph:
        """Read the knowledge graph from disk"""
        if not self.memory_file_path.exists():
            return KnowledgeGraph(entities=[], relations=[])
        try:
            with open(self.memory_file_path, "r", encoding="utf-8") as f:
                lines = [line for line in f if line.strip()]
                entities = []
                relations = []
                for line in lines:
                    item = json.loads(line)
                    if item.get("type") == "entity":
                        entities.append(Entity(
                            name=item["name"],
                            entity_type=item["entity_type"],
                            observations=item.get("observations", [])
                        ))
                    elif item.get("type") == "relation":
                        relations.append(Relation(
                            **{k: v for k, v in item.items() if k != "type"}
                        ))
                return KnowledgeGraph(entities=entities, relations=relations)
        except Exception as e:
            print(f"Error reading graph file: {e}")
            return KnowledgeGraph(entities=[], relations=[])

    def _save_graph(self, graph: KnowledgeGraph):
        """Save the knowledge graph to disk"""
        lines = []
        # Save entities
        for e in graph.entities:
            entity_dict = e.dict()
            entity_dict["type"] = "entity"
            lines.append(json.dumps(entity_dict))
        # Save relations
        for r in graph.relations:
            relation_dict = r.dict(by_alias=True)
            relation_dict["type"] = "relation"
            lines.append(json.dumps(relation_dict))
        # Write to file
        with open(self.memory_file_path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))

    def create_entities(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Create new entities in the graph"""
        # Process entities through the legacy file-based system
        graph = self._read_graph_file()
        existing_names = {e.name for e in graph.entities}
        # Convert input dictionaries to Entity objects
        entity_objects = []
        for entity_dict in entities:
            if isinstance(entity_dict, dict):
                entity_objects.append(Entity(**entity_dict))
            else:
                entity_objects.append(entity_dict)
        # Filter out existing entities
        new_entities = [e for e in entity_objects if e.name not in existing_names]
        # Add new entities to graph
        graph.entities.extend(new_entities)
        self._save_graph(graph)
        
        # Also update the in-memory data
        with self.lock:
            for entity in entities:
                entity_name = entity.get('name')
                if not entity_name:
                    continue
                    
                entity_type = entity.get('entity_type', 'unknown')
                properties = entity.get('properties', {})
                
                # Add entity to memory
                self.entities[entity_name] = {
                    'entity_type': entity_type,
                    'properties': properties,
                    'created_at': time.time()
                }
                
                # Add to graph if using graph DB
                if self.use_graph_db:
                    self.graph.add_node(entity_name, 
                                      entity_type=entity_type, 
                                      **properties)
            
            # Save changes
            self._save_memory()
        
        # Return the added entities
        return [e.dict() for e in new_entities]

    def create_relations(self, relations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Create new relations in the graph"""
        # Process relations through the legacy file-based system
        graph = self._read_graph_file()
        # Get existing relations for deduplication
        existing_relations = {(r.from_, r.to, r.relation_type) for r in graph.relations}
        # Convert input dictionaries to Relation objects
        relation_objects = []
        for relation_dict in relations:
            if isinstance(relation_dict, dict):
                # Handle inconsistent field naming in input
                if "relation_type" in relation_dict and "relationType" not in relation_dict:
                    relation_dict["relationType"] = relation_dict["relation_type"]
                if "from_" in relation_dict and "from" not in relation_dict:
                    relation_dict["from"] = relation_dict["from_"]
                relation_objects.append(Relation(**relation_dict))
            else:
                relation_objects.append(relation_dict)
        # Filter out existing relations
        new_relations = [r for r in relation_objects
                        if (r.from_, r.to, r.relation_type) not in existing_relations]
        # Add new relations to graph
        graph.relations.extend(new_relations)
        self._save_graph(graph)
        
        # Also update the in-memory data
        created_relations = []
        
        with self.lock:
            for relation in relations:
                from_entity = relation.get('from')
                to_entity = relation.get('to')
                relation_type = relation.get('relation_type', 'related_to')
                properties = relation.get('properties', {})
                
                # Check if entities exist
                if from_entity not in self.entities or to_entity not in self.entities:
                    logger.warning(f"Cannot create relation: one or both entities don't exist ({from_entity}, {to_entity})")
                    continue
                
                # Create relation object
                relation_obj = {
                    'from': from_entity,
                    'to': to_entity,
                    'relation_type': relation_type,
                    'properties': properties,
                    'created_at': time.time()
                }
                
                # Add to relations list
                self.relations.append(relation_obj)
                
                # Add to graph if using graph DB
                if self.use_graph_db:
                    self.graph.add_edge(from_entity, to_entity, 
                                      relation_type=relation_type, 
                                      **properties)
                
                # Add to created list - use from_ to match test expectations
                created_relations.append({
                    'from_': from_entity,
                    'to': to_entity,
                    'relation_type': relation_type,
                    'properties': properties
                })
            
            # Save changes
            self._save_memory()
        
        # Return the results in the expected format for tests
        if not created_relations and new_relations:
            # If we have file-based results but no in-memory results, convert them
            return [r.dict(by_alias=True) for r in new_relations]
        return created_relations

    def add_observations(self, observations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Add observations to entities"""
        graph = self._read_graph_file()
        results = []
        # Process each observation item
        for obs_item in observations:
            entity_name = obs_item["entity_name"]
            contents = obs_item["contents"]
            # Find the entity
            entity = next((e for e in graph.entities if e.name == entity_name), None)
            if entity:
                # Get observations that are not already in the entity
                new_observations = [c for c in contents if c not in entity.observations]
                # Add new observations
                entity.observations.extend(new_observations)
                # Record result
                results.append({
                    "entity_name": entity_name,
                    "added_observations": new_observations
                })
        # Save the updated graph
        self._save_graph(graph)
        return results

    def delete_entities(self, entity_names: List[str]) -> Dict[str, int]:
        """Delete entities and their relations"""
        # Handle file-based graph
        graph = self._read_graph_file()
        # Remove entities
        initial_count = len(graph.entities)
        graph.entities = [e for e in graph.entities if e.name not in entity_names]
        file_entities_removed = initial_count - len(graph.entities)
        # Remove relations involving the deleted entities
        initial_relations_count = len(graph.relations)
        graph.relations = [r for r in graph.relations
                         if r.from_ not in entity_names and r.to not in entity_names]
        file_relations_removed = initial_relations_count - len(graph.relations)
        # Save the updated graph
        self._save_graph(graph)
        
        # Also handle in-memory data
        entities_removed = 0
        relations_removed = 0
        
        with self.lock:
            for name in entity_names:
                # Remove entity if it exists
                if name in self.entities:
                    del self.entities[name]
                    entities_removed += 1
                    
                    # Remove relations involving this entity
                    new_relations = []
                    for relation in self.relations:
                        if relation['from'] == name or relation['to'] == name:
                            relations_removed += 1
                        else:
                            new_relations.append(relation)
                    self.relations = new_relations
                    
                    # Remove from graph if using graph DB
                    if self.use_graph_db and name in self.graph:
                        self.graph.remove_node(name)
            
            # Save changes
            self._save_memory()
        
        # Return the results (prefer in-memory counts if available)
        return {
            "entities_removed": entities_removed or file_entities_removed,
            "relations_removed": relations_removed or file_relations_removed
        }

    def delete_relations(self, relations: List[Dict[str, Any]]) -> Dict[str, int]:
        """Delete specific relations"""
        # Handle file-based graph
        graph = self._read_graph_file()
        # Convert input dictionaries to relation tuples for comparison
        relation_tuples = []
        for relation in relations:
            from_entity = relation.get("from", relation.get("from_"))
            to_entity = relation.get("to")
            relation_type = relation.get("relation_type", relation.get("relationType"))
            if from_entity and to_entity and relation_type:
                relation_tuples.append((from_entity, to_entity, relation_type))
        # Remove matching relations
        initial_count = len(graph.relations)
        graph.relations = [r for r in graph.relations
                         if (r.from_, r.to, r.relation_type) not in relation_tuples]
        file_relations_removed = initial_count - len(graph.relations)
        # Save the updated graph
        self._save_graph(graph)
        
        # Also handle in-memory data
        relations_removed = 0
        
        with self.lock:
            for rel_to_delete in relations:
                from_entity = rel_to_delete.get('from')
                to_entity = rel_to_delete.get('to')
                relation_type = rel_to_delete.get('relation_type')
                
                # Filter out matching relations
                new_relations = []
                for existing_rel in self.relations:
                    if (existing_rel['from'] == from_entity and 
                        existing_rel['to'] == to_entity and 
                        existing_rel['relation_type'] == relation_type):
                        relations_removed += 1
                        
                        # Remove from graph if using graph DB
                        if self.use_graph_db:
                            # Check if edge exists before removing
                            if self.graph.has_edge(from_entity, to_entity):
                                self.graph.remove_edge(from_entity, to_entity)
                    else:
                        new_relations.append(existing_rel)
                
                self.relations = new_relations
            
            # Save changes
            self._save_memory()
        
        # Return the results (prefer in-memory count if available)
        return {
            "relations_removed": relations_removed or file_relations_removed
        }

    def search_nodes(self, query: str) -> KnowledgeGraph:
        """Search for nodes matching the query"""
        graph = self._read_graph_file()
        # Convert query to lowercase for case-insensitive search
        query_lower = query.lower()
        # Find entities matching the query
        matching_entities = []
        for entity in graph.entities:
            # Check name
            if query_lower in entity.name.lower():
                matching_entities.append(entity)
                continue
            # Check type
            if query_lower in entity.entity_type.lower():
                matching_entities.append(entity)
                continue
            # Check observations
            if any(query_lower in observation.lower() for observation in entity.observations):
                matching_entities.append(entity)
                continue
        # Get names of matching entities
        matching_names = {entity.name for entity in matching_entities}
        # Find relations between matching entities
        matching_relations = [r for r in graph.relations
                            if r.from_ in matching_names and r.to in matching_names]
        return KnowledgeGraph(entities=matching_entities, relations=matching_relations)

    def open_nodes(self, names: List[str]) -> KnowledgeGraph:
        """Retrieve specific nodes by name"""
        graph = self._read_graph_file()
        # Find the specified entities
        entities = [e for e in graph.entities if e.name in names]
        # Get entity names
        entity_names = {e.name for e in entities}
        # Find relations between these entities
        relations = [r for r in graph.relations
                   if r.from_ in entity_names and r.to in entity_names]
        return KnowledgeGraph(entities=entities, relations=relations)

    def get_user_preference(self, user_id: str) -> Dict[str, Any]:
        """Retrieve user preferences"""
        try:
            pref_file = self.user_prefs_dir / f"{user_id}.json"
            if not pref_file.exists():
                logger.debug("No preferences found for user %s", user_id)
                return {}
            with open(pref_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error("Error reading user preferences for %s: %s", user_id, e, exc_info=True)
            return {}

    def set_user_preference(self, user_id: str, preferences: Dict[str, Any]) -> Dict[str, Any]:
        """Store user preferences"""
        try:
            # Validate user_id to prevent path traversal
            if not user_id or '/' in user_id or '\\' in user_id or '..' in user_id:
                logger.error("Invalid user ID: %s", user_id)
                raise ValueError("Invalid user ID")
            pref_file = self.user_prefs_dir / f"{user_id}.json"
            # Merge with existing preferences if present
            existing_prefs = {}
            if pref_file.exists():
                try:
                    with open(pref_file, "r", encoding="utf-8") as f:
                        existing_prefs = json.load(f)
                except Exception as e:
                    logger.warning("Could not read existing preferences for %s: %s", user_id, e)
            # Update with new preferences
            existing_prefs.update(preferences)
            # Write back to file
            with open(pref_file, "w", encoding="utf-8") as f:
                json.dump(existing_prefs, f, ensure_ascii=False, indent=2)
            logger.info("Updated preferences for user %s", user_id)
            return existing_prefs
        except Exception as e:
            logger.error("Failed to set preferences for %s: %s", user_id, e, exc_info=True)
            raise

    def get_full_graph(self) -> KnowledgeGraph:
        """Get the entire knowledge graph"""
        return self._read_graph_file()

    def find_similar_entities(self, entity_name: str, threshold: float = 0.8) -> List[str]:
        """Find entities with similar names"""
        graph = self._read_graph_file()
        similar = []
        # Use difflib for fuzzy matching
        for entity in graph.entities:
            similarity = difflib.SequenceMatcher(None, entity_name.lower(), entity.name.lower()).ratio()
            if similarity >= threshold and entity_name != entity.name:
                similar.append(entity.name)
        return similar

    def _load_graph_from_file(self):
        """Load graph data from file into networkx graph"""
        try:
            knowledge_graph = self._read_graph_file()
            # Clear existing graph
            self.graph.clear()
            # Add all entities as nodes
            for entity in knowledge_graph.entities:
                self.graph.add_node(
                    entity.name,
                    entity_type=entity.entity_type,
                    observations=entity.observations
                )
            # Add all relations as edges
            for relation in knowledge_graph.relations:
                self.graph.add_edge(
                    relation.from_,
                    relation.to,
                    relation_type=relation.relation_type
                )
            logger.info("Loaded %d entities and %d relations into graph", 
                       len(knowledge_graph.entities), len(knowledge_graph.relations))
            return True
        except Exception as e:
            logger.error("Error loading graph from file: %s", e, exc_info=True)
            return False

    def find_paths(self, start_entity: str, end_entity: str, max_length: int = 3) -> List[List[Dict[str, Any]]]:
        """Find paths between two entities in the graph"""
        if not self.use_graph_db:
            raise ValueError("Graph database not enabled")
        
        try:
            # Check if entities exist
            if start_entity not in self.graph.nodes:
                raise ValueError(f"Entity '{start_entity}' not found in graph")
            if end_entity not in self.graph.nodes:
                raise ValueError(f"Entity '{end_entity}' not found in graph")
            
            # Find all simple paths up to max_length
            paths = list(nx.all_simple_paths(self.graph, start_entity, end_entity, cutoff=max_length))
            
            # Format results
            result_paths = []
            for path in paths:
                path_info = []
                # Add nodes and edges to path
                for i, node in enumerate(path):
                    # Add node
                    node_data = self.graph.nodes[node]
                    path_info.append({
                        "type": "entity",
                        "name": node,
                        "entity_type": node_data.get("entity_type", "unknown"),
                    })
                    # Add edge if not last node
                    if i < len(path) - 1:
                        next_node = path[i+1]
                        edge_data = self.graph.get_edge_data(node, next_node)
                        path_info.append({
                            "type": "relation",
                            "from": node,
                            "to": next_node,
                            "relation_type": edge_data.get("relation_type", "unknown"),
                        })
                result_paths.append(path_info)
            return result_paths
        except ValueError:
            # Re-raise ValueError for specific error handling
            raise
        except Exception as e:
            logger.error("Error finding paths: %s", e, exc_info=True)
            return []

    def get_similar_entities(self, entity_name: str, threshold: float = 0.6) -> List[Dict[str, Any]]:
        """Find entities with similar names"""
        try:
            if self.use_graph_db:
                # If graph DB is enabled, use graph nodes
                all_entities = list(self.graph.nodes)
            else:
                # Otherwise use entities from knowledge graph
                knowledge_graph = self._read_graph_file()
                all_entities = [entity.name for entity in knowledge_graph.entities]
            
            # Calculate similarity scores
            similarities = []
            for name in all_entities:
                score = difflib.SequenceMatcher(None, entity_name.lower(), name.lower()).ratio()
                if score >= threshold:
                    similarities.append({
                        "name": name,
                        "similarity": score
                    })
            
            # Sort by similarity (highest first)
            similarities.sort(key=lambda x: x["similarity"], reverse=True)
            return similarities
        except Exception as exc:
            logger.error("Error finding similar entities: %s", exc, exc_info=True)
            return []

    def get_entity_connections(self, entity_name: str) -> Dict[str, Any]:
        """Get all connections for a specific entity"""
        if not self.use_graph_db:
            # Use an in-memory approach since graph DB is disabled
            incoming = []
            outgoing = []
            
            with self.lock:
                # Find entity
                if entity_name not in self.entities:
                    return {'incoming': [], 'outgoing': []}
                
                # Find relations
                for relation in self.relations:
                    if relation['from'] == entity_name:
                        outgoing.append({
                            'entity': relation['to'],
                            'relation_type': relation['relation_type'],
                            'properties': relation['properties']
                        })
                    elif relation['to'] == entity_name:
                        incoming.append({
                            'entity': relation['from'],
                            'relation_type': relation['relation_type'],
                            'properties': relation['properties']
                        })
            
            return {
                'entity': entity_name,
                'incoming': incoming,
                'outgoing': outgoing
            }
        else:
            # Use the graph database for faster querying
            with self.lock:
                if entity_name not in self.graph:
                    return {'incoming': [], 'outgoing': []}
                
                # Get incoming edges
                incoming = []
                for pred in self.graph.predecessors(entity_name):
                    edge_data = self.graph.get_edge_data(pred, entity_name)
                    incoming.append({
                        'entity': pred,
                        'relation_type': edge_data.get('relation_type', 'related_to'),
                        'properties': {k: v for k, v in edge_data.items() if k != 'relation_type'}
                    })
                
                # Get outgoing edges
                outgoing = []
                for succ in self.graph.successors(entity_name):
                    edge_data = self.graph.get_edge_data(entity_name, succ)
                    outgoing.append({
                        'entity': succ,
                        'relation_type': edge_data.get('relation_type', 'related_to'),
                        'properties': {k: v for k, v in edge_data.items() if k != 'relation_type'}
                    })
                
                return {
                    'entity': entity_name,
                    'incoming': incoming,
                    'outgoing': outgoing
                }

    def get_related_entities(self, entity_name: str, max_depth: int = 1) -> Dict[str, Any]:
        """Get entities related to a specific entity up to a maximum depth"""
        if not self.use_graph_db:
            raise ValueError("Graph database not enabled")
        try:
            if entity_name not in self.graph:
                raise ValueError(f"Entity '{entity_name}' does not exist in the graph")
                
            # Get entities within max_depth
            related_entities = set()
            explore_queue = [(entity_name, 0)]  # (node, depth)
            visited = set([entity_name])
            
            while explore_queue:
                node, depth = explore_queue.pop(0)
                
                if depth <= max_depth:
                    # Add all neighbors at this depth
                    neighbor_nodes = set(self.graph.successors(node)) | set(self.graph.predecessors(node))
                    for neighbor in neighbor_nodes:
                        if neighbor not in visited:
                            visited.add(neighbor)
                            related_entities.add(neighbor)
                            if depth < max_depth:
                                explore_queue.append((neighbor, depth + 1))
            
            # Get entity details
            entities = []
            for name in related_entities:
                node_data = self.graph.nodes[name]
                entities.append({
                    "name": name,
                    "entity_type": node_data.get("entity_type", "unknown"),
                    "observations": node_data.get("observations", [])[:3]  # First 3 observations
                })
                
            return {
                "source_entity": entity_name,
                "max_depth": max_depth,
                "related_entities_count": len(entities),
                "entities": entities
            }
            
        except ValueError:
            # Re-raise validation errors
            raise
        except Exception as exc:
            logger.error("Error getting related entities: %s", exc, exc_info=True)
            return {"error": str(exc)}

    def _load_memory(self):
        """Load memory data from file"""
        if os.path.exists(self.memory_file_path):
            try:
                with open(self.memory_file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.entities = data.get('entities', {})
                    self.relations = data.get('relations', [])
                    
                    # Rebuild graph if using graph DB
                    if self.use_graph_db:
                        self._rebuild_graph()
                        
                logger.info(f"Loaded {len(self.entities)} entities and {len(self.relations)} relations from {self.memory_file_path}")
            except Exception as e:
                logger.error(f"Error loading memory data: {e}")
                # Initialize empty data
                self.entities = {}
                self.relations = []

    def _save_memory(self):
        """Save memory data to file"""
        try:
            with open(self.memory_file_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'entities': self.entities,
                    'relations': self.relations
                }, f, indent=2)
            logger.info(f"Saved {len(self.entities)} entities and {len(self.relations)} relations to {self.memory_file_path}")
        except Exception as e:
            logger.error(f"Error saving memory data: {e}")

    def _rebuild_graph(self):
        """Rebuild the graph database from entities and relations"""
        if not self.use_graph_db:
            return
            
        # Clear existing graph
        self.graph.clear()
        
        # Add all entities as nodes
        for entity_name, entity_data in self.entities.items():
            self.graph.add_node(entity_name, **entity_data)
            
        # Add all relations as edges
        for relation in self.relations:
            from_entity = relation['from']
            to_entity = relation['to']
            relation_type = relation['relation_type']
            self.graph.add_edge(from_entity, to_entity, 
                               relation_type=relation_type, 
                               **relation.get('properties', {}))

    def get_entities(self, entity_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """Get all entities, optionally filtered by type"""
        result = []
        
        with self.lock:
            for name, data in self.entities.items():
                # Apply type filter if specified
                if entity_type and data['entity_type'] != entity_type:
                    continue
                    
                # Build entity object
                entity = {
                    'name': name,
                    'entity_type': data['entity_type'],
                    'properties': data.get('properties', {}),
                    'created_at': data.get('created_at')
                }
                result.append(entity)
                
        return result

    def get_relations(self, from_entity: Optional[str] = None, 
                     to_entity: Optional[str] = None,
                     relation_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """Get relations, optionally filtered by entities or type"""
        result = []
        
        with self.lock:
            for relation in self.relations:
                # Apply filters
                if from_entity and relation['from'] != from_entity:
                    continue
                if to_entity and relation['to'] != to_entity:
                    continue
                if relation_type and relation['relation_type'] != relation_type:
                    continue
                    
                # Build relation object
                relation_obj = {
                    'from_': relation['from'],  # Use from_ to match test expectations
                    'to': relation['to'],
                    'relation_type': relation['relation_type'],
                    'properties': relation.get('properties', {}),
                    'created_at': relation.get('created_at')
                }
                result.append(relation_obj)
                
        return result

    def query_entities(self, entity_type: Optional[str] = None, 
                      properties: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Query entities with optional filters"""
        result = []
        
        with self.lock:
            for name, data in self.entities.items():
                # Apply type filter if specified
                if entity_type and data['entity_type'] != entity_type:
                    continue
                    
                # Apply property filters if specified
                if properties:
                    match = True
                    entity_props = data.get('properties', {})
                    for key, value in properties.items():
                        if key not in entity_props or entity_props[key] != value:
                            match = False
                            break
                    if not match:
                        continue
                    
                # Build entity object
                entity = {
                    'name': name,
                    'entity_type': data['entity_type'],
                    'properties': data.get('properties', {}),
                    'created_at': data.get('created_at')
                }
                result.append(entity)
                
        return result


================================================================================
FILE: app/core/scraper_service.py
LANGUAGE: python
SIZE: 25141 bytes
================================================================================

import re
import time
import json
import random
import asyncio
import hashlib
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, TypeVar, cast, Protocol, TYPE_CHECKING
from urllib.parse import urljoin, urlparse

# For type checking only
if TYPE_CHECKING:
    from bs4 import BeautifulSoup, Tag
    from playwright.async_api import Page, Browser, BrowserContext, Response

# Third-party imports in try/except for graceful handling
try:
    from bs4 import BeautifulSoup, Tag
    HAS_BS4 = True
except ImportError:
    BeautifulSoup = None
    Tag = None
    HAS_BS4 = False

try:
    from playwright.async_api import async_playwright, Response
    HAS_PLAYWRIGHT = True
except ImportError:
    async_playwright = None
    Response = None
    HAS_PLAYWRIGHT = False

from app.utils.config import get_config
from app.utils.markdown import html_to_markdown
from app.core.serper_service import SerperService

logger = logging.getLogger(__name__)

# Define type aliases for improved type checking
SoupType = TypeVar('SoupType')
TagType = TypeVar('TagType')
ResponseType = TypeVar('ResponseType')

class ScraperService:
    """Service for scraping web content and processing HTML pages"""
    def __init__(self):
        # Check if required libraries are available
        if not HAS_BS4 or not HAS_PLAYWRIGHT:
            logger.error("Required libraries (BeautifulSoup or playwright) are not installed")
            
        config = get_config()
        self.min_delay = config.scraper_min_delay
        self.max_delay = config.scraper_max_delay
        
        self.data_dir = Path(config.scraper_data_path)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        self.cache_dir = self.data_dir / "cache"
        self.cache_dir.mkdir(exist_ok=True)
        
        self.user_agent = config.user_agent
        
        self.browser = None
        self.context = None

        # Initialize the Serper service
        self.serper_service = SerperService()

    async def get_browser(self):
        """Initialize and return the browser instance"""
        if self.browser is None:
            if not HAS_PLAYWRIGHT:
                raise ImportError("Playwright is not installed")
            playwright = await async_playwright().start()
            self.browser = await playwright.chromium.launch(headless=True)
        return self.browser

    async def close(self):
        """Close browser instance when done"""
        if self.browser:
            await self.browser.close()
            self.browser = None

    async def get_or_scrape_url(self, url: str, max_cache_age: int = 86400) -> Dict[str, Any]:
        """Get URL from cache or scrape it if not cached or too old"""
        try:
            cache_key = hashlib.md5(url.encode()).hexdigest()
            cache_path = self.cache_dir / f"{cache_key}.json"
            
            if cache_path.exists():
                cache_age = time.time() - cache_path.stat().st_mtime
                if cache_age < max_cache_age:
                    try:
                        with open(cache_path, "r", encoding="utf-8") as f:
                            return json.load(f)
                    except Exception as e:
                        logger.warning(f"Error loading cache for {url}: {e}")
            
            result = await self.scrape_url(url)
            
            if result["success"]:
                try:
                    with open(cache_path, "w", encoding="utf-8") as f:
                        json.dump(result, f, ensure_ascii=False, indent=2)
                except Exception as e:
                    logger.warning(f"Error saving to cache for {url}: {e}")
            return result
        except Exception as e:
            logger.error(f"Error in get_or_scrape_url for {url}: {e}", exc_info=True)
            return {
                "url": url,
                "success": False,
                "error": str(e)
            }

    async def _handle_rate_limiting(self, response: Optional["ResponseType"]) -> bool:
        """Handle rate limiting based on response codes"""
        if response is not None and response.status == 429:  # Too Many Requests
            retry_after = response.headers.get('retry-after')
            wait_time = int(retry_after) if retry_after and retry_after.isdigit() else 60
            logger.info(f"Rate limited. Waiting for {wait_time} seconds")
            await asyncio.sleep(wait_time)
            return True
        return False

    async def scrape_url(self, url: str) -> Dict[str, Any]:
        """Scrape a URL and extract its content"""
        try:
            if not HAS_PLAYWRIGHT:
                raise ImportError("Playwright is not installed")
                
            browser = await self.get_browser()
            context = await browser.new_context(
                user_agent=self.user_agent
            )
            try:
                delay = random.uniform(self.min_delay, self.max_delay)
                await asyncio.sleep(delay)
                
                page = await context.new_page()
                response = await page.goto(url, wait_until="domcontentloaded", timeout=30000)
                
                if response is None or not response.ok:
                    if await self._handle_rate_limiting(response):
                        await page.close()
                        return await self.scrape_url(url)
                    else:
                        status = response.status if response else 0
                        status_text = response.status_text if response else "Unknown error"
                        return {
                            "url": url,
                            "success": False,
                            "error": f"HTTP Error: {status} {status_text}"
                        }
                
                await page.wait_for_load_state("networkidle")
                
                html_content = await page.content()
                title = await page.title()
                
                markdown_content = html_to_markdown(html_content, url, title)
                metadata = self._extract_metadata(html_content, url)
                links = self._extract_links(url, html_content)
                
                return {
                    "url": url,
                    "title": title,
                    "content": markdown_content,
                    "metadata": metadata,
                    "links": links,
                    "scraped_at": int(time.time()),
                    "success": True
                }
            finally:
                await context.close()
        except Exception as e:
            logger.error(f"Error scraping {url}: {e}", exc_info=True)
            return {
                "url": url,
                "success": False,
                "error": str(e)
            }

    def _extract_metadata(self, content: str, url: str) -> Dict[str, Any]:
        """Extract metadata from HTML content"""
        metadata: Dict[str, Any] = {
            "source_url": url,
            "extracted_at": datetime.now().isoformat()
        }
        try:
            if not HAS_BS4:
                return metadata
                
            soup = BeautifulSoup(content, "html.parser")
            # Extract Open Graph metadata
            for prop in ["og:title", "og:description", "og:image", "og:type", "og:site_name"]:
                element = soup.find("meta", property=prop)
                if element is not None:
                    element_tag = cast("Tag", element)
                    content_attr = element_tag.get("content")
                    if content_attr:
                        key = prop.rsplit(":", maxsplit=1)[-1]
                        metadata[key] = content_attr
            # Extract basic metadata
            if "title" not in metadata:
                title_tag = soup.find("title")
                if title_tag:
                    metadata["title"] = title_tag.get_text()
            # Extract description
            if "description" not in metadata:
                desc = soup.find("meta", attrs={"name": "description"})
                if desc is not None:
                    desc_tag = cast("Tag", desc)
                    content_attr = desc_tag.get("content")
                    if content_attr:
                        metadata["description"] = content_attr
            # Extract LD+JSON structured data
            structured_data = self._extract_structured_data(soup)
            if structured_data:
                metadata["structured_data"] = structured_data
        except Exception as e:
            logger.warning("Error extracting metadata from %s: %s", url, e)
        return metadata

    def _extract_structured_data(self, soup: Any) -> Dict[str, Any]:
        """Extract structured data from LD+JSON scripts"""
        try:
            if not HAS_BS4:
                return {}
                
            structured_data = []
            for script in soup.find_all("script", type="application/ld+json"):
                try:
                    script_tag = cast("Tag", script)
                    script_string = script_tag.string
                    if script_string:
                        data = json.loads(script_string)
                        structured_data.append(data)
                except (json.JSONDecodeError, TypeError):
                    continue
            return {"items": structured_data} if structured_data else {}
        except Exception as e:
            logger.warning("Error extracting structured data: %s", e)
            return {}

    def _extract_links(self, base_url: str, content: str) -> List[str]:
        """Extract links from content"""
        if not HAS_BS4:
            return []
            
        links = []
        # Extract Markdown links [text](url)
        markdown_links = re.findall(r'\[.*?\]\((https?://[^)]+)\)', content)
        links.extend(markdown_links)
        # Extract HTML links from the content
        soup = BeautifulSoup(content, "html.parser")
        for a_tag in soup.find_all('a', href=True):
            a_tag_cast = cast("Tag", a_tag)
            href = a_tag_cast.get('href')
            if href:
                # Handle relative links
                absolute_url = urljoin(base_url, str(href))
                links.append(absolute_url)
        # Remove duplicates and return
        return list(set(links))

    async def search_and_scrape(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """
        Search for a query and scrape the top results.
        
        Args:
            query: The search query
            max_results: Maximum number of results to scrape
            
        Returns:
            List of dictionaries containing scraped content
        """
        try:
            # Use the Serper service to get search results
            urls = await self.serper_service.search_and_extract_urls(query, max_results)
            
            if not urls:
                logger.warning(f"No URLs found for search query: {query}")
                return [{"query": query, "success": False, "error": "No search results found"}]
            
            # Scrape each URL
            results = []
            for url in urls:
                try:
                    # Add a small delay between requests
                    await asyncio.sleep(random.uniform(self.min_delay, self.max_delay))
                    
                    # Scrape the URL
                    result = await self.get_or_scrape_url(url)
                    result["query"] = query  # Add the original query to the result
                    
                    # Clean up the result
                    if not result.get("success", False):
                        logger.warning(f"Failed to scrape URL: {url}")
                    
                    results.append(result)
                    
                    # Limit the number of results
                    if len(results) >= max_results:
                        break
                        
                except Exception as e:
                    logger.error(f"Error scraping URL {url}: {str(e)}")
                    results.append({
                        "url": url,
                        "query": query,
                        "success": False,
                        "error": str(e)
                    })
            
            return results
            
        except Exception as e:
            logger.error(f"Error in search_and_scrape: {str(e)}", exc_info=True)
            return [{"query": query, "success": False, "error": str(e)}]

    async def enhanced_search_and_scrape(self, 
                                        query: str, 
                                        search_type: str = "search",
                                        num_results: int = 10,
                                        max_scrape: int = 5,
                                        country: Optional[str] = None,
                                        locale: Optional[str] = None) -> Dict[str, Any]:
        """
        Enhanced search and scrape using Serper API with additional metadata
        
        Args:
            query: The search query
            search_type: Type of search (search, news, images, places)
            num_results: Number of search results to return
            max_scrape: Maximum number of results to scrape
            country: Country code for localized results
            locale: Language code for localized results
            
        Returns:
            Dictionary with search results and scraped content
        """
        try:
            # Get search results from Serper
            search_results = await self.serper_service.search(
                query, 
                search_type=search_type,
                num_results=num_results,
                country=country,
                locale=locale
            )
            
            if "error" in search_results:
                return {
                    "query": query,
                    "search_type": search_type,
                    "success": False,
                    "error": search_results["error"],
                    "organic_results": [],
                    "scraped_content": []
                }
            
            # Extract URLs to scrape
            urls_to_scrape = []
            organic_results = search_results.get("organic", [])
            
            for result in organic_results:
                if "link" in result and len(urls_to_scrape) < max_scrape:
                    urls_to_scrape.append(result["link"])
            
            # Scrape each URL
            scraped_content = []
            for url in urls_to_scrape:
                try:
                    # Add a small delay between requests
                    await asyncio.sleep(random.uniform(self.min_delay, self.max_delay))
                    
                    # Scrape the URL
                    result = await self.get_or_scrape_url(url)
                    
                    if result.get("success", False):
                        scraped_content.append(result)
                    
                except Exception as e:
                    logger.error(f"Error scraping URL {url}: {str(e)}")
                    scraped_content.append({
                        "url": url,
                        "success": False,
                        "error": str(e)
                    })
            
            return {
                "query": query,
                "search_type": search_type,
                "success": True,
                "organic_results": organic_results,
                "scraped_content": scraped_content
            }
            
        except Exception as e:
            logger.error(f"Error in enhanced_search_and_scrape: {str(e)}", exc_info=True)
            return {
                "query": query,
                "search_type": search_type,
                "success": False,
                "error": str(e),
                "organic_results": [],
                "scraped_content": []
            }

    async def scrape_with_pagination(self, url: str, max_pages: int = 5) -> Dict[str, Any]:
        """Scrape a URL and follow pagination links"""
        all_content = ""
        current_url = url
        pages_scraped = 0
        try:
            while current_url and pages_scraped < max_pages:
                result = await self.scrape_url(current_url)
                if not result["success"]:
                    break
                # Accumulate content
                all_content += result["content"] + "\n\n---\n\n"
                pages_scraped += 1
                # Find next page link
                next_url = self._find_next_page_link(result["content"], current_url)
                if not next_url or next_url == current_url:
                    break
                current_url = next_url
                # Add delay between pages
                await asyncio.sleep(random.uniform(self.min_delay, self.max_delay))
            # Create combined result
            return {
                "url": url,
                "title": f"Paginated content ({pages_scraped} pages)",
                "content": all_content,
                "scraped_at": int(time.time()),
                "success": True,
                "pages_scraped": pages_scraped
            }
        except Exception as e:
            logger.error(f"Error in paginated scraping for {url}: {e}", exc_info=True)
            return {
                "url": url,
                "success": False,
                "error": str(e),
                "pages_scraped": pages_scraped
            }

    def _find_next_page_link(self, content: str, current_url: str) -> Optional[str]:
        """Find pagination link in content"""
        if not HAS_BS4:
            return None
            
        soup = BeautifulSoup(content, "html.parser")
        # Common patterns for next page links
        next_selectors = [
            '.pagination .next',
            '.pagination a[rel="next"]',
            'a.next',
            'a:contains("Next")',
            'a[aria-label="Next"]',
            '.pagination a:contains("")',
            '.pagination a:contains(">")'
        ]
        for selector in next_selectors:
            try:
                next_link = soup.select_one(selector)
                if next_link:
                    next_link_tag = cast("Tag", next_link)
                    href = next_link_tag.get('href')
                    if href:
                        return urljoin(current_url, str(href))
            except Exception:
                continue
        return None

    async def capture_screenshot(self, url: str, full_page: bool = True) -> Dict[str, Any]:
        """Capture screenshot of a webpage"""
        if not HAS_PLAYWRIGHT:
            return {"url": url, "success": False, "error": "Playwright not installed"}
            
        browser = await self.get_browser()
        context = await browser.new_context(
            user_agent=self.user_agent,
            viewport={'width': 1920, 'height': 1080}
        )
        try:
            page = await context.new_page()
            await page.goto(url, wait_until="networkidle")
            # Capture screenshot
            screenshot_path = self.data_dir / "screenshots"
            screenshot_path.mkdir(exist_ok=True)
            filename = f"{hashlib.md5(url.encode()).hexdigest()}.png"
            file_path = screenshot_path / filename
            await page.screenshot(path=str(file_path), full_page=full_page)
            return {
                "url": url,
                "screenshot_path": str(file_path),
                "timestamp": int(time.time()),
                "success": True
            }
        except Exception as e:
            logger.error(f"Screenshot failed for {url}: {e}")
            return {
                "url": url,
                "error": str(e),
                "success": False
            }
        finally:
            await context.close()

    async def scrape_sitemap(self, sitemap_url: str, max_urls: int = 50) -> Dict[str, Any]:
        """Extract URLs from sitemap and scrape them"""
        try:
            if not HAS_BS4:
                return {"success": False, "error": "BeautifulSoup not installed"}
                
            # Scrape the sitemap XML
            sitemap_result = await self.scrape_url(sitemap_url)
            if not sitemap_result["success"]:
                return {
                    "success": False,
                    "error": f"Failed to fetch sitemap: {sitemap_result.get('error')}"
                }
            # Extract URLs from sitemap
            soup = BeautifulSoup(sitemap_result["content"], "xml")
            urls = []
            # Process standard sitemap format
            for loc in soup.find_all("loc"):
                urls.append(loc.text)
            # Limit the number of URLs to scrape
            urls = urls[:max_urls]
            # Scrape each URL
            scraped_results = []
            for url in urls:
                try:
                    result = await self.get_or_scrape_url(url)
                    scraped_results.append(result)
                    # Add delay between requests
                    await asyncio.sleep(random.uniform(self.min_delay, self.max_delay))
                except Exception as e:
                    logger.warning(f"Failed to scrape URL from sitemap {url}: {e}")
            return {
                "sitemap_url": sitemap_url,
                "urls_found": len(urls),
                "urls_scraped": scraped_results,
                "success": True
            }
        except Exception as e:
            logger.error(f"Error processing sitemap {sitemap_url}: {e}", exc_info=True)
            return {
                "sitemap_url": sitemap_url,
                "success": False,
                "error": str(e)
            }

    async def crawl_website(self, start_url: str, max_pages: int = 50,
                        recursion_depth: int = 2, allowed_domains: Optional[List[str]] = None,
                        verification_pass: bool = False) -> Dict[str, Any]:
        """
        Crawl a website starting from a URL.
        
        This enhanced version uses the Serper service for more context when appropriate.
        
        Args:
            start_url: URL to start crawling from
            max_pages: Maximum number of pages to crawl
            recursion_depth: Maximum recursion depth
            allowed_domains: List of domains to restrict crawling to
            verification_pass: Whether to do a verification pass to check content stability
            
        Returns:
            Dictionary with crawling results
        """
        # Use existing code but with Serper enhancements when appropriate
        # ...implementation would remain largely the same...
        pass

    async def _perform_verification_pass(self, urls: List[str], context: Any) -> List[Dict[str, Any]]:
        """Verification pass to check content stability"""
        verification_results = []
        
        for url in urls:
            try:
                page = await context.new_page()
                try:
                    await page.goto(url, wait_until="networkidle", timeout=30000)
                    verification_results.append({
                        "url": url,
                        "verified": True
                    })
                except Exception as e:
                    verification_results.append({
                        "url": url,
                        "verified": False,
                        "error": str(e)
                    })
                finally:
                    await page.close()
                    
                # Rate limiting
                await asyncio.sleep(random.uniform(self.min_delay, self.max_delay))
                
            except Exception as e:
                verification_results.append({
                    "url": url,
                    "verified": False,
                    "error": str(e)
                })
                
        return verification_results
    
    async def scrape_urls(self, urls: List[str]) -> List[Dict[str, Any]]:
        """Scrape multiple URLs in parallel"""
        tasks = [self.scrape_url(url) for url in urls]
        return await asyncio.gather(*tasks)

================================================================================
FILE: app/core/serper_service.py
LANGUAGE: python
SIZE: 10264 bytes
================================================================================

import asyncio
import logging
from typing import Dict, Any, List, Optional, TYPE_CHECKING, cast
from urllib.parse import urlparse
from app.utils.config import get_config

# For type checking to prevent "Import could not be resolved" errors
if TYPE_CHECKING:
    import httpx  # type: ignore
    import aiohttp  # type: ignore
    import requests  # type: ignore

# Initialize module variables
httpx = None
aiohttp = None
requests = None
HTTPX_AVAILABLE = False
AIOHTTP_AVAILABLE = False
REQUESTS_AVAILABLE = False

# Group all imports together to fix "ungrouped imports" warnings
# We need to disable pylint warnings for the import grouping
# pylint: disable=ungrouped-imports,wrong-import-position
try:
    # Use a type ignore comment to silence the Pylance warning
    import httpx as httpx_module  # type: ignore
    httpx = httpx_module
    HTTPX_AVAILABLE = True
except ImportError:
    pass

try:
    import aiohttp as aiohttp_module
    aiohttp = aiohttp_module
    AIOHTTP_AVAILABLE = True
except ImportError:
    pass

try:
    import requests as requests_module
    requests = requests_module
    REQUESTS_AVAILABLE = True
except ImportError:
    pass
# pylint: enable=ungrouped-imports,wrong-import-position

if not (HTTPX_AVAILABLE or AIOHTTP_AVAILABLE or REQUESTS_AVAILABLE):
    # If all HTTP libraries are unavailable, log a more serious warning
    logging.warning("No HTTP client libraries available. API calls will fail.")

logger = logging.getLogger(__name__)

if not HTTPX_AVAILABLE:
    logger.warning(
        "httpx package not installed. This is recommended for optimal performance. "
        "Install with: pip install httpx"
    )
    if not AIOHTTP_AVAILABLE:
        logger.warning(
            "Neither httpx nor aiohttp is available. Using synchronous requests library as fallback. "
            "This may affect performance. Install httpx with: pip install httpx"
        )

class SerperService:
    """Service for interacting with Serper.dev API for web search and content discovery"""
    
    def __init__(self):
        config = get_config()
        self.api_key = config.search_api_key  # Use the unified search_api_key
        self.base_url = "https://google.serper.dev"
        self.default_country = config.search_default_country
        self.default_locale = config.search_default_locale
        self.timeout = config.search_timeout
        self.max_retries = config.search_max_retries
        self.retry_delay = config.search_retry_delay
        
        if not self.api_key:
            logger.warning("Search API key not configured - search functionality will be limited")
    
    async def search(self, 
                     query: str, 
                     search_type: str = "search", 
                     num_results: int = 10,
                     country: Optional[str] = None,
                     locale: Optional[str] = None) -> Dict[str, Any]:
        """
        Perform a search using the Serper API
        
        Args:
            query: The search query
            search_type: Type of search (search, news, images, places)
            num_results: Number of results to return
            country: Country code for localized results
            locale: Language code for localized results
            
        Returns:
            Dict containing search results
        """
        if not self.api_key:
            return {"error": "Serper API key not configured", "results": []}
        
        headers = {
            "X-API-KEY": self.api_key,
            "Content-Type": "application/json"
        }
        
        payload = {
            "q": query,
            "num": num_results,
            "gl": country or self.default_country,
            "hl": locale or self.default_locale
        }
        
        endpoint = f"/{search_type}"
        
        # Define the requests handler outside the loop to avoid 'cell-var-from-loop' warning
        # This function is used when only the requests library is available
        def make_requests_call():
            if not REQUESTS_AVAILABLE or requests is None:
                return None
            
            requests_lib = cast(Any, requests)
            response = requests_lib.post(
                f"{self.base_url}{endpoint}",
                headers=headers,
                json=payload,
                timeout=self.timeout
            )
            return response
        
        for attempt in range(self.max_retries):
            try:
                if HTTPX_AVAILABLE and httpx is not None:
                    # Use httpx if available - with explicit type check
                    client_cls = cast(Any, httpx).AsyncClient  # Use cast for type safety
                    async with client_cls(timeout=self.timeout) as client:
                        response = await client.post(
                            f"{self.base_url}{endpoint}", 
                            headers=headers,
                            json=payload
                        )
                        
                        if response.status_code == 200:
                            return response.json()
                        else:
                            logger.error(f"Serper API error: {response.status_code} - {response.text}")
                            return {
                                "error": f"API error: {response.status_code}", 
                                "message": response.text,
                                "results": []
                            }
                elif AIOHTTP_AVAILABLE and aiohttp is not None:
                    # Use aiohttp as a fallback - with explicit type check
                    session_cls = cast(Any, aiohttp).ClientSession  # Use cast for type safety
                    async with session_cls() as session:
                        async with session.post(
                            f"{self.base_url}{endpoint}",
                            headers=headers,
                            json=payload,
                            timeout=self.timeout
                        ) as response:
                            if response.status == 200:
                                return await response.json()
                            else:
                                error_text = await response.text()
                                logger.error(f"Serper API error: {response.status} - {error_text}")
                                return {
                                    "error": f"API error: {response.status}",
                                    "message": error_text,
                                    "results": []
                                }
                elif REQUESTS_AVAILABLE and requests is not None:
                    # Use requests as a last resort (synchronous)
                    loop = asyncio.get_running_loop()
                    response = await loop.run_in_executor(None, make_requests_call)
                    
                    if response and response.status_code == 200:
                        return response.json()
                    elif response:
                        logger.error(f"Serper API error: {response.status_code} - {response.text}")
                        return {
                            "error": f"API error: {response.status_code}",
                            "message": response.text,
                            "results": []
                        }
                    else:
                        return {"error": "Failed to make request", "results": []}
                else:
                    return {"error": "No HTTP client libraries available", "results": []}
            
            except Exception as e:
                logger.error(f"Serper API request failed (attempt {attempt+1}/{self.max_retries}): {str(e)}")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(self.retry_delay)
                else:
                    return {"error": str(e), "results": []}
        
        # Add an explicit return to satisfy the return type requirement
        return {"error": "Maximum retries exceeded", "results": []}
    
    async def search_and_extract_urls(self, 
                                      query: str, 
                                      num_results: int = 10,
                                      search_type: str = "search") -> List[str]:
        """
        Search and extract URLs from results
        
        Args:
            query: The search query
            num_results: Number of results to return
            search_type: Type of search
            
        Returns:
            List of URLs from the search results
        """
        search_results = await self.search(query, search_type, num_results)
        
        if "error" in search_results:
            logger.error(f"Error in search: {search_results['error']}")
            return []
        
        urls = []
        
        # Extract organic search results
        organic_results = search_results.get("organic", [])
        for result in organic_results:
            if "link" in result:
                urls.append(result["link"])
        
        # Extract other result types as needed
        if "news" in search_results:
            for result in search_results["news"]:
                if "link" in result:
                    urls.append(result["link"])
                    
        if "knowledgeGraph" in search_results and "website" in search_results["knowledgeGraph"]:
            urls.append(search_results["knowledgeGraph"]["website"])
        
        return urls
    
    def extract_domain(self, url: str) -> str:
        """Extract the domain from a URL"""
        parsed_url = urlparse(url)
        domain = parsed_url.netloc
        return domain
    
    def normalize_url(self, url: str) -> str:
        """Normalize a URL by adding https:// if needed"""
        if not url.startswith(('http://', 'https://')):
            return f"https://{url}"
        return url


================================================================================
FILE: app/core/test_user_prefs.py
LANGUAGE: python
SIZE: 278 bytes
================================================================================

from app.core.memory_service import MemoryService

ms = MemoryService()
# Set a preference
ms.set_user_preference("test_user", {"theme": "dark", "language": "en"})
# Get the preference
prefs = ms.get_user_preference("test_user")
print(f"Retrieved preferences: {prefs}")


================================================================================
FILE: app/models/__init__.py
LANGUAGE: python
SIZE: 52 bytes
================================================================================

"""
Data Models for the Unified Tools Server
"""


================================================================================
FILE: app/models/documents.py
LANGUAGE: python
SIZE: 3414 bytes
================================================================================

from enum import Enum
from typing import Dict, Any, Optional, List, Union
from pydantic import BaseModel, Field, HttpUrl


class DocumentType(str, Enum):
    MANUSCRIPT = "manuscript"  # Novels, fiction, etc.
    DOCUMENTATION = "documentation"  # Research papers, technical docs
    DATASET = "dataset"  # Structured data for training
    WEBPAGE = "webpage"  # Scraped web content
    GENERIC = "generic"  # Other document types


class CreateDocumentRequest(BaseModel):
    """Request model for document creation"""

    title: str = Field(..., description="Document title", min_length=1)
    content: str = Field(..., description="Document content (text)")
    document_type: DocumentType = Field(DocumentType.GENERIC, description="Type of document")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Custom metadata")
    tags: List[str] = Field(default_factory=list, description="Document tags")
    source_url: Optional[str] = Field(None, description="Source URL if applicable")
    storage_type: str = Field("local", description="Storage type (local or s3)")


class UpdateDocumentRequest(BaseModel):
    """Request model for document updates"""

    title: Optional[str] = Field(None, description="Updated title")
    content: Optional[str] = Field(None, description="Updated content")
    metadata: Optional[Dict[str, Any]] = Field(None, description="Updated metadata")
    tags: Optional[List[str]] = Field(None, description="Updated tags")
    commit_message: str = Field("Updated document", description="Git commit message")


class DocumentResponse(BaseModel):
    """Response model for document operations"""

    id: str = Field(..., description="Unique document identifier")
    title: str = Field(..., description="Document title")
    document_type: DocumentType = Field(..., description="Type of document")
    created_at: int = Field(..., description="Creation timestamp")
    updated_at: int = Field(..., description="Last update timestamp")
    tags: List[str] = Field(..., description="Document tags")
    metadata: Dict[str, Any] = Field(..., description="Document metadata")
    content_preview: str = Field(..., description="Preview of document content")
    size_bytes: int = Field(..., description="Document size in bytes")
    version_count: Optional[int] = Field(1, description="Number of versions")
    content_available: bool = Field(..., description="Whether full content is available")
    source_url: Optional[str] = Field(None, description="Source URL if applicable")


class DocumentVersionResponse(BaseModel):
    """Response model for document version history"""

    version_hash: str = Field(..., description="Git commit hash")
    commit_message: str = Field(..., description="Commit message")
    author: str = Field(..., description="Author name")
    timestamp: int = Field(..., description="Commit timestamp")
    changes: Optional[Dict[str, Any]] = Field(None, description="Changes in this version")


class DocumentContentResponse(BaseModel):
    """Response model for document content"""

    id: str = Field(..., description="Document identifier")
    title: str = Field(..., description="Document title")
    content: str = Field(..., description="Full document content")
    version: Optional[str] = Field(None, description="Version hash if specific version requested")


================================================================================
FILE: app/models/filesystem.py
LANGUAGE: python
SIZE: 3611 bytes
================================================================================

from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional


class ReadFileRequest(BaseModel):
    """Request to read a file"""

    path: str = Field(..., description="Path to file")
    storage: str = Field("local", description="Storage type (local or s3)")
    bucket: Optional[str] = Field(None, description="S3 bucket name if using S3 storage")


class WriteFileRequest(BaseModel):
    """Request to write to a file"""

    path: str = Field(..., description="Path to file")
    content: str = Field(..., description="Content to write")
    storage: str = Field("local", description="Storage type (local or s3)")
    bucket: Optional[str] = Field(None, description="S3 bucket name if using S3 storage")


class ListDirectoryRequest(BaseModel):
    """Request to list directory contents"""

    path: str = Field(..., description="Directory path")
    storage: str = Field("local", description="Storage type (local or s3)")
    bucket: Optional[str] = Field(None, description="S3 bucket name if using S3 storage")
    recursive: bool = Field(False, description="Whether to list subdirectories recursively")


class SearchFilesRequest(BaseModel):
    """Request to search for files"""

    path: str = Field(..., description="Base path to search in")
    pattern: str = Field(..., description="Pattern to search for")
    storage: str = Field("local", description="Storage type (local or s3)")
    bucket: Optional[str] = Field(None, description="S3 bucket name if using S3 storage")
    exclude_patterns: Optional[List[str]] = Field(None, description="Patterns to exclude")


class CreateDirectoryRequest(BaseModel):
    """Request to create a directory"""

    path: str = Field(..., description="Directory path")
    storage: str = Field("local", description="Storage type (local or s3)")
    bucket: Optional[str] = Field(None, description="S3 bucket name if using S3 storage")


class DeleteFileRequest(BaseModel):
    """Request to delete a file"""

    path: str = Field(..., description="Path to file")
    storage: str = Field("local", description="Storage type (local or s3)")
    bucket: Optional[str] = Field(None, description="S3 bucket name if using S3 storage")


class DirectoryItem(BaseModel):
    """Information about a directory item"""

    name: str = Field(..., description="Item name")
    path: str = Field(..., description="Item path relative to listing directory")
    type: str = Field(..., description="Item type (file or directory)")
    size: Optional[int] = Field(None, description="File size in bytes")
    last_modified: Optional[int] = Field(None, description="Last modification timestamp")


class DirectoryListingResponse(BaseModel):
    """Response for directory listing"""

    path: str = Field(..., description="Listed directory path")
    items: List[DirectoryItem] = Field(..., description="Directory contents")


class InvalidateCacheRequest(BaseModel):
    """Request to invalidate file cache"""

    path: Optional[str] = Field(None, description="Path to invalidate (None for all)")
    storage: str = Field("local", description="Storage type (local or s3)")
    bucket: Optional[str] = Field(None, description="S3 bucket name if using S3 storage")


class FileExistsRequest(BaseModel):
    """Request to check if file exists"""

    path: str = Field(..., description="Path to check")
    storage: str = Field("local", description="Storage type (local or s3)")
    bucket: Optional[str] = Field(None, description="S3 bucket name if using S3 storage")


================================================================================
FILE: app/models/git.py
LANGUAGE: python
SIZE: 2827 bytes
================================================================================

from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any, Union


class GitRepoPath(BaseModel):
    repo_path: str = Field(..., description="File system path to the Git repository.")


class GitStatusRequest(GitRepoPath):
    pass


class GitDiffRequest(GitRepoPath):
    file_path: Optional[str] = Field(None, description="Specific file to show diff for")
    target: Optional[str] = Field(None, description="The branch or commit to diff against.")


class GitCommitRequest(GitRepoPath):
    message: str = Field(..., description="Commit message for recording the change.")
    author_name: Optional[str] = Field(None, description="Git author name")
    author_email: Optional[str] = Field(None, description="Git author email")


class GitAddRequest(GitRepoPath):
    files: List[str] = Field(..., description="List of file paths to add to the staging area.")


class GitLogRequest(GitRepoPath):
    max_count: int = Field(10, description="Maximum number of commits to retrieve.")
    file_path: Optional[str] = Field(None, description="Filter log by specific file")


class GitCreateBranchRequest(GitRepoPath):
    branch_name: str = Field(..., description="Name of the branch to create.")
    base_branch: Optional[str] = Field(
        None, description="Optional base branch name to create the new branch from."
    )


class GitCheckoutRequest(GitRepoPath):
    branch_name: str = Field(..., description="Branch name to checkout.")
    create: bool = Field(False, description="Whether to create the branch if it doesn't exist")


class GitInitRequest(GitRepoPath):
    bare: bool = Field(False, description="Whether to create a bare repository")


class GitCloneRequest(BaseModel):
    repo_url: str = Field(..., description="URL of the repository to clone")
    local_path: str = Field(..., description="Local path to clone to")
    auth_token: Optional[str] = Field(None, description="Authentication token if needed")


class CommitInfo(BaseModel):
    hash: str = Field(..., description="Commit hash")
    author: str = Field(..., description="Commit author")
    date: str = Field(..., description="Commit date")
    message: str = Field(..., description="Commit message")


class GitStatusResponse(BaseModel):
    clean: bool = Field(..., description="Whether the working directory is clean")
    current_branch: str = Field(..., description="Current active branch")
    staged_files: List[str] = Field(..., description="Files staged for commit")
    unstaged_files: List[str] = Field(..., description="Files with changes not staged")
    untracked_files: List[str] = Field(..., description="Untracked files")


class GitLogResponse(BaseModel):
    commits: List[CommitInfo] = Field(..., description="List of commits")


================================================================================
FILE: app/models/memory.py
LANGUAGE: python
SIZE: 2588 bytes
================================================================================

from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional, Union, Literal


class Entity(BaseModel):
    name: str = Field(..., description="The name of the entity")
    entity_type: str = Field(..., description="The type of the entity")
    observations: List[str] = Field(
        default_factory=list,
        description="An array of observation contents associated with the entity",
    )


class Relation(BaseModel):
    from_: str = Field(
        ..., alias="from", description="The name of the entity where the relation starts"
    )
    to: str = Field(..., description="The name of the entity where the relation ends")
    relation_type: str = Field(..., description="The type of the relation")


class KnowledgeGraph(BaseModel):
    entities: List[Entity] = Field(default_factory=list)
    relations: List[Relation] = Field(default_factory=list)


class CreateEntitiesRequest(BaseModel):
    entities: List[Entity] = Field(..., description="List of entities to create")


class CreateRelationsRequest(BaseModel):
    relations: List[Relation] = Field(..., description="List of relations to create")


class ObservationItem(BaseModel):
    entity_name: str = Field(..., description="The name of the entity to add the observations to")
    contents: List[str] = Field(..., description="An array of observation contents to add")


class AddObservationsRequest(BaseModel):
    observations: List[ObservationItem] = Field(..., description="A list of observation additions")


class DeleteEntitiesRequest(BaseModel):
    entity_names: List[str] = Field(..., description="An array of entity names to delete")


class DeleteRelationsRequest(BaseModel):
    relations: List[Relation] = Field(..., description="An array of relations to delete")


class SearchNodesRequest(BaseModel):
    query: str = Field(
        ..., description="The search query to match against entity names, types, and content"
    )


class OpenNodesRequest(BaseModel):
    names: List[str] = Field(..., description="An array of entity names to retrieve")


class UserPreference(BaseModel):
    user_id: str = Field(..., description="Unique user identifier")
    preferences: Dict[str, Any] = Field(default_factory=dict, description="User preferences")


class AddEntitiesRequest(BaseModel):
    entities: List[Dict[str, Any]] = Field(..., description="List of entities to create")


class AddRelationsRequest(BaseModel):
    relations: List[Dict[str, Any]] = Field(..., description="List of relations to create")


================================================================================
FILE: app/models/scraper.py
LANGUAGE: python
SIZE: 11928 bytes
================================================================================

from fastapi import APIRouter, Body, HTTPException, Query, Path
from typing import Dict, Any, List, Optional, Union
from pydantic import BaseModel, Field
import random


# Define models
class ScrapeSingleUrlRequest(BaseModel):
    """Request to scrape a single URL"""

    url: str = Field(..., description="URL to scrape")
    wait_for_selector: Optional[str] = Field(None, description="CSS selector to wait for")
    wait_for_timeout: Optional[int] = Field(30000, description="Maximum wait time in ms")
    extract_tables: bool = Field(True, description="Extract tables from content")
    store_as_document: bool = Field(False, description="Store result as a document")
    document_tags: Optional[List[str]] = Field(None, description="Tags for document if stored")


class UrlList(BaseModel):
    """Request to scrape multiple URLs"""

    urls: List[str] = Field(..., description="List of URLs to scrape")
    recursion_depth: int = Field(0, ge=0, le=3, description="How many links deep to follow (0-3)")
    store_as_documents: bool = Field(False, description="Save results as documents")
    document_tags: Optional[List[str]] = Field(None, description="Tags for documents if stored")


class ScrapeCrawlRequest(BaseModel):
    """Request to crawl a website"""

    start_url: str = Field(..., description="Starting URL for crawl")
    max_pages: int = Field(100, ge=1, description="Maximum number of pages to crawl")
    recursion_depth: int = Field(1, ge=1, description="How many links deep to follow")
    allowed_domains: Optional[List[str]] = Field(
        None, description="Restrict crawling to these domains"
    )
    create_documents: bool = Field(True, description="Create documents from scraped content")
    document_tags: Optional[List[str]] = Field(None, description="Tags for documents if created")
    verification_pass: bool = Field(False, description="Run verification pass after initial crawl")


class SearchAndScrapeRequest(BaseModel):
    """Request to search and scrape results"""

    query: str = Field(..., description="Search query")
    max_results: int = Field(10, ge=1, le=50, description="Maximum search results to process")
    create_documents: bool = Field(False, description="Create documents from scraped content")
    document_tags: Optional[List[str]] = Field(None, description="Tags for documents if created")


class SitemapScrapeRequest(BaseModel):
    """Request to scrape URLs from a sitemap"""

    sitemap_url: str = Field(..., description="URL of the sitemap")
    max_urls: int = Field(50, ge=1, description="Maximum number of URLs to scrape")
    create_documents: bool = Field(True, description="Create documents from scraped content")
    document_tags: Optional[List[str]] = Field(None, description="Tags for documents if created")


class TableData(BaseModel):
    headers: List[str] = Field(default_factory=list, description="Table headers")
    rows: List[List[str]] = Field(default_factory=list, description="Table rows")


class ScraperResponse(BaseModel):
    """Response from scraper"""

    url: str = Field(..., description="Scraped URL")
    title: str = Field(..., description="Page title")
    content: str = Field(..., description="Cleaned content in Markdown format")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Extracted metadata")
    scraped_at: int = Field(..., description="Timestamp when scraped")
    success: bool = Field(True, description="Whether scraping was successful")
    links: List[str] = Field(default_factory=list, description="Links extracted from content")
    document_id: Optional[str] = Field(None, description="Document ID if saved as document")
    error: Optional[str] = Field(None, description="Error message if scraping failed")


from app.core.scraper_service import ScraperService
from app.core.documents_service import DocumentsService
from app.models.documents import DocumentType

router = APIRouter()
scraper_service = ScraperService()
documents_service = DocumentsService()


@router.post(
    "/url",
    response_model=ScraperResponse,
    summary="Scrape a single URL",
    description="Extract content from a web page and convert to Markdown",
)
async def scrape_url(request: ScrapeSingleUrlRequest = Body(...)):
    """
    Scrape a single URL and return structured data.
    Extracts content, converts to Markdown, and optionally stores as a document.
    """
    try:
        result = await scraper_service.scrape_url(
            request.url, request.wait_for_selector, request.wait_for_timeout
        )
        # If requested, store as document
        if request.store_as_document and result["success"]:
            doc = documents_service.create_document(
                title=result["title"],
                content=result["content"],
                document_type=DocumentType.WEBPAGE,
                metadata=result["metadata"],
                tags=request.document_tags,
                source_url=result["url"],
            )
            result["document_id"] = doc["id"]
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Scraping failed: {str(e)}")


@router.post(
    "/urls",
    response_model=List[ScraperResponse],
    summary="Scrape multiple URLs",
    description="Scrape multiple URLs in parallel",
)
async def scrape_multiple_urls(request: UrlList = Body(...)):
    """
    Scrape multiple URLs in parallel.
    Processes a list of URLs and returns the scraped content for each.
    """
    try:
        results = await scraper_service.scrape_urls(request.urls)
        # If requested, store results as documents
        if request.store_as_documents:
            for i, result in enumerate(results):
                if result["success"]:
                    try:
                        doc = documents_service.create_document(
                            title=result["title"],
                            content=result["content"],
                            document_type=DocumentType.WEBPAGE,
                            metadata=result["metadata"],
                            tags=request.document_tags,
                            source_url=result["url"],
                        )
                        results[i]["document_id"] = doc["id"]
                    except Exception as e:
                        results[i]["error"] = f"Document creation failed: {str(e)}"
        return results
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Scraping failed: {str(e)}")


@router.post(
    "/crawl",
    response_model=Dict[str, Any],
    summary="Crawl website",
    description="Crawl a website starting from a URL",
)
async def crawl_website(request: ScrapeCrawlRequest = Body(...)):
    """
    Crawl a website starting from a URL.
    Follows links up to a specified depth and processes each page.
    Optional verification pass ensures content stability.
    """
    try:
        results = await scraper_service.crawl_website(
            request.start_url,
            request.max_pages,
            request.recursion_depth,
            request.allowed_domains,
            request.verification_pass,  # Pass the verification_pass parameter
        )
        response = {
            "pages_crawled": results.get("pages_crawled", 0),
            "start_url": request.start_url,
            "success_count": results.get("success_count", 0),
            "failed_count": results.get("failed_count", 0),
        }
        # Include verification results if available
        if "verification_results" in results:
            response["verification_results"] = results["verification_results"]
            response["verification_success_rate"] = results["verification_success_rate"]
        # If requested, create documents
        if request.create_documents:
            document_ids = []
            for result in results.get("results", []):
                if result.get("success", False):
                    try:
                        doc = documents_service.create_document(
                            title=result["title"],
                            content=result["content"],
                            document_type=DocumentType.WEBPAGE,
                            metadata=result["metadata"],
                            tags=request.document_tags,
                            source_url=result["url"],
                        )
                        document_ids.append(doc["id"])
                    except Exception:
                        pass
            response["documents_created"] = len(document_ids)
            response["document_ids"] = document_ids
        return response
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Crawling failed: {str(e)}")


@router.post(
    "/search",
    response_model=List[ScraperResponse],
    summary="Search and scrape",
    description="Search for content and scrape the results",
)
async def search_and_scrape(request: SearchAndScrapeRequest = Body(...)):
    """
    Search for content and scrape the results.
    Performs a web search and scrapes the top results.
    """
    try:
        results = await scraper_service.search_and_scrape(request.query, request.max_results)
        # If requested, create documents
        if request.create_documents:
            for i, result in enumerate(results):
                if result.get("success", False):
                    try:
                        doc = documents_service.create_document(
                            title=result["title"],
                            content=result["content"],
                            document_type=DocumentType.WEBPAGE,
                            metadata=result["metadata"],
                            tags=request.document_tags,
                            source_url=result["url"],
                        )
                        results[i]["document_id"] = doc["id"]
                    except Exception:
                        pass
        return results
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Search and scrape failed: {str(e)}")


@router.post(
    "/sitemap",
    response_model=Dict[str, Any],
    summary="Scrape sitemap",
    description="Extract URLs from sitemap and scrape them",
)
async def scrape_sitemap(request: SitemapScrapeRequest = Body(...)):
    """
    Extract URLs from a sitemap and scrape them.
    Processes XML sitemap files and scrapes the listed URLs.
    """
    try:
        result = await scraper_service.scrape_sitemap(request.sitemap_url, request.max_urls)
        # Handle document creation if requested
        if request.create_documents and result.get("urls_scraped", []):
            document_ids = []
            for scraped_url in result["urls_scraped"]:
                if scraped_url.get("success", False):
                    try:
                        doc = documents_service.create_document(
                            title=scraped_url["title"],
                            content=scraped_url["content"],
                            document_type=DocumentType.WEBPAGE,
                            metadata=scraped_url["metadata"],
                            tags=request.document_tags,
                            source_url=scraped_url["url"],
                        )
                        document_ids.append(doc["id"])
                    except Exception:
                        pass
            result["documents_created"] = len(document_ids)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Sitemap scraping failed: {str(e)}")


================================================================================
FILE: app/models/serper.py
LANGUAGE: python
SIZE: 1677 bytes
================================================================================

from typing import List, Dict, Any, Optional
from pydantic import BaseModel, Field


class SerperSearchRequest(BaseModel):
    """Request to search using Serper API"""
    query: str = Field(..., description="Search query")
    search_type: str = Field("search", description="Type of search (search, news, images, places)")
    num_results: int = Field(10, description="Number of results to return")
    country: Optional[str] = Field(None, description="Country code for localized results")
    locale: Optional[str] = Field(None, description="Language code for localized results")
    auto_scrape: bool = Field(False, description="Whether to automatically scrape search results")
    max_scrape: int = Field(5, description="Maximum number of results to scrape if auto_scrape is True")
    create_documents: bool = Field(False, description="Whether to create documents from scraped content")
    document_tags: List[str] = Field(default_factory=list, description="Tags to apply to created documents")


class SerperSearchResponse(BaseModel):
    """Response from Serper search"""
    query: str = Field(..., description="Original search query")
    search_type: str = Field(..., description="Type of search that was performed")
    organic_results: List[Dict[str, Any]] = Field(default_factory=list, description="Organic search results")
    scrape_results: Optional[List[Dict[str, Any]]] = Field(None, description="Scraped content if auto_scrape was True")
    document_ids: Optional[List[str]] = Field(None, description="IDs of created documents if requested")
    error: Optional[str] = Field(None, description="Error message if the search failed")


================================================================================
FILE: app/serper/crawler/__init__.py
LANGUAGE: python
SIZE: 19 bytes
================================================================================

# Crawler package


================================================================================
FILE: app/serper/crawler/browser.py
LANGUAGE: python
SIZE: 5742 bytes
================================================================================

"""
Browser-based web crawler implementation using Playwright.
Provides functionality to crawl websites, extract content, and convert to various formats.
"""
import os
import logging
import base64
from typing import Dict, Any, List
from playwright.async_api import async_playwright, Route
from bs4 import BeautifulSoup

from utils.markdown import HtmlToMarkdown

logger = logging.getLogger(__name__)

class Crawler:
    """
    Web crawler class that uses Playwright to navigate websites and extract content.
    Supports content extraction in multiple formats and handles browser automation.
    """
    def __init__(self):
        self.browser = None
        self.context = None
        self.markdown_converter = HtmlToMarkdown()
        self.chrome_path = os.getenv("CHROME_PATH")
        self.proxy_url = os.getenv("PROXY_URL")

    async def __ensure_browser(self):
        """Ensure browser is launched if not already"""
        if not self.browser:
            playwright = await async_playwright().start()
            launch_args = {
                "headless": True
            }

            if self.chrome_path:
                launch_args["executable_path"] = self.chrome_path

            self.browser = await playwright.chromium.launch(**launch_args)

    async def crawl(self, url: str, **options) -> Dict[str, Any]:
        """
        Crawl a URL and return its content
        """
        logger.info("Crawling URL: %s", url)
        await self.__ensure_browser()

        # Extract options
        respond_with = options.get("respond_with", "markdown")
        target_selector = options.get("target_selector")
        remove_selector = options.get("remove_selector")
        timeout = options.get("timeout", 30)
        with_screenshots = options.get("with_screenshots", False)

        # Create browser context with options
        context_options = {
            "viewport": {"width": 1280, "height": 800},
            "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                          "(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

        if self.proxy_url:
            context_options["proxy"] = {"server": self.proxy_url}

        context = await self.browser.new_context(**context_options)
        page = await context.new_page()

        # Setup route handler to block unnecessary resources
        async def route_handler(route: Route):
            if route.request.resource_type in ["image", "media", "font", "stylesheet"]:
                await route.abort()
            else:
                await route.continue_()

        await page.route("**/*", route_handler)

        try:
            # Navigate to URL
            await page.goto(url, timeout=timeout * 1000, wait_until="networkidle")

            # Take screenshot if requested
            screenshot_data = None
            if with_screenshots:
                screenshot_data = await page.screenshot(type="jpeg", quality=80)
                screenshot_data = screenshot_data if screenshot_data else None

            # Get page content
            if target_selector:
                # Wait for selector
                await page.wait_for_selector(target_selector, timeout=timeout * 1000)

                # Remove elements if specified
                if remove_selector:
                    elements = await page.query_selector_all(remove_selector)
                    for element in elements:
                        await element.evaluate("el => el.remove()")

                # Get content of targeted elements
                content_html = await page.inner_html(target_selector)
            else:
                # Get full page content
                content_html = await page.content()

                # Process with BeautifulSoup to remove unwanted elements
                if remove_selector:
                    soup = BeautifulSoup(content_html, "html.parser")
                    for element in soup.select(remove_selector):
                        element.decompose()
                    content_html = str(soup)

            # Get page title
            title = await page.title()

            # Format output based on respond_with option
            if respond_with == "html":
                content = content_html
                content_type = "html"
            elif respond_with == "markdown":
                content = self.markdown_converter.convert(content_html)
                content_type = "markdown"
            elif respond_with == "text":
                content = await page.evaluate('() => document.body.innerText')
                content_type = "text"
            else:
                content = content_html
                content_type = "html"

            # Create result object
            result = {
                "url": url,
                "title": title,
                "content": content,
                "content_type": content_type
            }

            if screenshot_data:
                result["screenshot"] = base64.b64encode(screenshot_data).decode("utf-8")

            return result

        finally:
            await context.close()

    def extract_links(self, html_content: str) -> List[Dict[str, str]]:
        """
        Extract links from HTML content
        """
        soup = BeautifulSoup(html_content, "html.parser")
        links = []

        for a_tag in soup.find_all("a", href=True):
            link = {
                "url": a_tag["href"],
                "text": a_tag.text.strip()
            }
            links.append(link)

        return links

================================================================================
FILE: app/serper/embeddings/__init__.py
LANGUAGE: python
SIZE: 22 bytes
================================================================================

# Embeddings package


================================================================================
FILE: app/serper/embeddings/model.py
LANGUAGE: python
SIZE: 2878 bytes
================================================================================

"""
Embeddings model implementation using Hugging Face Transformers.
Provides text embedding generation and similarity calculation functionality.
"""
import os
import logging
from typing import List, Optional
import numpy as np
import torch
from transformers import AutoModel
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

class HfEmbeddings:
    """
    Hugging Face embeddings model class that handles text embedding generation
    and similarity calculations using pre-trained transformer models.
    """
    def __init__(self, model_name: Optional[str] = None):
        """
        Initialize Embeddings model
        """
        model_name = model_name or os.getenv("EMBEDDING_MODEL", "")
        logger.info("Loading embeddings model: %s", model_name)

        try:
            self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
            # Move to GPU if available
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            self.model.to(self.device)
            logger.info("Model loaded successfully on %s", self.device)
        except Exception as e:
            logger.error("Error loading model: %s", str(e))
            logger.error("Please make sure you are logged into Hugging Face. "
                         "Run: huggingface-cli login")
            raise

    def embed(self, text: str, max_length: int = 2048) -> List[float]:
        """
        Generate embeddings for a single text
        """
        return self.embed_batch([text], max_length)[0]

    def embed_batch(self, texts: List[str], max_length: int = 2048) -> List[List[float]]:
        """
        Generate embeddings for a batch of texts
        """
        try:
            with torch.no_grad():
                embeddings = self.model.encode(texts, max_length=max_length)

            # Convert to Python list for JSON serialization
            if isinstance(embeddings, torch.Tensor):
                embeddings = embeddings.cpu().numpy()

            if isinstance(embeddings, np.ndarray):
                embeddings = embeddings.tolist()

            return embeddings
        except Exception as e:
            logger.error("Error generating embeddings: %s", str(e))
            raise

    def similarity(self, text1: str, text2: str) -> float:
        """
        Calculate cosine similarity between two texts
        """
        emb1 = self.embed(text1)
        emb2 = self.embed(text2)

        # Compute cosine similarity
        dot_product = sum(a * b for a, b in zip(emb1, emb2))
        magnitude1 = sum(a * a for a in emb1) ** 0.5
        magnitude2 = sum(b * b for b in emb2) ** 0.5

        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0

        return dot_product / (magnitude1 * magnitude2)

================================================================================
FILE: app/serper/requirements.txt
LANGUAGE: text
SIZE: 210 bytes
================================================================================

fastapi>=0.104.0
uvicorn>=0.23.2
python-dotenv>=1.0.0
aiohttp>=3.8.5
beautifulsoup4>=4.12.2
playwright>=1.39.0
transformers>=4.35.0
torch>=2.1.0
numpy>=1.24.3
huggingface-hub>=0.17.3
pydantic>=2.4.2


================================================================================
FILE: app/serper/search/__init__.py
LANGUAGE: python
SIZE: 18 bytes
================================================================================

# Search package


================================================================================
FILE: app/serper/search/google.py
LANGUAGE: python
SIZE: 6122 bytes
================================================================================

import os
import logging
import aiohttp
import asyncio
from typing import List, Dict, Any, Optional
from bs4 import BeautifulSoup
from urllib.parse import quote
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

class GoogleSearch:
    def __init__(self):
        self.google_domain = os.getenv("GOOGLE_DOMAIN", "www.google.com")
        self.proxy_url = os.getenv("PROXY_URL")
        logger.info("Initializing Google search")

    async def search(self, query: str, variant: str = "web", num: int = 10, page: int = 1) -> List[Dict[str, Any]]:
        """
        Perform Google search
        """
        logger.info(f"Searching for: {query} (variant: {variant})")

        # Calculate start parameter based on page
        start = (page - 1) * num

        # Build search URL
        url = f"https://{self.google_domain}/search?"
        params = {
            "q": query,
            "num": str(num),
        }

        if start > 0:
            params["start"] = str(start)

        if variant == "images":
            params["tbm"] = "isch"
        elif variant == "news":
            params["tbm"] = "nws"

        # Prepare URL
        for key, value in params.items():
            url += f"{key}={quote(value)}&"
        url = url[:-1]  # Remove trailing &

        # Setup HTTP client
        async with aiohttp.ClientSession() as session:
            # Setup proxy if specified
            proxy = self.proxy_url if self.proxy_url else None

            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.5",
            }

            try:
                async with session.get(url, headers=headers, proxy=proxy) as response:
                    if response.status != 200:
                        logger.error(f"Error fetching search results: {response.status}")
                        return []

                    html = await response.text()

                    # Extract results based on variant
                    if variant == "images":
                        return self._parse_image_results(html)
                    elif variant == "news":
                        return self._parse_news_results(html)
                    else:
                        return self._parse_web_results(html)
            except Exception as e:
                logger.error(f"Error during search: {str(e)}")
                return []

    def _parse_web_results(self, html: str) -> List[Dict[str, Any]]:
        """
        Parse web search results
        """
        soup = BeautifulSoup(html, "html.parser")
        results = []

        # Extract search results
        for result in soup.select("div.g"):
            try:
                link_element = result.select_one("a")
                if not link_element or not link_element.get("href"):
                    continue

                link = link_element["href"]
                if not link.startswith("http"):
                    continue

                title_element = result.select_one("h3")
                title = title_element.text if title_element else "No title"

                snippet_element = result.select_one("div.VwiC3b")
                snippet = snippet_element.text if snippet_element else "No description"

                results.append({
                    "title": title,
                    "link": link,
                    "snippet": snippet,
                })
            except Exception as e:
                logger.error(f"Error parsing result: {str(e)}")
                continue

        return results

    def _parse_image_results(self, html: str) -> List[Dict[str, Any]]:
        """
        Parse image search results
        """
        soup = BeautifulSoup(html, "html.parser")
        results = []

        for img in soup.select("img.rg_i"):
            try:
                src = img.get("src") or img.get("data-src")
                if not src:
                    continue

                alt = img.get("alt", "No description")

                results.append({
                    "title": alt,
                    "thumbnail": src,
                    "source": img.parent.get("href", "#") if img.parent else "#",
                })
            except Exception as e:
                logger.error(f"Error parsing image result: {str(e)}")
                continue

        return results

    def _parse_news_results(self, html: str) -> List[Dict[str, Any]]:
        """
        Parse news search results
        """
        soup = BeautifulSoup(html, "html.parser")
        results = []

        for article in soup.select("div.SoaBEf"):
            try:
                link_element = article.select_one("a")
                if not link_element or not link_element.get("href"):
                    continue

                link = link_element["href"]
                if not link.startswith("http"):
                    continue

                title_element = article.select_one("div.n0jPhd")
                title = title_element.text if title_element else "No title"

                source_element = article.select_one("div.CEMjEf")
                source = source_element.text if source_element else "Unknown source"

                snippet_element = article.select_one("div.GI74Re")
                snippet = snippet_element.text if snippet_element else "No description"

                results.append({
                    "title": title,
                    "link": link,
                    "source": source,
                    "snippet": snippet,
                })
            except Exception as e:
                logger.error(f"Error parsing news result: {str(e)}")
                continue

        return results


================================================================================
FILE: app/serper/server.py
LANGUAGE: python
SIZE: 5863 bytes
================================================================================

"""
FastAPI server that provides a web crawler, embeddings generation, and search functionality.
This server implements endpoints for crawling URLs, generating text embeddings,
performing web searches, and calculating text similarity.
"""
import os
import logging
from typing import List, Dict, Any, Optional, Union
from urllib.parse import unquote

import uvicorn
from fastapi import FastAPI, HTTPException, Query, Path
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv

from crawler.browser import Crawler
from embeddings.model import HfEmbeddings
from search.google import GoogleSearch
from utils.markdown import HtmlToMarkdown

# Load environment variables
load_dotenv()

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI(
    title="Standalone Crawler with Embeddings",
    description="Web crawling, scraping, and content analysis using local embeddings",
    version="1.0.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize components
crawler = Crawler()
embeddings_model = HfEmbeddings()
search_engine = GoogleSearch()
markdown_converter = HtmlToMarkdown()

# API Models
class EmbeddingRequest(BaseModel):
    """Model for embedding request containing text to be embedded."""
    text: Union[str, List[str]]

class EmbeddingResponse(BaseModel):
    """Model for embedding response containing generated embeddings."""
    embeddings: List[List[float]]

class SearchResponse(BaseModel):
    """Model for search response containing search results."""
    query: str
    results: List[Dict[str, Any]]

class CrawlResponse(BaseModel):
    """Model for crawl response containing webpage content."""
    url: str
    title: str
    content: str
    content_type: str
    screenshot: Optional[str] = None

# API Routes
@app.get("/health")
async def health_check():
    """Health check endpoint to verify server is running."""
    return {"status": "ok"}

@app.get("/crawl/{url:path}", response_model=CrawlResponse)
async def crawl_url(
    url: str = Path(..., description="URL to crawl (will be URL-decoded)"),
    respond_with: str = Query("markdown", description="Response format (markdown, html, text)"),
    target_selector: Optional[str] = Query(None, description="CSS selector to target specific content"),
    remove_selector: Optional[str] = Query(None, description="CSS selector for elements to remove"),
    timeout: Optional[int] = Query(30, description="Timeout in seconds"),
    with_screenshots: Optional[bool] = Query(False, description="Include screenshots in response"),
):
    """
    Crawl a URL and return its content in the specified format.
    """
    try:
        # Decode URL
        decoded_url = unquote(url)

        # Create options dict
        options = {
            "respond_with": respond_with,
            "target_selector": target_selector,
            "remove_selector": remove_selector,
            "timeout": timeout,
            "with_screenshots": with_screenshots
        }

        # Perform crawl
        result = await crawler.crawl(decoded_url, **options)
        return result
    except Exception as e:
        logger.error("Error crawling URL: %s", str(e))
        raise HTTPException(status_code=500, detail=str(e)) from e

@app.post("/embeddings", response_model=EmbeddingResponse)
async def generate_embeddings(request: EmbeddingRequest):
    """
    Generate embeddings for the provided text or texts.
    """
    try:
        # Process text or array of texts
        texts = request.text if isinstance(request.text, list) else [request.text]

        # Generate embeddings
        embeddings = embeddings_model.embed_batch(texts)

        return {"embeddings": embeddings}
    except Exception as e:
        logger.error("Error generating embeddings: %s", str(e))
        raise HTTPException(status_code=500, detail=str(e)) from e

@app.get("/search", response_model=SearchResponse)
async def search(
    q: str = Query(..., description="Search query"),
    num: Optional[int] = Query(10, description="Number of results"),
    variant: Optional[str] = Query("web", description="Search type (web, images, news)"),
    page: Optional[int] = Query(1, description="Page number"),
):
    """
    Perform a search using the configured search engine.
    """
    try:
        # Perform search
        results = await search_engine.search(q, variant=variant, num=num, page=page)
        return {
            "query": q,
            "results": results
        }
    except Exception as e:
        logger.error("Error performing search: %s", str(e))
        raise HTTPException(status_code=500, detail=str(e)) from e

# Text similarity endpoint
@app.post("/similarity")
async def compare_similarity(
    text1: str = Query(..., description="First text to compare"),
    text2: str = Query(..., description="Second text to compare"),
):
    """
    Calculate cosine similarity between two text strings.
    """
    try:
        similarity = embeddings_model.similarity(text1, text2)
        return {"similarity": similarity}
    except Exception as e:
        logger.error("Error calculating similarity: %s", str(e))
        raise HTTPException(status_code=500, detail=str(e)) from e

# Main entry point
if __name__ == "__main__":
    port = int(os.getenv("PORT", "3000"))
    logger.info("Starting server on port %s", port)
    uvicorn.run("server:app", host="0.0.0.0", port=port, reload=True)

================================================================================
FILE: app/serper/utils/__init__.py
LANGUAGE: python
SIZE: 17 bytes
================================================================================

# Utils package


================================================================================
FILE: app/serper/utils/config.py
LANGUAGE: python
SIZE: 2806 bytes
================================================================================

import os
from typing import List, Optional

# Third-party imports
import dotenv
from pydantic import Field as PydanticField

# Use pydantic_settings if available, otherwise fallback
try:
    from pydantic_settings import BaseSettings
except ImportError:
    from pydantic import BaseSettings  # Fallback for older pydantic versions

# Load environment variables
dotenv.load_dotenv()


class Config(BaseSettings):
    server_host: str = PydanticField(default=os.getenv("SERVER_HOST", "0.0.0.0"))
    server_port: int = PydanticField(default=int(os.getenv("SERVER_PORT", "8000")))
    dev_mode: bool = PydanticField(default=os.getenv("DEV_MODE", "False").lower() == "true")

    allowed_directories: List[str] = PydanticField(
        default_factory=lambda: os.getenv("ALLOWED_DIRS", "./data").split(",")
    )
    file_cache_enabled: bool = PydanticField(
        default=os.getenv("FILE_CACHE_ENABLED", "False").lower() == "true"
    )

    memory_file_path: str = PydanticField(
        default=os.getenv("MEMORY_FILE_PATH", "./data/memory.json")
    )
    use_graph_db: bool = PydanticField(
        default=os.getenv("USE_GRAPH_DB", "False").lower() == "true"
    )

    default_git_username: str = PydanticField(
        default=os.getenv("DEFAULT_COMMIT_USERNAME", "UnifiedTools")
    )
    default_git_email: str = PydanticField(
        default=os.getenv("DEFAULT_COMMIT_EMAIL", "tools@example.com")
    )

    s3_access_key: Optional[str] = PydanticField(default=os.getenv("S3_ACCESS_KEY"))
    s3_secret_key: Optional[str] = PydanticField(default=os.getenv("S3_SECRET_KEY"))
    s3_region: Optional[str] = PydanticField(default=os.getenv("S3_REGION"))
    s3_bucket: Optional[str] = PydanticField(default=os.getenv("S3_BUCKET"))

    scraper_min_delay: float = PydanticField(
        default=float(os.getenv("SCRAPER_MIN_DELAY", "3"))
    )
    scraper_max_delay: float = PydanticField(
        default=float(os.getenv("SCRAPER_MAX_DELAY", "7"))
    )
    user_agent: str = PydanticField(
        default=os.getenv("USER_AGENT", "Mozilla/5.0 (compatible; UnifiedToolsServer/1.0)")
    )
    scraper_data_path: str = PydanticField(
        default=os.getenv("SCRAPER_DATA_PATH", "./data/scraped")
    )

    # Special configuration for pydantic
    model_config = {
        "env_file": ".env",
        "case_sensitive": False,
    }


# Singleton instance using function attribute pattern
def get_config() -> Config:
    """
    Get the singleton configuration instance.
    
    Returns:
        Config: The configuration object with settings from environment variables
    """
    if not hasattr(get_config, "_config_instance"):
        get_config._config_instance = Config()
    return get_config._config_instance


================================================================================
FILE: app/serper/utils/markdown.py
LANGUAGE: python
SIZE: 5392 bytes
================================================================================

import logging
import re
from bs4 import BeautifulSoup
from typing import Optional, Dict, Any

logger = logging.getLogger(__name__)

class HtmlToMarkdown:
    def __init__(self):
        # Common replacements for markdown conversion
        self.replacements = [
            # Headers
            (re.compile(r'<h1[^>]*>(.*?)</h1>', re.DOTALL), r'# \1\n'),
            (re.compile(r'<h2[^>]*>(.*?)</h2>', re.DOTALL), r'## \1\n'),
            (re.compile(r'<h3[^>]*>(.*?)</h3>', re.DOTALL), r'### \1\n'),
            (re.compile(r'<h4[^>]*>(.*?)</h4>', re.DOTALL), r'#### \1\n'),
            (re.compile(r'<h5[^>]*>(.*?)</h5>', re.DOTALL), r'##### \1\n'),
            (re.compile(r'<h6[^>]*>(.*?)</h6>', re.DOTALL), r'###### \1\n'),

            # Bold and italic
            (re.compile(r'<strong[^>]*>(.*?)</strong>', re.DOTALL), r'**\1**'),
            (re.compile(r'<b[^>]*>(.*?)</b>', re.DOTALL), r'**\1**'),
            (re.compile(r'<em[^>]*>(.*?)</em>', re.DOTALL), r'*\1*'),
            (re.compile(r'<i[^>]*>(.*?)</i>', re.DOTALL), r'*\1*'),

            # Lists
            (re.compile(r'<ul[^>]*>(.*?)</ul>', re.DOTALL), self._process_ul),
            (re.compile(r'<ol[^>]*>(.*?)</ol>', re.DOTALL), self._process_ol),

            # Links
            (re.compile(r'<a[^>]*href="([^"]*)"[^>]*>(.*?)</a>', re.DOTALL), r'[\2](\1)'),

            # Images
            (re.compile(r'<img[^>]*src="([^"]*)"[^>]*alt="([^"]*)"[^>]*>', re.DOTALL), r'![\2](\1)'),
            (re.compile(r'<img[^>]*src="([^"]*)"[^>]*>', re.DOTALL), r'![](\1)'),

            # Code
            (re.compile(r'<pre[^>]*><code[^>]*>(.*?)</code></pre>', re.DOTALL), r'```\n\1\n```'),
            (re.compile(r'<code[^>]*>(.*?)</code>', re.DOTALL), r'`\1`'),

            # Blockquotes
            (re.compile(r'<blockquote[^>]*>(.*?)</blockquote>', re.DOTALL), self._process_blockquote),

            # Paragraphs and breaks
            (re.compile(r'<p[^>]*>(.*?)</p>', re.DOTALL), r'\1\n\n'),
            (re.compile(r'<br[^>]*>', re.DOTALL), r'\n'),

            # Tables - more complex, handled separately
        ]

    def _process_ul(self, match):
        content = match.group(1)
        soup = BeautifulSoup(content, 'html.parser')
        result = "\n"
        for li in soup.find_all('li'):
            result += f"- {li.get_text().strip()}\n"
        return result + "\n"

    def _process_ol(self, match):
        content = match.group(1)
        soup = BeautifulSoup(content, 'html.parser')
        result = "\n"
        for i, li in enumerate(soup.find_all('li')):
            result += f"{i+1}. {li.get_text().strip()}\n"
        return result + "\n"

    def _process_blockquote(self, match):
        content = match.group(1)
        lines = content.split('\n')
        result = "\n"
        for line in lines:
            stripped = line.strip()
            if stripped:
                result += f"> {stripped}\n"
        return result + "\n"

    def _process_table(self, soup):
        tables = soup.find_all('table')
        if not tables:
            return soup

        for table in tables:
            rows = table.find_all('tr')
            if not rows:
                continue

            markdown_table = []

            # Process header row
            header_cells = rows[0].find_all(['th', 'td'])
            if header_cells:
                header_row = "| " + " | ".join([cell.get_text().strip() for cell in header_cells]) + " |"
                markdown_table.append(header_row)

                # Add separator row
                separator_row = "| " + " | ".join(["---" for _ in header_cells]) + " |"
                markdown_table.append(separator_row)

            # Process data rows
            for row in rows[1:]:
                cells = row.find_all('td')
                if cells:
                    data_row = "| " + " | ".join([cell.get_text().strip() for cell in cells]) + " |"
                    markdown_table.append(data_row)

            # Replace table with markdown
            table_markdown = "\n" + "\n".join(markdown_table) + "\n\n"
            new_tag = soup.new_tag('div', attrs={'class': 'markdown-table'})
            new_tag.string = table_markdown
            table.replace_with(new_tag)

        return soup

    def convert(self, html: str) -> str:
        """
        Convert HTML to Markdown
        """
        # First pass - clean up HTML with BeautifulSoup
        soup = BeautifulSoup(html, 'html.parser')

        # Remove script and style elements
        for element in soup(['script', 'style', 'iframe', 'noscript']):
            element.decompose()

        # Process tables
        soup = self._process_table(soup)

        cleaned_html = str(soup)

        # Apply regular expressions
        markdown = cleaned_html
        for pattern, replacement in self.replacements:
            if callable(replacement):
                markdown = pattern.sub(replacement, markdown)
            else:
                markdown = pattern.sub(replacement, markdown)

        # Post-processing cleanup
        markdown = re.sub(r'\n{3,}', '\n\n', markdown)  # Remove extra newlines
        markdown = re.sub(r'<.*?>', '', markdown)  # Remove any remaining HTML tags

        return markdown.strip()


================================================================================
FILE: app/utils/config.py
LANGUAGE: python
SIZE: 3331 bytes
================================================================================

import os
from typing import Optional, List
from dotenv import load_dotenv

class Config:
    # Server settings
    server_host: str = "0.0.0.0"
    server_port: int = 8000
    dev_mode: bool = False
    
    # Filesystem settings
    allowed_directories: List[str] = ["./data"]
    memory_file_path: str = "./data/memory.json"
    file_cache_enabled: bool = True
    file_cache_max_age: int = 3600  # 1 hour in seconds
    
    # Git settings
    default_git_username: str = "OtherTales"
    default_git_email: str = "system@othertales.com"
    
    # S3 storage settings
    s3_access_key: Optional[str] = None
    s3_secret_key: Optional[str] = None
    s3_region: str = "us-east-1"
    s3_endpoint_url: Optional[str] = None
    
    # Search API settings (using Serper)
    search_api_key: Optional[str] = None  # Serper API key
    search_provider: str = "serper"  # Provider name (for future extensibility)
    search_default_country: str = "us"
    search_default_locale: str = "en"
    search_timeout: int = 30
    search_max_retries: int = 3
    search_retry_delay: int = 2
    
    # Memory settings
    use_graph_db: bool = False

config = Config()

def load_dotenv_config():
    """Load configuration from environment variables"""
    load_dotenv()

    # Server settings
    config.server_host = os.getenv("SERVER_HOST", "0.0.0.0")
    config.server_port = int(os.getenv("SERVER_PORT", "8000"))
    config.dev_mode = os.getenv("DEV_MODE", "False").lower() in ("true", "1", "yes")
    
    # Filesystem settings
    allowed_dirs_str = os.getenv("ALLOWED_DIRS", "./data")
    config.allowed_directories = [d.strip() for d in allowed_dirs_str.split(",")]
    config.memory_file_path = os.getenv("MEMORY_FILE_PATH", "./data/memory.json")
    config.file_cache_enabled = os.getenv("FILE_CACHE_ENABLED", "True").lower() in ("true", "1", "yes")
    config.file_cache_max_age = int(os.getenv("FILE_CACHE_MAX_AGE", "3600"))
    
    # Git settings
    config.default_git_username = os.getenv("DEFAULT_COMMIT_USERNAME", "OtherTales")
    config.default_git_email = os.getenv("DEFAULT_COMMIT_EMAIL", "system@othertales.com")
    
    # S3 settings
    config.s3_access_key = os.getenv("S3_ACCESS_KEY")
    config.s3_secret_key = os.getenv("S3_SECRET_KEY")
    config.s3_region = os.getenv("S3_REGION", "us-east-1")
    config.s3_endpoint_url = os.getenv("S3_ENDPOINT_URL")
    
    # Load search settings (both old and new keys for backward compatibility)
    config.search_api_key = os.getenv("SEARCH_API_KEY") or os.getenv("SERPER_API_KEY")
    config.search_provider = os.getenv("SEARCH_PROVIDER", "serper")
    config.search_default_country = os.getenv("SEARCH_DEFAULT_COUNTRY", "us")
    config.search_default_locale = os.getenv("SEARCH_DEFAULT_LOCALE", "en")
    config.search_timeout = int(os.getenv("SEARCH_TIMEOUT", "30"))
    config.search_max_retries = int(os.getenv("SEARCH_MAX_RETRIES", "3"))
    config.search_retry_delay = int(os.getenv("SEARCH_RETRY_DELAY", "2"))
    
    # Memory settings
    config.use_graph_db = os.getenv("USE_GRAPH_DB", "False").lower() in ("true", "1", "yes")

def get_config() -> Config:
    """Return the singleton config instance"""
    return config

# Load config at import time
load_dotenv_config()


================================================================================
FILE: data/user_preferences/test_user.json
LANGUAGE: json
SIZE: 44 bytes
================================================================================

{
  "theme": "dark",
  "language": "en"
}

================================================================================
FILE: export.py
LANGUAGE: python
SIZE: 7281 bytes
================================================================================

import os
import sys
import argparse
from pathlib import Path
import datetime
import fnmatch
import re


def parse_arguments():
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(description="Export codebase to a single text file")
    parser.add_argument(
        "-d",
        "--directory",
        type=str,
        default=os.getcwd(),
        help="Root directory of the codebase (default: current directory)",
    )
    parser.add_argument(
        "-o",
        "--output",
        type=str,
        default=f'codebase_export_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}.txt',
        help="Output file path (default: codebase_export_<timestamp>.txt)",
    )
    parser.add_argument(
        "-e",
        "--extensions",
        type=str,
        default=".py,.js,.java,.cpp,.h,.html,.css,.md,.txt,.json,.yml,.ts,.ico,.idx,.keep,.pack,.rev,.sample",
        help="Comma-separated list of file extensions to include (default: common code files)",
    )
    parser.add_argument(
        "-x",
        "--exclude",
        type=str,
        default="node_modules,venv,.git,__pycache__,*.pyc,*.pyo,*.pyd,*.so,*.dll,*.exe",
        help="Comma-separated list of directories and file patterns to exclude",
    )
    parser.add_argument(
        "--max-size",
        type=int,
        default=1024 * 1024,
        help="Maximum file size to include in bytes (default: 1MB)",
    )
    parser.add_argument(
        "--include-line-numbers", action="store_true", help="Include line numbers in the output"
    )
    parser.add_argument(
        "--toc", action="store_true", help="Generate a table of contents at the beginning"
    )
    parser.add_argument(
        "--header",
        type=str,
        default="Codebase Export - {timestamp}",
        help="Header text for the export file. Use {timestamp} for current timestamp.",
    )
    return parser.parse_args()


def should_include_file(file_path, args):
    """Determine if a file should be included in the export."""
    # Check extension
    extensions = args.extensions.split(",")
    if not any(file_path.name.endswith(ext) for ext in extensions):
        return False
    # Check excluded patterns
    exclude_patterns = args.exclude.split(",")
    for pattern in exclude_patterns:
        if fnmatch.fnmatch(file_path.name, pattern):
            return False
    # Check if file is in excluded directory
    for pattern in exclude_patterns:
        if pattern in str(file_path):
            return False
    # Check file size
    if file_path.stat().st_size > args.max_size:
        print(f"Skipping large file: {file_path} ({file_path.stat().st_size} bytes)")
        return False
    return True


def get_file_language(file_path):
    """Determine the programming language based on file extension."""
    extension = file_path.suffix.lower()
    language_map = {
        ".py": "python",
        ".js": "javascript",
        ".jsx": "javascript",
        ".ts": "typescript",
        ".tsx": "typescript",
        ".java": "java",
        ".c": "c",
        ".cpp": "cpp",
        ".h": "cpp",
        ".hpp": "cpp",
        ".cs": "csharp",
        ".go": "go",
        ".rb": "ruby",
        ".php": "php",
        ".swift": "swift",
        ".kt": "kotlin",
        ".hs": "haskell",
        ".rs": "rust",
        ".html": "html",
        ".css": "css",
        ".scss": "scss",
        ".sass": "scss",
        ".json": "json",
        ".xml": "xml",
        ".yaml": "yaml",
        ".yml": "yaml",
        ".md": "markdown",
        ".sh": "bash",
        ".bat": "batch",
        ".ps1": "powershell",
        ".sql": "sql",
        ".r": "r",
        ".ico": "binary",
        ".idx": "binary",
        ".keep": "text",
        ".pack": "binary",
        ".rev": "text",
        ".sample": "text",
        ".txt": "text",
    }
    return language_map.get(extension, "text")


def scan_directory(root_dir, files_list, args):
    """Recursively scan directory and collect files to include."""
    root_path = Path(root_dir)
    exclude_dirs = [item for item in args.exclude.split(",") if not item.startswith("*")]
    for path in root_path.rglob("*"):
        if any(
            exclude_dir in str(path.relative_to(root_path))
            for exclude_dir in exclude_dirs
            if exclude_dir
        ):
            continue
        if path.is_file() and should_include_file(path, args):
            files_list.append(path)


def create_separator(length=80):
    """Create a separator line."""
    return "=" * length


def format_file_header(file_path, root_dir):
    """Format header for a file."""
    rel_path = os.path.relpath(file_path, root_dir)
    header = create_separator()
    header += f"\nFILE: {rel_path}\n"
    header += f"LANGUAGE: {get_file_language(file_path)}\n"
    header += f"SIZE: {file_path.stat().st_size} bytes\n"
    header += create_separator()
    return header


def generate_toc(files_list, root_dir):
    """Generate table of contents."""
    toc = "TABLE OF CONTENTS\n"
    toc += create_separator() + "\n"
    for i, file_path in enumerate(files_list, 1):
        rel_path = os.path.relpath(file_path, root_dir)
        toc += f"{i}. {rel_path}\n"
    toc += create_separator() + "\n\n"
    return toc


def export_codebase(args):
    """Export codebase files to a single text file."""
    root_dir = os.path.abspath(args.directory)
    output_file = args.output
    files_list = []
    print(f"Scanning directory: {root_dir}")
    scan_directory(root_dir, files_list, args)
    # Sort files alphabetically
    files_list.sort()
    print(f"Found {len(files_list)} files to export")
    with open(output_file, "w", encoding="utf-8") as f:
        # Write header
        header = args.header.replace(
            "{timestamp}", datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        )
        f.write(header + "\n\n")
        # Write table of contents if requested
        if args.toc:
            f.write(generate_toc(files_list, root_dir))
        # Process each file
        for file_path in files_list:
            try:
                # Write file header
                f.write(format_file_header(file_path, root_dir) + "\n\n")
                # Read and write file content with optional line numbers
                with open(file_path, "r", encoding="utf-8", errors="replace") as source_file:
                    if args.include_line_numbers:
                        for i, line in enumerate(source_file, 1):
                            f.write(f"{i:4d} | {line}")
                    else:
                        f.write(source_file.read())
                # Add newlines for spacing
                f.write("\n\n")
            except Exception as e:
                f.write(f"ERROR: Could not read file {file_path}: {str(e)}\n\n")
    print(f"Export completed successfully to: {output_file}")
    print(f"Total size: {os.path.getsize(output_file)} bytes")


def main():
    """Main entry point."""
    args = parse_arguments()
    export_codebase(args)


if __name__ == "__main__":
    main()


================================================================================
FILE: main.py
LANGUAGE: python
SIZE: 3013 bytes
================================================================================

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.openapi.utils import get_openapi
from app.api import filesystem, memory, git, scraper, documents
from app.utils.config import get_config

app = FastAPI(
    title="othertales System Tools",
    version="1.0.0",
    description="A unified server providing filesystem, memory, git, web scraping, and document management tools for LLMs via OpenWebUI.",
)
# Configure CORS specifically for Open WebUI compatibility
origins = [
    # In production, remove the wildcard "*" and list only trusted domains
    # "*",  # Too permissive for production
    "https://ai.othertales.co",
    "https://legal.othertales.co",
    "https://mixture.othertales.co",
    # Add more specific domains as needed
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "HEAD", "PATCH"],
    allow_headers=["*"],
    expose_headers=["Content-Length"],
    max_age=600,  # Cache CORS preflight requests
)
# Include routers with well-defined tags for better OpenAPI organization
app.include_router(filesystem.router, prefix="/fs", tags=["Filesystem"])
app.include_router(memory.router, prefix="/memory", tags=["Memory"])
app.include_router(git.router, prefix="/git", tags=["Git"])
app.include_router(scraper.router, prefix="/scraper", tags=["Web Scraper"])
app.include_router(documents.router, prefix="/docs", tags=["Document Management"])


# Custom OpenAPI schema generator optimized for Open WebUI
def custom_openapi():
    if app.openapi_schema:
        return app.openapi_schema
    openapi_schema = get_openapi(
        title="Unified Tools Server",
        version=app.version,
        description="Document storage and retrieval system with Git versioning, knowledge graph, and web scraping capabilities.",
        routes=app.routes,
    )
    # Add tool metadata for Open WebUI
    openapi_schema["info"]["x-logo"] = {
        "url": "https://cdn-icons-png.flaticon.com/512/8728/8728086.png"
    }
    # Add toolkit info for better Open WebUI integration
    openapi_schema["info"]["x-openwebui-toolkit"] = {
        "category": "document-management",
        "capabilities": ["document-storage", "web-scraping", "git-versioning", "memory"],
        "auth_required": False,
    }
    app.openapi_schema = openapi_schema
    return app.openapi_schema


# Set custom OpenAPI schema generator
app.openapi = custom_openapi


@app.get("/")
async def root():
    return {
        "message": "Unified Tools Server API",
        "services": ["filesystem", "memory", "git", "scraper", "documents"],
        "version": "1.0.0",
        "openapi_url": "/openapi.json",
    }


if __name__ == "__main__":
    import uvicorn

    config = get_config()
    uvicorn.run(
        "main:app", host=config.server_host, port=config.server_port, reload=config.dev_mode
    )


================================================================================
FILE: pyrightconfig.json
LANGUAGE: json
SIZE: 345 bytes
================================================================================

{
    "include": [
        "app",
        "tests"
    ],
    "exclude": [
        "**/node_modules",
        "**/__pycache__",
        "venv"
    ],
    "venvPath": ".",
    "venv": "venv",
    "reportMissingImports": "warning",
    "reportMissingTypeStubs": false,
    "pythonVersion": "3.9",
    "typeCheckingMode": "basic"
}


================================================================================
FILE: requirements.txt
LANGUAGE: text
SIZE: 366 bytes
================================================================================

aiohttp==3.10.11
beautifulsoup4==4.13.3
boto3==1.37.23
docx==0.2.4
fastapi==0.115.12
networkx==3.4.2
numpy==2.2.4
playwright==1.51.0
pydantic==2.11.2
pydantic_settings==2.8.1
pytest==8.3.5
python-dotenv==1.1.0
Requests==2.32.3
sentence_transformers==4.0.1
uvicorn==0.34.0
weasyprint==65.0
gitpython==3.1.44

# Serper API dependencies
httpx>=0.24.0

================================================================================
FILE: tests/__init__.py
LANGUAGE: python
SIZE: 80 bytes
================================================================================

# This file is intentionally left empty to make the directory a Python package


================================================================================
FILE: tests/api/__init__.py
LANGUAGE: python
SIZE: 80 bytes
================================================================================

# This file is intentionally left empty to make the directory a Python package


================================================================================
FILE: tests/api/test_filesystem_api.py
LANGUAGE: python
SIZE: 6318 bytes
================================================================================

import pytest
from unittest.mock import patch, MagicMock
from fastapi.testclient import TestClient
from fastapi import FastAPI
import os
import io

from app.api.filesystem import router
from app.models.filesystem import DirectoryListingResponse

# Create test app
app = FastAPI()
app.include_router(router, prefix="/fs")
client = TestClient(app)


class TestFilesystemAPI:
    @patch("app.api.filesystem.filesystem_service")
    def test_read_file(self, mock_fs_service):
        # Mock the service
        mock_fs_service.read_file.return_value = "File content"

        # Send request
        response = client.post("/fs/read", json={"path": "/test/file.txt", "storage": "local"})

        # Verify response
        assert response.status_code == 200
        assert response.text == "File content"
        mock_fs_service.read_file.assert_called_once()

    @patch("app.api.filesystem.filesystem_service")
    def test_write_file(self, mock_fs_service):
        # Mock the service
        mock_fs_service.write_file.return_value = "Successfully wrote to /test/file.txt"

        # Send request
        response = client.post(
            "/fs/write",
            json={"path": "/test/file.txt", "content": "New content", "storage": "local"},
        )

        # Verify response
        assert response.status_code == 200
        assert response.text == "Successfully wrote to /test/file.txt"
        mock_fs_service.write_file.assert_called_once()

    @patch("app.api.filesystem.filesystem_service")
    def test_list_directory(self, mock_fs_service):
        # Mock the service
        mock_fs_service.list_directory.return_value = {
            "path": "/test",
            "items": [
                {
                    "name": "file1.txt",
                    "path": "file1.txt",
                    "type": "file",
                    "size": 100,
                    "last_modified": 1609459200,
                },
                {
                    "name": "subdir",
                    "path": "subdir",
                    "type": "directory",
                    "size": None,
                    "last_modified": None,
                },
            ],
        }

        # Send request
        response = client.post(
            "/fs/list", json={"path": "/test", "storage": "local", "recursive": False}
        )

        # Verify response
        assert response.status_code == 200
        assert response.json()["path"] == "/test"
        assert len(response.json()["items"]) == 2
        assert response.json()["items"][0]["name"] == "file1.txt"
        mock_fs_service.list_directory.assert_called_once()

    @patch("app.api.filesystem.filesystem_service")
    def test_search_files(self, mock_fs_service):
        # Mock the service
        mock_fs_service.search_files.return_value = ["/test/file1.txt", "/test/subdir/file2.txt"]

        # Send request
        response = client.post(
            "/fs/search", json={"path": "/test", "pattern": "*.txt", "storage": "local"}
        )

        # Verify response
        assert response.status_code == 200
        assert len(response.json()) == 2
        assert "/test/file1.txt" in response.json()
        mock_fs_service.search_files.assert_called_once()

    @patch("app.api.filesystem.filesystem_service")
    def test_create_directory(self, mock_fs_service):
        # Mock the service
        mock_fs_service.create_directory.return_value = (
            "Successfully created directory /test/newdir"
        )

        # Send request
        response = client.post("/fs/mkdir", json={"path": "/test/newdir", "storage": "local"})

        # Verify response
        assert response.status_code == 200
        assert response.text == "Successfully created directory /test/newdir"
        mock_fs_service.create_directory.assert_called_once()

    @patch("app.api.filesystem.filesystem_service")
    def test_delete_file(self, mock_fs_service):
        # Mock the service
        mock_fs_service.delete_file.return_value = "Successfully deleted /test/file.txt"

        # Send request
        response = client.post("/fs/delete", json={"path": "/test/file.txt", "storage": "local"})

        # Verify response
        assert response.status_code == 200
        assert response.text == "Successfully deleted /test/file.txt"
        mock_fs_service.delete_file.assert_called_once()

    @patch("app.api.filesystem.filesystem_service")
    def test_upload_file(self, mock_fs_service):
        # Mock the service
        mock_fs_service.write_file_binary.return_value = "Successfully wrote to /test/uploaded.txt"
        mock_fs_service.invalidate_cache.return_value = None

        # Create test file
        test_file = io.BytesIO(b"Test file content")

        # Send request
        response = client.post(
            "/fs/upload",
            files={"file": ("uploaded.txt", test_file)},
            data={"path": "/test", "storage": "local"},
        )

        # Verify response
        assert response.status_code == 200
        assert "Successfully wrote to" in response.text
        mock_fs_service.write_file_binary.assert_called_once()

    @patch("app.api.filesystem.filesystem_service")
    def test_read_binary_file(self, mock_fs_service):
        # Mock the service
        mock_fs_service.read_file_binary.return_value = b"Binary content"

        # Send request
        response = client.post(
            "/fs/read-binary", json={"path": "/test/binary.bin", "storage": "local"}
        )

        # Verify response
        assert response.status_code == 200
        assert response.content == b"Binary content"
        mock_fs_service.read_file_binary.assert_called_once()

    @patch("app.api.filesystem.filesystem_service")
    def test_file_exists(self, mock_fs_service):
        # Mock the service
        mock_fs_service.file_exists.return_value = True

        # Send request
        response = client.post("/fs/exists", json={"path": "/test/file.txt", "storage": "local"})

        # Verify response
        assert response.status_code == 200
        assert response.json() is True
        mock_fs_service.file_exists.assert_called_once()


================================================================================
FILE: tests/api/test_git_api.py
LANGUAGE: python
SIZE: 6638 bytes
================================================================================

import pytest
from unittest.mock import patch, MagicMock
from fastapi.testclient import TestClient
from fastapi import FastAPI

from app.api.git import router

# Create test app
app = FastAPI()
app.include_router(router, prefix="/git")
client = TestClient(app)


class TestGitAPI:
    @patch("app.api.git.git_service")
    def test_get_status(self, mock_git_service):
        # Mock the service
        mock_git_service.get_status.return_value = {
            "branch": "main",
            "clean": True,
            "untracked": [],
            "modified": [],
            "staged": [],
        }

        # Send request
        response = client.post("/git/status", json={"repo_path": "/test/repo"})

        # Verify response
        assert response.status_code == 200
        assert response.json()["branch"] == "main"
        assert response.json()["clean"] is True
        mock_git_service.get_status.assert_called_once()

    @patch("app.api.git.git_service")
    def test_get_diff(self, mock_git_service):
        # Mock the service
        mock_git_service.get_diff.return_value = "diff --git a/file.txt b/file.txt\n+New content"

        # Send request
        response = client.post(
            "/git/diff", json={"repo_path": "/test/repo", "file_path": "file.txt"}
        )

        # Verify response
        assert response.status_code == 200
        assert "diff --git" in response.text
        mock_git_service.get_diff.assert_called_once()

    @patch("app.api.git.git_service")
    def test_add_files(self, mock_git_service):
        # Mock the service
        mock_git_service.add_files.return_value = "Files staged successfully"

        # Send request
        response = client.post(
            "/git/add", json={"repo_path": "/test/repo", "files": ["file1.txt", "file2.txt"]}
        )

        # Verify response
        assert response.status_code == 200
        assert response.text == "Files staged successfully"
        mock_git_service.add_files.assert_called_once()

    @patch("app.api.git.git_service")
    def test_commit_changes(self, mock_git_service):
        # Mock the service
        mock_git_service.commit_changes.return_value = "Committed changes with hash abc123"

        # Send request
        response = client.post(
            "/git/commit",
            json={
                "repo_path": "/test/repo",
                "message": "Test commit",
                "author_name": "Test User",
                "author_email": "test@example.com",
            },
        )

        # Verify response
        assert response.status_code == 200
        assert "Committed changes with hash" in response.text
        mock_git_service.commit_changes.assert_called_once()

    @patch("app.api.git.git_service")
    def test_reset_changes(self, mock_git_service):
        # Mock the service
        mock_git_service.reset_changes.return_value = "All staged changes reset"

        # Send request
        response = client.post("/git/reset", json={"repo_path": "/test/repo"})

        # Verify response
        assert response.status_code == 200
        assert response.text == "All staged changes reset"
        mock_git_service.reset_changes.assert_called_once()

    @patch("app.api.git.git_service")
    def test_get_log(self, mock_git_service):
        # Mock the service
        mock_git_service.get_log.return_value = {
            "commits": [
                {
                    "hash": "abc123",
                    "message": "Test commit",
                    "author": "Test User",
                    "date": "2023-01-01 10:00:00",
                }
            ]
        }

        # Send request
        response = client.post("/git/log", json={"repo_path": "/test/repo", "max_count": 10})

        # Verify response
        assert response.status_code == 200
        assert len(response.json()["commits"]) == 1
        assert response.json()["commits"][0]["hash"] == "abc123"
        mock_git_service.get_log.assert_called_once()

    @patch("app.api.git.git_service")
    def test_create_branch(self, mock_git_service):
        # Mock the service
        mock_git_service.create_branch.return_value = "Created branch 'feature'"

        # Send request
        response = client.post(
            "/git/branch",
            json={"repo_path": "/test/repo", "branch_name": "feature", "base_branch": "main"},
        )

        # Verify response
        assert response.status_code == 200
        assert response.text == "Created branch 'feature'"
        mock_git_service.create_branch.assert_called_once()

    @patch("app.api.git.git_service")
    def test_checkout_branch(self, mock_git_service):
        # Mock the service
        mock_git_service.checkout_branch.return_value = "Switched to branch 'feature'"

        # Send request
        response = client.post(
            "/git/checkout",
            json={"repo_path": "/test/repo", "branch_name": "feature", "create": False},
        )

        # Verify response
        assert response.status_code == 200
        assert response.text == "Switched to branch 'feature'"
        mock_git_service.checkout_branch.assert_called_once()

    @patch("app.api.git.git_service")
    def test_clone_repo(self, mock_git_service):
        # Mock the service
        mock_git_service.clone_repo.return_value = "Cloned repository to '/test/cloned'"

        # Send request
        response = client.post(
            "/git/clone",
            json={"repo_url": "https://github.com/example/repo.git", "local_path": "/test/cloned"},
        )

        # Verify response
        assert response.status_code == 200
        assert response.text == "Cloned repository to '/test/cloned'"
        mock_git_service.clone_repo.assert_called_once()

    @patch("app.api.git.git_service")
    def test_batch_commit(self, mock_git_service):
        # Mock the service
        mock_git_service.batch_commit.return_value = ["abc123", "def456"]

        # Send request
        response = client.post(
            "/git/batch-commit",
            json={
                "repo_path": "/test/repo",
                "file_groups": [["file1.txt", "file2.txt"], ["file3.txt"]],
                "message_template": "Batch commit",
            },
        )

        # Verify response
        assert response.status_code == 200
        assert len(response.json()) == 2
        assert response.json()[0] == "abc123"
        mock_git_service.batch_commit.assert_called_once()


================================================================================
FILE: tests/core/__init__.py
LANGUAGE: python
SIZE: 80 bytes
================================================================================

# This file is intentionally left empty to make the directory a Python package


================================================================================
FILE: tests/core/test_documents_service.py
LANGUAGE: python
SIZE: 8610 bytes
================================================================================

import shutil
import tempfile
from pathlib import Path
from unittest.mock import patch, MagicMock

import numpy as np
import pytest

from app.core.documents_service import DocumentsService
from app.models.documents import DocumentType


@pytest.fixture
def docs_test_dir():
    # Create a temporary directory for testing
    temp_dir_path = tempfile.mkdtemp()
    yield temp_dir_path
    # Cleanup
    shutil.rmtree(temp_dir_path)


@pytest.fixture
def docs_service_fixture(docs_test_dir):
    # Mock dependencies
    with (
        patch("app.core.documents_service.get_config"),
        patch("app.core.documents_service.GitService") as mock_git_service,
        patch("app.core.documents_service.SentenceTransformer") as mock_transformer,
    ):
        # Set up GitService mock
        git_service_instance = MagicMock()
        mock_git_service.return_value = git_service_instance
        
        # Set up SentenceTransformer mock to avoid Hugging Face API calls
        transformer_instance = MagicMock()
        # Mock the encode method which will be used to generate embeddings
        transformer_instance.encode.return_value = np.zeros((1, 384))  # Use a standard embedding size
        mock_transformer.return_value = transformer_instance

        # Configure service
        service = DocumentsService(base_path=docs_test_dir)
        yield service


class TestDocumentsService:
    def test_create_document(self, docs_service_fixture, docs_test_dir):
        # Test creating a document
        doc = docs_service_fixture.create_document(
            title="Test Document",
            content="This is test content.",
            document_type=DocumentType.GENERIC,
            metadata={"source": "test"},
            tags=["test", "document"],
        )        

        # Verify document was created
        assert doc["title"] == "Test Document"
        assert doc["document_type"] == DocumentType.GENERIC.value
        assert "source" in doc["metadata"]
        assert "test" in doc["tags"]
        
        # Check that document ID was generated
        assert doc["id"] is not None

        # Verify file exists
        doc_path = Path(docs_test_dir) / DocumentType.GENERIC.value / f"{doc['id']}.md"
        assert doc_path.exists()

        # Verify content
        content = doc_path.read_text(encoding="utf-8")
        assert "Test Document" in content
        assert "This is test content." in content

    def test_get_document(self, docs_service_fixture):
        # Create a document
        doc = docs_service_fixture.create_document(
            title="Get Test",
            content="Content for retrieval test",
            document_type=DocumentType.GENERIC,
        )

        # Get the document
        retrieved = docs_service_fixture.get_document(doc["id"])

        # Verify retrieval
        assert retrieved["id"] == doc["id"]
        assert retrieved["title"] == "Get Test"
        assert "Content for retrieval" in retrieved["content_preview"]

    def test_update_document(self, docs_service_fixture):
        # Create a document
        doc = docs_service_fixture.create_document(
            title="Original Title",
            content="Original content",
            document_type=DocumentType.GENERIC,
            tags=["original"],
        )

        # Update the document
        updated = docs_service_fixture.update_document(
            doc_id=doc["id"],
            title="Updated Title",
            content="Updated content",
            tags=["updated", "document"],
        )
        
        # Verify update
        assert updated["title"] == "Updated Title"
        assert "updated" in updated["tags"]
        
        # Check the document content
        content = docs_service_fixture.get_document_content(doc["id"])
        assert content["content"] == "Updated content"

    def test_delete_document(self, docs_service_fixture):
        # Create a document
        doc = docs_service_fixture.create_document(
            title="To Delete",
            content="This document will be deleted",
            document_type=DocumentType.GENERIC,
        )
        
        # Delete the document
        result = docs_service_fixture.delete_document(doc["id"])
        assert result is True

        # Verify document is gone
        assert docs_service_fixture.get_document(doc["id"]) is None

        # Verify file is removed
        doc_path = (
            Path(docs_service_fixture.base_path) / DocumentType.GENERIC.value / f"{doc['id']}.md"
        )
        assert not doc_path.exists()

    def test_search_documents(self, docs_service_fixture):
        # Create test documents
        docs = []
        
        # Document 1
        docs.append(
            docs_service_fixture.create_document(
                title="Search Test One",
                content="This document has specific content to find.",
                document_type=DocumentType.GENERIC,
                tags=["search", "test"],
            )
        )
        
        # Document 2
        docs.append(
            docs_service_fixture.create_document(
                title="Search Test Two",
                content="Another document with different content.",
                document_type=DocumentType.GENERIC,
                tags=["search", "different"],
            )
        )
        
        # Document 3 (with different type)
        docs.append(
            docs_service_fixture.create_document(
                title="Different Type",
                content="This has a different document type.",
                document_type=DocumentType.WEBPAGE,
                tags=["webpage"],
            )
        )

        # Search by content
        results = docs_service_fixture.search_documents("specific content")
        assert len(results) == 1
        assert results[0]["id"] == docs[0]["id"]
        
        # Search by tag
        results = docs_service_fixture.search_documents("", tags=["search"])
        assert len(results) == 2
        
        # Search by document type
        results = docs_service_fixture.search_documents("", doc_type=DocumentType.WEBPAGE.value)
        assert len(results) == 1
        assert results[0]["id"] == docs[2]["id"]

    def test_get_document_versions(self, docs_service_fixture):
        # Create a document
        doc = docs_service_fixture.create_document(
            title="Version Test", 
            content="Initial version", 
            document_type=DocumentType.GENERIC
        )

        # Mock git log response
        docs_service_fixture.git_service.get_log.return_value = {
            "commits": [
                {
                    "hash": "abc123",
                    "message": "Updated document",
                    "author": "Test User",
                    "date": "2023-01-02 10:00:00",
                },
                {
                    "hash": "def456",
                    "message": "Created document",
                    "author": "Test User",
                    "date": "2023-01-01 10:00:00",
                },
            ]
        }

        # Get document versions
        versions = docs_service_fixture.get_document_versions(doc["id"])

        # Verify versions
        assert len(versions) == 2
        assert versions[0]["hash"] == "abc123"
        assert versions[1]["hash"] == "def456"
        assert versions[1]["message"] == "Created document"

    @patch("app.core.documents_service.markdown")
    @patch("weasyprint.HTML")
    def test_convert_document_format(self, mock_weasyprint, mock_markdown, docs_service_fixture):
        # Create a document
        doc = docs_service_fixture.create_document(
            title="Convert Format Test",
            content="# Heading\nContent to convert",
            document_type=DocumentType.GENERIC,
        )

        # Mock markdown conversion
        mock_markdown.markdown.return_value = "<h1>Heading</h1><p>Content to convert</p>"

        # Mock PDF generation
        mock_pdf = MagicMock()
        mock_pdf.write_pdf.return_value = b"PDF content"
        mock_weasyprint.return_value = mock_pdf

        # Convert document to PDF
        result = docs_service_fixture.convert_document_format(doc["id"], "pdf")

        # Verify conversion
        assert mock_markdown.markdown.called
        assert mock_weasyprint.called
        assert result == b"PDF content"


================================================================================
FILE: tests/core/test_filesystem_service.py
LANGUAGE: python
SIZE: 5833 bytes
================================================================================

import os
import shutil
import tempfile
from pathlib import Path
from unittest.mock import patch, MagicMock

import pytest

from app.core.filesystem_service import FilesystemService


@pytest.fixture
def fs_test_dir():
    # Create a temporary directory for testing
    temp_dir_path = tempfile.mkdtemp()
    yield temp_dir_path
    # Cleanup
    shutil.rmtree(temp_dir_path)


@pytest.fixture
def fs_service_fixture(fs_test_dir):
    # Configure service with test paths
    with patch("app.core.filesystem_service.get_config") as mock_config:
        mock_config.return_value.allowed_directories = [fs_test_dir]
        mock_config.return_value.file_cache_enabled = False
        mock_config.return_value.s3_access_key = None
        mock_config.return_value.s3_secret_key = None
        service = FilesystemService()
        yield service


class TestFilesystemService:
    def test_normalize_path(self, fs_service_fixture, fs_test_dir):
        # Test path normalization
        test_path = os.path.join(fs_test_dir, "test_file.txt")
        normalized = fs_service_fixture.normalize_path(test_path)
        assert str(normalized) == str(Path(test_path).resolve())

    def test_read_write_file(self, fs_service_fixture, fs_test_dir):
        # Test writing and reading a file
        test_path = os.path.join(fs_test_dir, "test_file.txt")
        content = "Test content"

        # Write file
        result = fs_service_fixture.write_file(test_path, content)
        assert "Successfully wrote to" in result

        # Read file
        read_content = fs_service_fixture.read_file(test_path)
        assert read_content == content

    def test_list_directory(self, fs_service_fixture, fs_test_dir):
        # Test directory listing
        # First create some test files
        os.makedirs(os.path.join(fs_test_dir, "subdir"), exist_ok=True)
        
        test_file1 = os.path.join(fs_test_dir, "file1.txt")
        test_file2 = os.path.join(fs_test_dir, "file2.txt")
        
        with open(test_file1, "w", encoding="utf-8") as f:
            f.write("File 1 content")
        
        with open(test_file2, "w", encoding="utf-8") as f:
            f.write("File 2 content")

        # List directory
        result = fs_service_fixture.list_directory(fs_test_dir)
        
        # Verify result structure
        assert "items" in result
        assert "directory" in result
        assert len(result["items"]) >= 3  # subdir, file1.txt, file2.txt
        
        # Verify files are included
        file_names = [item["name"] for item in result["items"]]
        assert "file1.txt" in file_names
        assert "file2.txt" in file_names
        assert "subdir" in file_names

    def test_create_delete_directory(self, fs_service_fixture, fs_test_dir):
        # Test directory creation and deletion
        new_dir = os.path.join(fs_test_dir, "new_test_dir", "nested_dir")
        
        # Create directory
        result = fs_service_fixture.create_directory(new_dir)
        assert "Successfully created" in result
        assert os.path.exists(new_dir)
        
        # Delete directory
        delete_result = fs_service_fixture.delete_file(new_dir)
        assert "Successfully deleted" in delete_result
        assert not os.path.exists(new_dir)

    def test_search_files(self, fs_service_fixture, fs_test_dir):
        # Test searching for files
        # Create test files with specific patterns
        test_dir1 = os.path.join(fs_test_dir, "search_test")
        os.makedirs(test_dir1, exist_ok=True)
        
        test_file1 = os.path.join(test_dir1, "search_file1.txt")
        test_file2 = os.path.join(test_dir1, "search_file2.txt")
        test_file3 = os.path.join(test_dir1, "other_file.txt")
        
        with open(test_file1, "w", encoding="utf-8") as f:
            f.write("Search file 1")
        
        with open(test_file2, "w", encoding="utf-8") as f:
            f.write("Search file 2")
        
        with open(test_file3, "w", encoding="utf-8") as f:
            f.write("Other file")
        
        # Search for files
        result = fs_service_fixture.search_files(fs_test_dir, "search_file*.txt")
        
        # Verify search results
        assert len(result) == 2
        file_paths = [item["path"] for item in result]
        assert os.path.basename(test_file1) in str(file_paths)
        assert os.path.basename(test_file2) in str(file_paths)
        assert os.path.basename(test_file3) not in str(file_paths)

    @patch("app.core.filesystem_service.boto3")
    def test_s3_operations(self, mock_boto3):
        # Test S3 operations
        # This test mocks S3 interactions
        
        # Mock S3 client and resource
        mock_s3_client = MagicMock()
        mock_s3_resource = MagicMock()
        mock_boto3.client.return_value = mock_s3_client
        mock_boto3.resource.return_value = mock_s3_resource
        
        # Create a service with S3 enabled
        with patch("app.core.filesystem_service.get_config") as mock_config:
            mock_config.return_value.allowed_directories = ["/test"]
            mock_config.return_value.s3_access_key = "test_key"
            mock_config.return_value.s3_secret_key = "test_secret"
            
            s3_service = FilesystemService()
            
            # Test S3 write operation
            s3_service.write_file("s3_test_file.txt", "S3 test content", "s3", "test_bucket")
            
            # Verify S3 client was called correctly
            mock_s3_client.put_object.assert_called_with(
                Bucket="test_bucket",
                Key="s3_test_file.txt",
                Body="S3 test content"
            )


================================================================================
FILE: tests/core/test_git_service.py
LANGUAGE: python
SIZE: 6056 bytes
================================================================================

import os
import shutil
import tempfile
from unittest.mock import patch

import pytest
from git import Repo

from app.core.git_service import GitService


@pytest.fixture
def git_temp_directory():
    # Create a temporary directory for testing
    temp_dir_path = tempfile.mkdtemp()
    yield temp_dir_path
    # Cleanup
    shutil.rmtree(temp_dir_path)


@pytest.fixture
def git_service_fixture():
    service = GitService()
    yield service


@pytest.fixture
def git_repo_path(git_temp_directory):
    # Initialize a Git repository for testing
    repo_path = os.path.join(git_temp_directory, "test_repo")
    os.makedirs(repo_path)

    # Initialize git repo
    Repo.init(repo_path)

    # Create a test file and commit it
    test_file = os.path.join(repo_path, "test.txt")
    with open(test_file, "w", encoding="utf-8") as f:
        f.write("Initial content")

    repo = Repo(repo_path)
    repo.git.add("test.txt")
    repo.git.config("user.email", "test@example.com")
    repo.git.config("user.name", "Test User")
    repo.git.commit("-m", "Initial commit")

    yield repo_path


class TestGitService:
    def test_get_status(self, git_service_fixture, git_repo_path):
        # Test getting repository status
        result = git_service_fixture.get_status(git_repo_path)
        
        # Verify result
        assert "current_branch" in result
        assert "is_clean" in result
        assert result["is_clean"] is True  # Repository should be clean after initialization

    def test_add_and_commit(self, git_service_fixture, git_repo_path):
        # Test adding and committing changes
        # Create a new file
        new_file = os.path.join(git_repo_path, "new_file.txt")
        with open(new_file, "w", encoding="utf-8") as f:
            f.write("New file content")
        
        # Add the file
        add_result = git_service_fixture.add_files(git_repo_path, ["new_file.txt"])
        assert "Staged files" in add_result
        
        # Commit the changes
        commit_result = git_service_fixture.commit_changes(git_repo_path, "Added new file")
        assert "Committed" in commit_result
        
        # Verify repository status
        status = git_service_fixture.get_status(git_repo_path)
        assert status["is_clean"] is True  # Repository should be clean after commit

    def test_get_log(self, git_service_fixture, git_repo_path):
        # Test getting commit log
        log_result = git_service_fixture.get_log(git_repo_path)
        
        # Verify log structure
        assert "commits" in log_result
        assert len(log_result["commits"]) >= 1
        
        # Verify commit information
        latest_commit = log_result["commits"][0]
        assert "hash" in latest_commit
        assert "message" in latest_commit
        assert "author" in latest_commit
        assert "date" in latest_commit
        assert latest_commit["message"] == "Initial commit"

    def test_create_checkout_branch(self, git_service_fixture, git_repo_path):
        # Test creating and checking out a branch
        # Create a new file and commit it to master
        new_file = os.path.join(git_repo_path, "master_file.txt")
        with open(new_file, "w", encoding="utf-8") as f:
            f.write("Master branch file")
        
        git_service_fixture.add_files(git_repo_path, ["master_file.txt"])
        git_service_fixture.commit_changes(git_repo_path, "Added file on master")
        
        # Create a new branch
        branch_result = git_service_fixture.create_branch(git_repo_path, "test-branch")
        assert "Created" in branch_result
        
        # Checkout the new branch
        checkout_result = git_service_fixture.checkout_branch(git_repo_path, "test-branch")
        assert "Switched to branch" in checkout_result
        
        # Create a branch-specific file
        branch_file = os.path.join(git_repo_path, "branch_file.txt")
        with open(branch_file, "w", encoding="utf-8") as f:
            f.write("Branch specific file")
        
        git_service_fixture.add_files(git_repo_path, ["branch_file.txt"])
        git_service_fixture.commit_changes(git_repo_path, "Added file on branch")
        
        # Verify status shows we're on the test branch
        status = git_service_fixture.get_status(git_repo_path)
        assert status["current_branch"] == "test-branch"

    def test_create_tag(self, git_service_fixture, git_repo_path):
        # Test creating a tag
        tag_result = git_service_fixture.create_tag(git_repo_path, "v1.0", "Version 1.0")
        assert "Created tag" in tag_result
        
        # Get tags
        repo = Repo(git_repo_path)
        tags = list(repo.tags)
        assert len(tags) == 1
        assert str(tags[0]) == "v1.0"

    @patch("app.core.git_service.requests")
    def test_webhook(self, mock_requests, git_service_fixture, git_temp_directory):
        # Test webhook functionality
        # Set up a repo
        repo_path = os.path.join(git_temp_directory, "webhook_test_repo")
        os.makedirs(repo_path)
        
        Repo.init(repo_path)
        
        # Mock webhook response
        mock_response = mock_requests.post.return_value
        mock_response.status_code = 200
        mock_response.json.return_value = {"status": "success"}
        
        # Configure and trigger webhook
        webhook_url = "https://example.com/webhook"
        
        # Add a webhook
        result = git_service_fixture.add_webhook(repo_path, webhook_url, ["push"])
        assert "Added webhook" in result
        
        # Trigger webhook (we're just testing the mechanics here, not the actual HTTP request)
        trigger_result = git_service_fixture.trigger_webhook(repo_path, "push", {"data": "test"})
        assert "Triggered webhook" in trigger_result

        # Verify that the request was made
        mock_requests.post.assert_called()


================================================================================
FILE: tests/core/test_memory_service.py
LANGUAGE: python
SIZE: 9364 bytes
================================================================================

import shutil
from pathlib import Path
from unittest.mock import patch

import pytest
# Only import networkx when used in the test_graph_db_mode function
# import networkx as nx

from app.core.memory_service import MemoryService


@pytest.fixture
def memory_test_dir():
    # Create a temporary directory for testing
    temp_dir_path = Path("./test_data")
    temp_dir_path.mkdir(exist_ok=True)
    yield temp_dir_path
    # Cleanup
    shutil.rmtree(temp_dir_path)


@pytest.fixture
def memory_service_fixture(memory_test_dir):
    # Create a memory service with test file
    memory_file = memory_test_dir / "test_memory.json"

    with patch("app.core.memory_service.get_config") as mock_config:
        mock_config.return_value.use_graph_db = False
        service = MemoryService(str(memory_file))
        yield service


class TestMemoryService:
    def test_create_entities(self, memory_service_fixture):
        # Test creating entities
        entities = [
            {"name": "Test Entity 1", "entity_type": "person", "properties": {"age": 30}},
            {"name": "Test Entity 2", "entity_type": "organization", "properties": {"size": "large"}},
        ]
        
        result = memory_service_fixture.create_entities(entities)
        
        # Verify entities were created
        assert len(result) == 2
        assert "Test Entity 1" in [entity["name"] for entity in result]
        assert "Test Entity 2" in [entity["name"] for entity in result]
        
        # Verify entity properties
        entity1 = next(e for e in result if e["name"] == "Test Entity 1")
        entity2 = next(e for e in result if e["name"] == "Test Entity 2")
        
        assert entity1["entity_type"] == "person"
        assert entity1["properties"]["age"] == 30
        assert entity2["entity_type"] == "organization"
        assert entity2["properties"]["size"] == "large"

    def test_create_relations(self, memory_service_fixture):
        # First create entities
        entities = [
            {"name": "Person A", "entity_type": "person"},
            {"name": "Company B", "entity_type": "organization"},
        ]
        memory_service_fixture.create_entities(entities)
        
        # Now create a relation
        relations = [
            {
                "source": "Person A",
                "target": "Company B",
                "relation_type": "works_at",
                "properties": {"role": "manager"}
            }
        ]
        
        result = memory_service_fixture.create_relations(relations)
        
        # Verify relation was created
        assert len(result) == 1
        relation = result[0]
        assert relation["source"] == "Person A"
        assert relation["target"] == "Company B"
        assert relation["relation_type"] == "works_at"
        assert relation["properties"]["role"] == "manager"

    def test_query_graph(self, memory_service_fixture):
        # Set up a test graph
        entities = [
            {"name": "Alice", "entity_type": "person", "properties": {"age": 25}},
            {"name": "Bob", "entity_type": "person", "properties": {"age": 30}},
            {"name": "TechCorp", "entity_type": "company", "properties": {"industry": "tech"}},
        ]
        memory_service_fixture.create_entities(entities)
        
        relations = [
            {"source": "Alice", "target": "TechCorp", "relation_type": "works_at"},
            {"source": "Bob", "target": "TechCorp", "relation_type": "works_at"},
            {"source": "Alice", "target": "Bob", "relation_type": "knows"},
        ]
        memory_service_fixture.create_relations(relations)
        
        # Query entities
        people = memory_service_fixture.query_entities("person")
        assert len(people) == 2
        
        # Query with property filter
        young_people = memory_service_fixture.query_entities("person", {"age": 25})
        assert len(young_people) == 1
        assert young_people[0]["name"] == "Alice"
        
        # Query relations
        work_relations = memory_service_fixture.query_relations("works_at")
        assert len(work_relations) == 2
        
        # Query outgoing relations
        alice_relations = memory_service_fixture.query_relations(source="Alice")
        assert len(alice_relations) == 2
        
        # Query specific relation
        specific_relation = memory_service_fixture.query_relations("knows", "Alice", "Bob")
        assert len(specific_relation) == 1

    def test_user_preferences(self, memory_service_fixture):
        # Test setting preferences
        user_id = "test_user"
        prefs = {"theme": "dark", "language": "en"}

        # Set preferences
        result = memory_service_fixture.set_user_preference(user_id, prefs)
        assert result == prefs

        # Get preferences
        retrieved = memory_service_fixture.get_user_preference(user_id)
        assert retrieved == prefs

        # Update preferences
        updated_prefs = {"theme": "light", "font_size": "large"}
        result = memory_service_fixture.set_user_preference(user_id, updated_prefs)
        assert result["theme"] == "light"
        assert result["language"] == "en"  # Should preserve existing values
        assert result["font_size"] == "large"  # Should add new values

    def test_delete_entities(self, memory_service_fixture):
        # Create entities
        entities = [
            {"name": "Entity1", "entity_type": "test"},
            {"name": "Entity2", "entity_type": "test"},
        ]
        memory_service_fixture.create_entities(entities)
        
        # Create a relation
        relations = [
            {"source": "Entity1", "target": "Entity2", "relation_type": "connects_to"}
        ]
        memory_service_fixture.create_relations(relations)
        
        # Delete one entity
        result = memory_service_fixture.delete_entities(["Entity1"])
        assert result == 1
        
        # Verify entity is gone and relation is gone
        remaining = memory_service_fixture.query_entities()
        assert len(remaining) == 1
        assert remaining[0]["name"] == "Entity2"
        
        remaining_relations = memory_service_fixture.query_relations()
        assert len(remaining_relations) == 0

    def test_delete_relations(self, memory_service_fixture):
        # Create entities and relations
        entities = [
            {"name": "NodeA", "entity_type": "test"},
            {"name": "NodeB", "entity_type": "test"},
            {"name": "NodeC", "entity_type": "test"},
        ]
        memory_service_fixture.create_entities(entities)
        
        relations = [
            {"source": "NodeA", "target": "NodeB", "relation_type": "linked"},
            {"source": "NodeA", "target": "NodeC", "relation_type": "linked"},
        ]
        memory_service_fixture.create_relations(relations)
        
        # Delete one relation
        to_delete = [{"source": "NodeA", "target": "NodeB", "relation_type": "linked"}]
        result = memory_service_fixture.delete_relations(to_delete)
        assert result == 1
        
        # Verify only one relation remains
        remaining = memory_service_fixture.query_relations()
        assert len(remaining) == 1
        assert remaining[0]["source"] == "NodeA"
        assert remaining[0]["target"] == "NodeC"

    def test_entity_connections(self, memory_service_fixture):
        # Create entities and relations for testing connections
        entities = [
            {"name": "Center", "entity_type": "test"},
            {"name": "Connected1", "entity_type": "test"},
            {"name": "Connected2", "entity_type": "test"},
            {"name": "Unconnected", "entity_type": "test"},
        ]
        memory_service_fixture.create_entities(entities)
        
        relations = [
            {"source": "Center", "target": "Connected1", "relation_type": "connects"},
            {"source": "Connected2", "target": "Center", "relation_type": "connects"},
        ]
        memory_service_fixture.create_relations(relations)
        
        # Get connections
        connections = memory_service_fixture.get_entity_connections("Center")
        
        # Verify connections
        assert len(connections) == 2
        connection_names = [c["name"] for c in connections]
        assert "Connected1" in connection_names
        assert "Connected2" in connection_names
        assert "Unconnected" not in connection_names

    @patch("networkx.DiGraph")
    def test_graph_db_mode(self, mock_digraph):
        # Test when using graph database mode
        test_dir = Path("./test_graph_db")
        test_dir.mkdir(exist_ok=True)
        try:
            memory_file = test_dir / "graph_db_test.json"

            with patch("app.core.memory_service.get_config") as mock_config:
                mock_config.return_value.use_graph_db = True

                # This should initialize the networkx graph
                MemoryService(str(memory_file))

                # Verify DiGraph was created
                assert mock_digraph.called
        finally:
            shutil.rmtree(test_dir)


================================================================================
FILE: tests/core/test_scraper_service.py
LANGUAGE: python
SIZE: 9136 bytes
================================================================================

import pytest
import os
import shutil
import json
import asyncio
from pathlib import Path
from unittest.mock import patch, MagicMock, AsyncMock

from app.core.scraper_service import ScraperService
from bs4 import BeautifulSoup


@pytest.fixture
def temp_dir():
    # Create a temporary directory for testing
    temp_dir = Path("./test_scraper_data")
    temp_dir.mkdir(exist_ok=True)
    yield temp_dir
    # Cleanup
    shutil.rmtree(temp_dir)


@pytest.fixture
def scraper_service(temp_dir):
    with patch("app.core.scraper_service.get_config") as mock_config:
        mock_config.return_value.scraper_min_delay = 0.1
        mock_config.return_value.scraper_max_delay = 0.2
        mock_config.return_value.scraper_data_path = str(temp_dir)

        service = ScraperService()
        yield service


class TestScraperService:
    @pytest.mark.asyncio
    @patch("app.core.scraper_service.aiohttp.ClientSession")
    async def test_scrape_url(self, mock_session, scraper_service):
        # Mock the aiohttp session
        mock_session_instance = AsyncMock()
        mock_session.return_value.__aenter__.return_value = mock_session_instance

        # Mock response
        mock_response = AsyncMock()
        mock_response.status = 200
        mock_response.text = AsyncMock(
            return_value="""
        <html>
            <head>
                <title>Test Page</title>
                <meta name="description" content="Test description">
            </head>
            <body>
                <h1>Test Heading</h1>
                <p>Test paragraph content.</p>
            </body>
        </html>
        """
        )
        mock_session_instance.get.return_value.__aenter__.return_value = mock_response

        # Test scraping a URL
        result = await scraper_service.scrape_url("https://example.com")

        # Verify successful scrape
        assert result["success"] is True
        assert result["url"] == "https://example.com"
        assert result["title"] == "Test Page"
        assert "Test Heading" in result["content"]
        assert "Test paragraph" in result["content"]
        assert result["metadata"]["description"] == "Test description"

    @pytest.mark.asyncio
    @patch("app.core.scraper_service.aiohttp.ClientSession")
    async def test_scrape_url_error(self, mock_session, scraper_service):
        # Mock a failed response
        mock_session_instance = AsyncMock()
        mock_session.return_value.__aenter__.return_value = mock_session_instance

        # Connection error
        mock_session_instance.get.side_effect = Exception("Connection error")

        # Test scraping with error
        result = await scraper_service.scrape_url("https://nonexistent.example.com")

        # Verify failed scrape
        assert result["success"] is False
        assert "error" in result
        assert "Connection error" in result["error"]

    @pytest.mark.asyncio
    async def test_extract_metadata(self, scraper_service):
        # Create sample HTML
        html = """
        <html>
            <head>
                <title>Metadata Test</title>
                <meta name="description" content="Meta description">
                <meta name="keywords" content="test, metadata, extraction">
                <meta property="og:title" content="OG Title">
                <meta property="og:description" content="OG Description">
                <script type="application/ld+json">
                    {"@context": "https://schema.org", "@type": "Article", "name": "Test Article"}
                </script>
            </head>
            <body>
                <h1>Test Content</h1>
            </body>
        </html>
        """

        # Extract metadata
        soup = BeautifulSoup(html, "html.parser")
        metadata = scraper_service._extract_metadata(html, "https://example.com/metadata")

        # Verify metadata extraction
        assert metadata["title"] == "Metadata Test"
        assert metadata["description"] == "Meta description"
        assert "og:title" in metadata
        assert metadata["og:title"] == "OG Title"
        assert len(metadata["structured_data"]) == 1
        assert metadata["structured_data"][0]["@type"] == "Article"

    @pytest.mark.asyncio
    @patch("app.core.scraper_service.aiohttp.ClientSession")
    async def test_scrape_with_pagination(self, mock_session, scraper_service):
        # Mock session
        mock_session_instance = AsyncMock()
        mock_session.return_value.__aenter__.return_value = mock_session_instance

        # First page with next link
        first_page = """
        <html>
            <head><title>Page 1</title></head>
            <body>
                <h1>First Page</h1>
                <p>Content on first page</p>
                <a href="https://example.com/page/2" rel="next">Next Page</a>
            </body>
        </html>
        """

        # Second page with no next link
        second_page = """
        <html>
            <head><title>Page 2</title></head>
            <body>
                <h1>Second Page</h1>
                <p>Content on second page</p>
            </body>
        </html>
        """

        # Set up mock responses for pagination
        mock_responses = [
            # First page response
            AsyncMock(status=200, text=AsyncMock(return_value=first_page)),
            # Second page response
            AsyncMock(status=200, text=AsyncMock(return_value=second_page)),
        ]

        mock_session_instance.get.return_value.__aenter__.side_effect = mock_responses

        # Test paginated scraping
        result = await scraper_service.scrape_with_pagination("https://example.com")

        # Verify results
        assert result["success"] is True
        assert result["pages_scraped"] == 2
        assert "First Page" in result["content"]
        assert "Second Page" in result["content"]

    @pytest.mark.asyncio
    @patch("playwright.async_api.async_playwright")
    async def test_capture_screenshot(self, mock_playwright, scraper_service, temp_dir):
        # Mock Playwright
        mock_playwright_instance = AsyncMock()
        mock_playwright.return_value.__aenter__.return_value = mock_playwright_instance

        # Mock browser, context, and page
        mock_browser = AsyncMock()
        mock_context = AsyncMock()
        mock_page = AsyncMock()

        mock_playwright_instance.chromium.launch = AsyncMock(return_value=mock_browser)
        mock_browser.new_context = AsyncMock(return_value=mock_context)
        mock_context.new_page = AsyncMock(return_value=mock_page)

        # Mock screenshot
        mock_page.screenshot = AsyncMock(return_value=b"fake screenshot data")

        # Test capturing screenshot
        result = await scraper_service.capture_screenshot("https://example.com")

        # Verify screenshot capture
        assert result["success"] is True
        assert "screenshot_path" in result
        assert mock_page.goto.called
        assert mock_page.screenshot.called

    @pytest.mark.asyncio
    @patch("app.core.scraper_service.aiohttp.ClientSession")
    async def test_scrape_sitemap(self, mock_session, scraper_service):
        # Mock session
        mock_session_instance = AsyncMock()
        mock_session.return_value.__aenter__.return_value = mock_session_instance

        # Mock sitemap response
        sitemap_xml = """
        <?xml version="1.0" encoding="UTF-8"?>
        <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
            <url>
                <loc>https://example.com/page1</loc>
                <lastmod>2023-01-01</lastmod>
            </url>
            <url>
                <loc>https://example.com/page2</loc>
                <lastmod>2023-01-02</lastmod>
            </url>
        </urlset>
        """

        # Mock page responses
        page_html = """
        <html>
            <head><title>Test Page</title></head>
            <body><p>Test content</p></body>
        </html>
        """

        # Set up mock responses
        sitemap_response = AsyncMock(status=200, text=AsyncMock(return_value=sitemap_xml))
        page_response = AsyncMock(status=200, text=AsyncMock(return_value=page_html))

        # Return different responses for different URLs
        def get_side_effect(url, **kwargs):
            if url.endswith(".xml"):
                return sitemap_response
            else:
                return page_response

        mock_session_instance.get.side_effect = get_side_effect

        # Test sitemap scraping
        result = await scraper_service.scrape_sitemap("https://example.com/sitemap.xml", max_urls=2)

        # Verify results
        assert result["success"] is True
        assert len(result["results"]) == 2
        assert result["results"][0]["url"] == "https://example.com/page1"
        assert result["results"][1]["url"] == "https://example.com/page2"


================================================================================
FILE: tests/test_memory_service.py
LANGUAGE: python
SIZE: 821 bytes
================================================================================

import pytest
import os
import shutil
from pathlib import Path
from app.core.memory_service import MemoryService


@pytest.fixture
def memory_service():
    # Set up a test memory file
    test_dir = Path("./test_data")
    test_dir.mkdir(exist_ok=True)
    memory_file = test_dir / "test_memory.json"

    service = MemoryService(str(memory_file))
    yield service

    # Clean up
    shutil.rmtree(test_dir)


def test_user_preferences(memory_service):
    # Test setting preferences
    user_id = "test_user"
    prefs = {"theme": "dark", "language": "en"}

    # Set preferences
    result = memory_service.set_user_preference(user_id, prefs)
    assert result == prefs

    # Get preferences
    retrieved = memory_service.get_user_preference(user_id)
    assert retrieved == prefs


