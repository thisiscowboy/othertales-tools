Codebase Export - 2025-04-07 00:57:21

================================================================================
FILE: .pytest_cache\README.md
LANGUAGE: markdown
SIZE: 310 bytes
================================================================================

# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.


================================================================================
FILE: .vscode\settings.json
LANGUAGE: json
SIZE: 153 bytes
================================================================================

{
    "python.testing.pytestArgs": [
        "tests"
    ],
    "python.testing.unittestEnabled": false,
    "python.testing.pytestEnabled": true
}

================================================================================
FILE: app\__init__.py
LANGUAGE: python
SIZE: 77 bytes
================================================================================

"""
Unified Tools Server Application Package
"""

__version__ = "1.0.0"


================================================================================
FILE: app\api\__init__.py
LANGUAGE: python
SIZE: 52 bytes
================================================================================

"""
API Routers for the Unified Tools Server
"""


================================================================================
FILE: app\api\documents.py
LANGUAGE: python
SIZE: 21224 bytes
================================================================================

import asyncio
import hashlib
import json
import os
import tempfile
import time
import random
import uuid
import logging
import threading
from typing import List, Optional, Dict, Any, Union, Set, Tuple
from pathlib import Path
from urllib.parse import urljoin, urlparse
import urllib.robotparser as robotparser
import requests
import git
from git import Repo
from fastapi import APIRouter, Body, Query, HTTPException, Path, UploadFile, File, Form, Response
from pydantic import BaseModel, Field
from app.models.documents import (
    DocumentType,
    CreateDocumentRequest,
    UpdateDocumentRequest,
    DocumentResponse,
    DocumentVersionResponse,
    DocumentContentResponse,
)
from app.core.documents_service import DocumentsService
from app.utils.config import get_config

# Set up logger
logger = logging.getLogger(__name__)
# Create router
router = APIRouter()


class GitService:
    def __init__(self):
        config = get_config()
        self.default_username = config.default_git_username
        self.default_email = config.default_git_email
        self.temp_auth_files = {}
        self.repo_locks = {}

    def _get_repo(self, repo_path: str) -> git.Repo:
        """Get git repository object"""
        try:
            repo = Repo(repo_path)
            return repo
        except git.exc.InvalidGitRepositoryError:
            raise ValueError(f"Invalid Git repository at '{repo_path}'")
        except Exception as e:
            raise ValueError(f"Failed to get repository: {str(e)}")

    def _get_repo_lock(self, repo_path: str) -> threading.Lock:
        """Get a lock for a specific repository to prevent concurrent modifications"""
        if repo_path not in self.repo_locks:
            self.repo_locks[repo_path] = threading.Lock()
        return self.repo_locks[repo_path]

    def get_status(self, repo_path: str) -> Dict[str, Any]:
        """Get the status of a Git repository"""
        repo = self._get_repo(repo_path)
        current_branch = repo.active_branch.name
        # Get staged files
        staged_files = [item.a_path for item in repo.index.diff("HEAD")]
        # Get modified but unstaged files
        unstaged_files = [item.a_path for item in repo.index.diff(None)]
        # Get untracked files
        untracked_files = repo.untracked_files
        return {
            "clean": not (staged_files or unstaged_files or untracked_files),
            "current_branch": current_branch,
            "staged_files": staged_files,
            "unstaged_files": unstaged_files,
            "untracked_files": untracked_files,
        }

    def get_diff(
        self, repo_path: str, file_path: Optional[str] = None, target: Optional[str] = None
    ) -> str:
        """Get diff of changes"""
        repo = self._get_repo(repo_path)
        if file_path and target:
            return repo.git.diff(target, file_path)
        elif file_path:
            return repo.git.diff("HEAD", file_path)
        elif target:
            return repo.git.diff(target)
        else:
            return repo.git.diff()

    def add_files(self, repo_path: str, files: List[str]) -> str:
        """Stage files for commit"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            repo.git.add(files)
            return "Files staged successfully"

    def commit_changes(
        self,
        repo_path: str,
        message: str,
        author_name: Optional[str] = None,
        author_email: Optional[str] = None,
    ) -> str:
        """Commit staged changes"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            author_name = author_name or self.default_username
            author_email = author_email or self.default_email
            # Set author for this commit
            with repo.config_writer() as config:
                config.set_value("user", "name", author_name)
                config.set_value("user", "email", author_email)
            # Commit changes
            commit = repo.index.commit(message)
            return f"Committed changes with hash {commit.hexsha}"

    def reset_changes(self, repo_path: str) -> str:
        """Reset staged changes"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            repo.git.reset()
            return "All staged changes reset"

    def get_log(
        self, repo_path: str, max_count: int = 10, file_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """Get log of commits"""
        repo = self._get_repo(repo_path)
        if file_path:
            commits = list(repo.iter_commits(paths=file_path, max_count=max_count))
        else:
            commits = list(repo.iter_commits(max_count=max_count))
        log_data = []
        for commit in commits:
            log_data.append(
                {
                    "hash": commit.hexsha,
                    "author": f"{commit.author.name} <{commit.author.email}>",
                    "date": commit.committed_datetime.strftime("%Y-%m-%d %H:%M:%S %z"),
                    "message": commit.message.strip(),
                }
            )
        return log_data

    def create_branch(
        self, repo_path: str, branch_name: str, base_branch: Optional[str] = None
    ) -> str:
        """Create a new branch"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            if base_branch:
                repo.git.checkout(base_branch)
            repo.git.checkout("-b", branch_name)
            return f"Created branch '{branch_name}'"

    def checkout_branch(self, repo_path: str, branch_name: str, create: bool = False) -> str:
        """Checkout an existing branch or create a new one"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            if create:
                if branch_name not in repo.refs:
                    repo.git.checkout("-b", branch_name)
                else:
                    raise ValueError(f"Branch '{branch_name}' already exists")
            else:
                repo.git.checkout(branch_name)
            return f"Checked out branch '{branch_name}'"

    def clone_repo(self, repo_url: str, local_path: str, auth_token: Optional[str] = None) -> str:
        """Clone a Git repository"""
        try:
            if auth_token:
                if repo_url.startswith("https://"):
                    repo_url = repo_url.replace("https://", f"https://x-access-token:{auth_token}@")
                Repo.clone_from(repo_url, local_path)
            else:
                Repo.clone_from(repo_url, local_path)
            return f"Cloned repository to '{local_path}'"
        except Exception as e:
            raise ValueError(f"Failed to clone repository: {str(e)}")

    def remove_file(self, repo_path: str, file_path: str) -> str:
        """Remove a file from the repository"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            try:
                repo.index.remove([file_path])
                repo.index.commit(f"Removed {file_path}")
                return f"Successfully removed {file_path} from Git"
            except Exception as e:
                raise ValueError(f"Failed to remove file: {str(e)}")

    def get_file_content(self, repo_path: str, file_path: str, version: str) -> str:
        """Get the content of a file at a specific Git version"""
        repo = self._get_repo(repo_path)
        try:
            blob = repo.git.show(f"{version}:{file_path}")
            return blob
        except Exception as e:
            raise ValueError(f"Failed to get file content at version {version}: {str(e)}")

    def configure_lfs(self, repo_path: str, file_patterns: List[str]) -> str:
        """Configure Git LFS for the repository"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            try:
                repo.git.execute(["git", "lfs", "install"])
                for pattern in file_patterns:
                    repo.git.execute(["git", "lfs", "track", pattern])
                repo.index.commit("Set up Git LFS tracking")
                return "Git LFS configured successfully"
            except Exception as e:
                raise ValueError(f"Failed to set up Git LFS: {str(e)}")

    def batch_commit(
        self, repo_path: str, file_groups: List[List[str]], message_template: str
    ) -> List[str]:
        """Commit files in batches for better performance"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            commit_hashes = []
            for i, file_group in enumerate(file_groups):
                repo.git.add(file_group)
                commit = repo.index.commit(f"{message_template} (batch {i+1}/{len(file_groups)})")
                commit_hashes.append(commit.hexsha)
            return commit_hashes

    def pull_changes(
        self, repo_path: str, remote: str = "origin", branch: str = None, all_remotes: bool = False
    ) -> str:
        """Pull changes from a remote repository"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            try:
                if all_remotes:
                    result = repo.git.fetch(all=True)
                else:
                    result = repo.git.pull(remote, branch)
                return result
            except Exception as e:
                raise ValueError(f"Failed to pull changes: {str(e)}")

    def create_tag(
        self, repo_path: str, tag_name: str, message: str = None, commit: str = "HEAD"
    ) -> str:
        """Create a new Git tag"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            try:
                if message:
                    repo.create_tag(tag_name, ref=commit, message=message)
                else:
                    repo.create_tag(tag_name, ref=commit)
                return f"Created tag '{tag_name}'"
            except Exception as e:
                raise ValueError(f"Failed to create tag: {str(e)}")

    def list_tags(self, repo_path: str) -> List[Dict[str, str]]:
        """List all tags in the repository"""
        repo = self._get_repo(repo_path)
        try:
            tags = []
            for tag in repo.tags:
                tags.append(
                    {
                        "name": tag.name,
                        "commit": tag.commit.hexsha,
                        "date": tag.commit.committed_datetime.strftime("%Y-%m-%d %H:%M:%S %z"),
                    }
                )
            return tags
        except Exception as e:
            raise ValueError(f"Failed to list tags: {str(e)}")

    def optimize_repo(self, repo_path: str) -> str:
        """Optimize the Git repository"""
        repo = self._get_repo(repo_path)
        try:
            repo.git.gc("--aggressive", "--prune=now")
            return "Repository optimized successfully"
        except Exception as e:
            raise ValueError(f"Failed to optimize repository: {str(e)}")

    def configure_auth(self, repo_path: str, username: str, password: str) -> str:
        """Configure authentication for repository operations"""
        if not username or not password:
            raise ValueError("Username and password required for HTTPS authentication")
        # Note: Storing passwords in git config is not secure
        # Consider using git credential store or credential manager
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            with repo.config_writer() as config:
                config.set_value("user", "name", username)
                config.set_value("user", "password", password)
            return "Authentication configured successfully"

    def register_webhook(self, repo_path: str, webhook: Dict[str, Any]) -> str:
        """Register a webhook for Git events"""
        # Note: This is a placeholder implementation. In a real-world scenario, you would need to handle webhooks
        # using Git hooks or a custom implementation.
        hook_path = os.path.join(repo_path, ".git", "hooks", "post-commit")
        with open(hook_path, "w") as hook_file:
            hook_file.write(
                f"#!/bin/sh\ncurl -X POST {webhook['url']} -d @- <<'EOF'\n$(git log -1 --pretty=format:'%H')\nEOF\n"
            )
        os.chmod(hook_path, 0o755)
        return "Webhook registered successfully"

    def restore_file_version(self, repo_path: str, file_path: str, version: str) -> bool:
        """Restore a file to a specific version"""
        try:
            # Get the file content at the specified version
            content = self.get_file_content(repo_path, file_path, version)
            # Write that content to the current file
            full_path = os.path.join(repo_path, file_path)
            os.makedirs(os.path.dirname(full_path), exist_ok=True)
            with open(full_path, "w", encoding="utf-8") as f:
                f.write(content)
            # Add and commit the change
            self.add_files(repo_path, [file_path])
            self.commit_changes(repo_path, f"Restored file to version {version}")
            return True
        except Exception as e:
            logger.error(f"Error restoring file version: {e}", exc_info=True)
            return False


class GitRepoPath(BaseModel):
    repo_path: str = Field(..., description="Path to the Git repository")


class GitCommitRequest(GitRepoPath):
    files: List[str] = Field(..., description="List of files to add")
    message: str = Field(..., description="Commit message")
    author_name: Optional[str] = Field(None, description="Author name")
    author_email: Optional[str] = Field(None, description="Author email")


class GitDiffRequest(GitRepoPath):
    file_path: Optional[str] = Field(None, description="Path to the file to diff")
    target: Optional[str] = Field(None, description="Target to diff against")


class GitLogRequest(GitRepoPath):
    max_count: int = Field(10, description="Maximum number of commits to return")
    file_path: Optional[str] = Field(None, description="Path to the file to get log for")


class GitBranchRequest(GitRepoPath):
    branch_name: str = Field(..., description="Name of the branch to create")
    base_branch: Optional[str] = Field(
        None, description="Base branch to create the new branch from"
    )


class GitCheckoutRequest(GitRepoPath):
    branch_name: str = Field(..., description="Name of the branch to checkout")
    create: bool = Field(False, description="Create the branch if it doesn't exist")


class GitCloneRequest(BaseModel):
    repo_url: str = Field(..., description="URL of the repository to clone")
    local_path: str = Field(..., description="Path to clone the repository to")
    auth_token: Optional[str] = Field(
        None, description="Authentication token for private repositories"
    )


class GitRemoveFileRequest(GitRepoPath):
    file_path: str = Field(..., description="Path to the file to remove")


class GitFileContentRequest(GitRepoPath):
    file_path: str = Field(..., description="Path to the file")
    version: str = Field(..., description="Git version to get the file content from")


class GitLFSRequest(GitRepoPath):
    file_patterns: List[str] = Field(..., description="List of file patterns to track with LFS")


class GitBatchCommitRequest(GitRepoPath):
    file_groups: List[List[str]] = Field(
        ..., description="List of file groups to commit in batches"
    )
    message_template: str = Field(..., description="Template for commit messages")


class GitPullRequest(GitRepoPath):
    remote: str = Field("origin", description="Remote to pull from")
    branch: Optional[str] = Field(None, description="Branch to pull")
    all_remotes: bool = Field(False, description="Fetch from all remotes")


class GitTagRequest(GitRepoPath):
    tag_name: str = Field(..., description="Tag name")
    message: Optional[str] = Field(None, description="Tag message")
    commit: str = Field("HEAD", description="Commit to tag")


class GitTagsResponse(BaseModel):
    tags: List[Dict[str, str]] = Field(..., description="List of tags")


class GitWebhook(BaseModel):
    url: str = Field(..., description="Webhook URL")
    events: List[str] = Field(..., description="List of events to trigger the webhook")
    secret: Optional[str] = Field(None, description="Webhook secret")


router = APIRouter()
git_service = GitService()


@router.post(
    "/status",
    response_model=Dict[str, Any],
    summary="Get repository status",
    description="Get the status of a Git repository, including the current branch, staged changes, and unstaged changes.",
)
async def get_status(request: GitRepoPath = Body(...)):
    """Get the status of a Git repository."""
    try:
        return git_service.get_status(request.repo_path)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get status: {str(e)}")


@router.post(
    "/diff",
    response_model=str,
    summary="Get diff of changes",
    description="Get the difference between working directory and HEAD or a specified target.",
)
async def get_diff(request: GitDiffRequest = Body(...)):
    """Get the difference between working directory and HEAD or a specified target."""
    try:
        return git_service.get_diff(request.repo_path, request.file_path, request.target)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get diff: {str(e)}")


@router.post(
    "/add",
    response_model=str,
    summary="Stage files for commit",
    description="Stage files for commit.",
)
async def add_files(request: GitCommitRequest = Body(...)):
    """Stage files for commit."""
    try:
        return git_service.add_files(request.repo_path, request.files)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to add files: {str(e)}")


@router.post(
    "/commit",
    response_model=str,
    summary="Commit changes",
    description="Commit staged changes with a commit message. Optionally, specify the author name and email.",
)
async def commit_changes(request: GitCommitRequest = Body(...)):
    """Commit staged changes with a commit message. Optionally, specify the author name and email."""
    try:
        return git_service.commit_changes(
            request.repo_path, request.message, request.author_name, request.author_email
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to commit changes: {str(e)}")


@router.post(
    "/reset",
    response_model=str,
    summary="Reset staged changes",
    description="Reset all staged changes.",
)
async def reset_changes(request: GitRepoPath = Body(...)):
    """Reset all staged changes."""
    try:
        return git_service.reset_changes(request.repo_path)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to reset changes: {str(e)}")


@router.post(
    "/log",
    response_model=List[Dict[str, Any]],
    summary="Get commit log",
    description="Get the commit log of the repository",
)
async def get_log(request: GitLogRequest = Body(...)):
    """Get the commit log of the repository."""
    try:
        return git_service.get_log(request.repo_path, request.max_count, request.file_path)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get log: {str(e)}")


@router.post(
    "/branch",
    response_model=str,
    summary="Create branch",
    description="Create a new branch from a base branch.",
)
async def create_branch(request: GitBranchRequest = Body(...)):
    """Create a new branch from a base branch."""
    try:
        return git_service.create_branch(
            request.repo_path, request.branch_name, request.base_branch
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create branch: {str(e)}")


================================================================================
FILE: app\api\filesystem.py
LANGUAGE: python
SIZE: 10515 bytes
================================================================================

from fastapi import APIRouter, Body, HTTPException, Query, Path, File, UploadFile, Form
from fastapi.responses import PlainTextResponse, FileResponse, Response
from typing import List, Dict, Any, Optional
import os
import logging
from app.models.filesystem import (
    ReadFileRequest,
    WriteFileRequest,
    ListDirectoryRequest,
    SearchFilesRequest,
    CreateDirectoryRequest,
    DeleteFileRequest,
    DirectoryListingResponse,
    InvalidateCacheRequest,
    FileExistsRequest,
)
from app.core.filesystem_service import FilesystemService

logger = logging.getLogger(__name__)
router = APIRouter(
    responses={
        400: {"description": "Bad request"},
        403: {"description": "Access denied"},
        404: {"description": "File not found"},
        500: {"description": "Server error"},
    }
)
filesystem_service = FilesystemService()


@router.post(
    "/read",
    response_class=PlainTextResponse,
    summary="Read a file",
    description="Read the entire contents of a file from local or S3 storage",
)
async def read_file(request: ReadFileRequest = Body(...)):
    """
    Read the entire contents of a file.
    Supports both local filesystem and S3 storage.
    """
    try:
        return filesystem_service.read_file(request.path, request.storage, request.bucket)
    except ValueError as e:
        logger.warning(f"Access denied: {e}")
        raise HTTPException(status_code=403, detail=str(e))
    except FileNotFoundError:
        logger.warning(f"File not found: {request.path}")
        raise HTTPException(status_code=404, detail=f"File not found: {request.path}")
    except Exception as e:
        logger.error(f"Error reading file: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/write",
    response_class=PlainTextResponse,
    summary="Write to a file",
    description="Write content to a file, overwriting if it exists",
)
async def write_file(request: WriteFileRequest = Body(...)):
    """
    Write content to a file, overwriting if it exists.
    Supports both local filesystem and S3 storage.
    """
    try:
        result = filesystem_service.write_file(
            request.path, request.content, request.storage, request.bucket
        )
        # Invalidate cache if caching is enabled
        try:
            filesystem_service.invalidate_cache(request.path, request.storage, request.bucket)
        except Exception as cache_error:
            logger.warning(f"Failed to invalidate cache: {cache_error}")
        return result
    except ValueError as e:
        logger.warning(f"Access denied: {e}")
        raise HTTPException(status_code=403, detail=str(e))
    except Exception as e:
        logger.error(f"Error writing file: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/list",
    response_model=DirectoryListingResponse,
    summary="List directory contents",
    description="List contents of a directory",
)
async def list_directory(request: ListDirectoryRequest = Body(...)):
    """
    List contents of a directory.
    Supports both local filesystem and S3 storage.
    """
    try:
        return filesystem_service.list_directory(
            request.path, request.storage, request.bucket, request.recursive
        )
    except ValueError as e:
        raise HTTPException(status_code=403, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.post(
    "/search", summary="Search for files", description="Search for files matching a pattern"
)
async def search_files(request: SearchFilesRequest = Body(...)):
    """
    Search for files matching a pattern.
    Supports both local filesystem and S3 storage.
    """
    try:
        results = filesystem_service.search_files(
            request.path, request.pattern, request.storage, request.bucket, request.exclude_patterns
        )
        return {"matches": results or ["No matches found"]}
    except ValueError as e:
        raise HTTPException(status_code=403, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.post(
    "/mkdir",
    response_class=PlainTextResponse,
    summary="Create a directory",
    description="Create a directory and parent directories if needed",
)
async def create_directory(request: CreateDirectoryRequest = Body(...)):
    """
    Create a new directory recursively.
    Supports both local filesystem and S3 storage.
    """
    try:
        return filesystem_service.create_directory(request.path, request.storage, request.bucket)
    except ValueError as e:
        raise HTTPException(status_code=403, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.post(
    "/delete",
    response_class=PlainTextResponse,
    summary="Delete a file",
    description="Delete a file from storage",
)
async def delete_file(request: DeleteFileRequest = Body(...)):
    """
    Delete a file.
    Supports both local filesystem and S3 storage.
    """
    try:
        return filesystem_service.delete_file(request.path, request.storage, request.bucket)
    except ValueError as e:
        raise HTTPException(status_code=403, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.post(
    "/read-binary",
    summary="Read binary file",
    description="Read a binary file and return its contents",
    responses={
        200: {"content": {"application/octet-stream": {}}, "description": "Binary file content"}
    },
)
async def read_binary_file(request: ReadFileRequest = Body(...)):
    """
    Read the contents of a binary file.
    Returns the file as a downloadable binary response.
    """
    try:
        content = filesystem_service.read_file_binary(request.path, request.storage, request.bucket)
        # Get filename from path
        filename = os.path.basename(request.path)
        return Response(
            content=content,
            media_type="application/octet-stream",
            headers={"Content-Disposition": f"attachment; filename={filename}"},
        )
    except ValueError as e:
        logger.warning(f"Access denied: {e}")
        raise HTTPException(status_code=403, detail=str(e))
    except FileNotFoundError:
        logger.warning(f"File not found: {request.path}")
        raise HTTPException(status_code=404, detail=f"File not found: {request.path}")
    except Exception as e:
        logger.error(f"Error reading binary file: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/upload",
    response_class=PlainTextResponse,
    summary="Upload file",
    description="Upload a file to storage",
)
async def upload_file(
    file: UploadFile = File(...),
    path: str = Form(...),
    storage: str = Form("local"),
    bucket: Optional[str] = Form(None),
):
    """
    Upload a file to storage.
    Supports both local filesystem and S3 storage.
    """
    try:
        content = await file.read()
        result = filesystem_service.write_file_binary(
            os.path.join(path, file.filename), content, storage, bucket
        )
        # Invalidate cache if caching is enabled
        try:
            filesystem_service.invalidate_cache(os.path.join(path, file.filename), storage, bucket)
        except Exception as cache_error:
            logger.warning(f"Failed to invalidate cache: {cache_error}")
        return result
    except ValueError as e:
        logger.warning(f"Access denied: {e}")
        raise HTTPException(status_code=403, detail=str(e))
    except Exception as e:
        logger.error(f"Error uploading file: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/exists",
    summary="Check if file exists",
    description="Check if a file or directory exists in storage",
)
async def file_exists(request: FileExistsRequest = Body(...)):
    """
    Check if a file or directory exists.
    Supports both local filesystem and S3 storage.
    """
    try:
        exists = False
        if request.storage == "local":
            try:
                path = filesystem_service.normalize_path(request.path)
                exists = path.exists()
            except ValueError:
                # If normalize_path fails, file doesn't exist or is inaccessible
                exists = False
        elif request.storage == "s3":
            if not request.bucket:
                raise ValueError("S3 bucket name is required for S3 storage")
            if not filesystem_service.s3_client:
                raise ValueError("S3 client not configured")
            try:
                filesystem_service.s3_client.head_object(Bucket=request.bucket, Key=request.path)
                exists = True
            except:
                exists = False
        else:
            raise ValueError(f"Unsupported storage type: {request.storage}")
        return {"exists": exists, "path": request.path}
    except ValueError as e:
        logger.warning(f"Invalid request: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Error checking file existence: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/invalidate-cache",
    response_class=PlainTextResponse,
    summary="Invalidate cache",
    description="Invalidate file cache for a path or all paths",
)
async def invalidate_cache(request: InvalidateCacheRequest = Body(...)):
    """
    Invalidate file cache.
    Can invalidate a specific path or all cached files.
    """
    try:
        if request.path:
            filesystem_service.invalidate_cache(request.path, request.storage, request.bucket)
            return f"Successfully invalidated cache for {request.path}"
        else:
            filesystem_service.invalidate_cache()
            return "Successfully invalidated all cache entries"
    except Exception as e:
        logger.error(f"Error invalidating cache: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


================================================================================
FILE: app\api\git.py
LANGUAGE: python
SIZE: 24204 bytes
================================================================================

import os
import tempfile
from typing import List, Optional, Dict, Any, Union
import git
from git import Repo
from fastapi import APIRouter, Body, HTTPException
from pydantic import BaseModel, Field
from app.utils.config import get_config

router = APIRouter()


class GitRepoPath(BaseModel):
    repo_path: str = Field(..., description="Path to the Git repository")


class GitAddRequest(GitRepoPath):
    files: List[str] = Field(..., description="List of files to add")


class GitCommitRequest(GitRepoPath):
    message: str = Field(..., description="Commit message")
    author_name: Optional[str] = Field(None, description="Author name")
    author_email: Optional[str] = Field(None, description="Author email")


class GitStatusRequest(GitRepoPath):
    pass


class GitDiffRequest(GitRepoPath):
    file_path: Optional[str] = Field(None, description="Path to the file to diff")
    target: Optional[str] = Field(None, description="Target to diff against")


class GitLogRequest(GitRepoPath):
    max_count: int = Field(10, description="Maximum number of commits to return")
    file_path: Optional[str] = Field(None, description="Path to the file to get log for")


class GitBranchRequest(GitRepoPath):
    branch_name: str = Field(..., description="Name of the branch")
    base_branch: Optional[str] = Field(
        None, description="Base branch to create the new branch from"
    )


class GitCheckoutRequest(GitRepoPath):
    branch_name: str = Field(..., description="Name of the branch to checkout")
    create: bool = Field(False, description="Create the branch if it doesn't exist")


class GitCloneRequest(BaseModel):
    repo_url: str = Field(..., description="URL of the repository to clone")
    local_path: str = Field(..., description="Local path to clone the repository to")
    auth_token: Optional[str] = Field(
        None, description="Authentication token for private repositories"
    )


class GitRemoveFileRequest(GitRepoPath):
    file_path: str = Field(..., description="Path to the file to remove")


class GitFileContentRequest(GitRepoPath):
    file_path: str = Field(..., description="Path to the file")
    version: str = Field(..., description="Git version to get the file content from")


class GitLFSRequest(GitRepoPath):
    file_patterns: List[str] = Field(..., description="List of file patterns to track with LFS")


class GitBatchCommitRequest(GitRepoPath):
    file_groups: List[List[str]] = Field(
        ..., description="List of file groups to commit in batches"
    )
    message_template: str = Field(..., description="Template for commit messages")


class GitPullRequest(GitRepoPath):
    remote: str = Field("origin", description="Name of the remote")
    branch: Optional[str] = Field(None, description="Branch to pull")


class GitFetchRequest(GitRepoPath):
    remote: str = Field("origin", description="Name of the remote")
    all_remotes: bool = Field(False, description="Fetch from all remotes")


class GitCreateTagRequest(GitRepoPath):
    tag_name: str = Field(..., description="Name of the tag to create")
    message: Optional[str] = Field(None, description="Tag message")
    commit: str = Field("HEAD", description="Commit to tag")


class TagInfo(BaseModel):
    name: str = Field(..., description="Tag name")
    commit: str = Field(..., description="Tagged commit hash")
    date: str = Field(..., description="Date of tagged commit")


class GitTagsResponse(BaseModel):
    tags: List[TagInfo] = Field(..., description="List of tags")


class GitService:
    def __init__(self):
        config = get_config()
        self.default_username = config.default_git_username
        self.default_email = config.default_git_email
        self.temp_auth_files = {}

    def _get_repo(self, repo_path: str) -> git.Repo:
        """Get git repository object"""
        try:
            repo = Repo(repo_path)
            return repo
        except git.exc.InvalidGitRepositoryError:
            raise ValueError(f"Invalid Git repository at '{repo_path}'")
        except Exception as e:
            raise ValueError(f"Failed to get repository: {str(e)}")

    def get_status(self, repo_path: str) -> Dict[str, Any]:
        """Get the status of a Git repository"""
        repo = self._get_repo(repo_path)
        current_branch = repo.active_branch.name
        # Get staged files
        staged_files = [item.a_path for item in repo.index.diff("HEAD")]
        # Get modified but unstaged files
        unstaged_files = [item.a_path for item in repo.index.diff(None)]
        # Get untracked files
        untracked_files = repo.untracked_files
        return {
            "clean": not (staged_files or unstaged_files or untracked_files),
            "current_branch": current_branch,
            "staged_files": staged_files,
            "unstaged_files": unstaged_files,
            "untracked_files": untracked_files,
        }

    def get_diff(
        self, repo_path: str, file_path: Optional[str] = None, target: Optional[str] = None
    ) -> str:
        """Get diff of changes"""
        repo = self._get_repo(repo_path)
        if file_path and target:
            return repo.git.diff(target, file_path)
        elif file_path:
            return repo.git.diff("HEAD", file_path)
        elif target:
            return repo.git.diff(target)
        else:
            return repo.git.diff()

    def add_files(self, repo_path: str, files: List[str]) -> str:
        """Stage files for commit"""
        repo = self._get_repo(repo_path)
        repo.git.add(files)
        return "Files staged successfully"

    def commit_changes(
        self,
        repo_path: str,
        message: str,
        author_name: Optional[str] = None,
        author_email: Optional[str] = None,
    ) -> str:
        """Commit staged changes"""
        repo = self._get_repo(repo_path)
        author_name = author_name or self.default_username
        author_email = author_email or self.default_email
        # Set author for this commit
        with repo.config_writer() as config:
            config.set_value("user", "name", author_name)
            config.set_value("user", "email", author_email)
        # Commit changes
        commit = repo.index.commit(message)
        return f"Committed changes with hash {commit.hexsha}"

    def reset_changes(self, repo_path: str) -> str:
        """Reset staged changes"""
        repo = self._get_repo(repo_path)
        repo.git.reset()
        return "All staged changes reset"

    def get_log(
        self, repo_path: str, max_count: int = 10, file_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """Get commit log"""
        repo = self._get_repo(repo_path)
        if file_path:
            # Get log for specific file
            commits = list(repo.iter_commits(paths=file_path, max_count=max_count))
        else:
            # Get log for entire repo
            commits = list(repo.iter_commits(max_count=max_count))
        commit_data = []
        for commit in commits:
            commit_data.append(
                {
                    "hash": commit.hexsha,
                    "author": f"{commit.author.name} <{commit.author.email}>",
                    "date": commit.committed_datetime.strftime("%Y-%m-%d %H:%M:%S %z"),
                    "message": commit.message.strip(),
                }
            )
        return {"commits": commit_data}

    def create_branch(
        self, repo_path: str, branch_name: str, base_branch: Optional[str] = None
    ) -> str:
        """Create a new branch"""
        repo = self._get_repo(repo_path)
        if base_branch:
            base = repo.refs[base_branch]
        else:
            base = repo.active_branch
        repo.create_head(branch_name, base)
        return f"Created branch '{branch_name}'"

    def checkout_branch(self, repo_path: str, branch_name: str, create: bool = False) -> str:
        """Checkout a branch"""
        repo = self._get_repo(repo_path)
        if create:
            # Create branch if it doesn't exist
            if branch_name not in repo.refs:
                repo.create_head(branch_name)
        # Checkout the branch
        repo.git.checkout(branch_name)
        return f"Switched to branch '{branch_name}'"

    def clone_repo(self, repo_url: str, local_path: str, auth_token: Optional[str] = None) -> str:
        """Clone a Git repository"""
        try:
            # If auth token is provided, modify the URL
            if auth_token:
                # Parse the URL to insert authentication
                if repo_url.startswith("https://"):
                    parsed_url = repo_url.replace(
                        "https://", f"https://x-access-token:{auth_token}@"
                    )
                    repo = git.Repo.clone_from(parsed_url, local_path)
                else:
                    # For SSH or other protocols, use standard clone
                    repo = git.Repo.clone_from(repo_url, local_path)
            else:
                repo = git.Repo.clone_from(repo_url, local_path)
            return f"Successfully cloned repository to '{local_path}'"
        except Exception as e:
            raise ValueError(f"Failed to clone repository: {str(e)}")

    def remove_file(self, repo_path: str, file_path: str) -> str:
        """Remove a file from Git"""
        repo = self._get_repo(repo_path)
        try:
            repo.index.remove([file_path])
            return f"Successfully removed {file_path} from Git"
        except Exception as e:
            raise ValueError(f"Failed to remove file: {str(e)}")

    def get_file_content_at_version(self, repo_path: str, file_path: str, version: str) -> str:
        """Get file content at a specific Git version"""
        repo = self._get_repo(repo_path)
        try:
            return repo.git.show(f"{version}:{file_path}")
        except Exception as e:
            raise ValueError(f"Failed to get file content at version {version}: {str(e)}")

    def setup_lfs(self, repo_path: str, file_patterns: List[str]) -> str:
        """Configure Git LFS for the repository"""
        repo = self._get_repo(repo_path)
        try:
            # Initialize LFS
            repo.git.execute(["git", "lfs", "install"])
            # Track file patterns
            for pattern in file_patterns:
                repo.git.execute(["git", "lfs", "track", pattern])
            # Add .gitattributes
            repo.git.add(".gitattributes")
            repo.git.commit("-m", "Set up Git LFS tracking")
            return "Git LFS configured successfully"
        except Exception as e:
            raise ValueError(f"Failed to set up Git LFS: {str(e)}")

    def batch_commit(
        self, repo_path: str, file_groups: List[List[str]], message_template: str
    ) -> List[str]:
        """Commit files in batches for better performance"""
        repo = self._get_repo(repo_path)
        commit_hashes = []
        for i, files in enumerate(file_groups):
            repo.git.add(files)
            commit = repo.index.commit(f"{message_template} (batch {i+1}/{len(file_groups)})")
            commit_hashes.append(commit.hexsha)
        return commit_hashes

    def pull(self, repo_path: str, remote: str = "origin", branch: str = None) -> str:
        """Pull changes from remote repository"""
        repo = self._get_repo(repo_path)
        try:
            if branch:
                result = repo.git.pull(remote, branch)
            else:
                result = repo.git.pull(remote)
            return result
        except Exception as e:
            raise ValueError(f"Failed to pull changes: {str(e)}")

    def fetch(self, repo_path: str, remote: str = "origin", all_remotes: bool = False) -> str:
        """Fetch changes from remote repository"""
        repo = self._get_repo(repo_path)
        try:
            if all_remotes:
                result = repo.git.fetch(all=True)
            else:
                result = repo.git.fetch(remote)
            return result
        except Exception as e:
            raise ValueError(f"Failed to fetch changes: {str(e)}")

    def create_tag(
        self, repo_path: str, tag_name: str, message: str = None, commit: str = "HEAD"
    ) -> str:
        """Create a new Git tag"""
        repo = self._get_repo(repo_path)
        try:
            if message:
                repo.create_tag(tag_name, ref=commit, message=message)
            else:
                repo.create_tag(tag_name, ref=commit)
            return f"Created tag '{tag_name}'"
        except Exception as e:
            raise ValueError(f"Failed to create tag: {str(e)}")

    def list_tags(self, repo_path: str) -> List[Dict[str, str]]:
        """List all tags in the repository"""
        repo = self._get_repo(repo_path)
        try:
            tags = []
            for tag in repo.tags:
                tags.append(
                    {
                        "name": tag.name,
                        "commit": tag.commit.hexsha,
                        "date": tag.commit.committed_datetime.strftime("%Y-%m-%d %H:%M:%S %z"),
                    }
                )
            return tags
        except Exception as e:
            raise ValueError(f"Failed to list tags: {str(e)}")


git_service = GitService()


@router.post(
    "/status",
    response_model=Dict[str, Any],
    summary="Get repository status",
    description="Get the current status of a Git repository, including the current branch, staged changes, and unstaged changes.",
)
async def get_status(request: GitStatusRequest = Body(...)):
    """
    Get the current status of a Git repository.
    Returns the current branch, staged changes, and unstaged changes.
    """
    try:
        return git_service.get_status(request.repo_path)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get status: {str(e)}")


@router.post(
    "/diff",
    response_model=str,
    summary="Get diff of changes",
    description="Get difference between working directory and HEAD or a specified target",
)
async def get_diff(request: GitDiffRequest = Body(...)):
    """
    Get difference between working directory and HEAD or a specified target.
    Show diff for specific files or the entire repository.
    """
    try:
        return git_service.get_diff(request.repo_path, request.file_path, request.target)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get diff: {str(e)}")


@router.post(
    "/add", response_model=str, summary="Stage files", description="Stage files for commit"
)
async def add_files(request: GitAddRequest = Body(...)):
    """
    Stage files for commit.
    Adds the specified files to the staging area.
    """
    try:
        return git_service.add_files(request.repo_path, request.files)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to add files: {str(e)}")


@router.post(
    "/commit",
    response_model=str,
    summary="Commit changes",
    description="Commit staged changes with a commit message.",
)
async def commit_changes(request: GitCommitRequest = Body(...)):
    """
    Commit staged changes with a commit message.
    Optionally, specify the author name and email.
    """
    try:
        return git_service.commit_changes(
            request.repo_path, request.message, request.author_name, request.author_email
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to commit changes: {str(e)}")


@router.post(
    "/log",
    response_model=Dict[str, Any],
    summary="Get commit log",
    description="Get the commit history of the repository",
)
async def get_log(request: GitLogRequest = Body(...)):
    """
    Get the commit history of the repository.
    Returns a list of commits with hash, author, date, and message.
    """
    try:
        return git_service.get_log(request.repo_path, request.max_count, request.file_path)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get log: {str(e)}")


@router.post(
    "/branch",
    response_model=str,
    summary="Create branch",
    description="Create a new branch in the repository",
)
async def create_branch(request: GitBranchRequest = Body(...)):
    """
    Create a new branch in the repository.
    Optionally, specify the base branch to create the new branch from.
    """
    try:
        return git_service.create_branch(
            request.repo_path, request.branch_name, request.base_branch
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create branch: {str(e)}")


@router.post(
    "/checkout", response_model=str, summary="Checkout branch", description="Checkout a branch"
)
async def checkout_branch(request: GitCheckoutRequest = Body(...)):
    """
    Checkout a branch.
    Optionally, create the branch if it doesn't exist.
    """
    try:
        return git_service.checkout_branch(request.repo_path, request.branch_name, request.create)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to checkout branch: {str(e)}")


@router.post(
    "/clone",
    response_model=str,
    summary="Clone repository",
    description="Clone a Git repository from a URL",
)
async def clone_repo(request: GitCloneRequest = Body(...)):
    """
    Clone a Git repository from a URL.
    Clones the repository from the specified URL to a local path.
    """
    try:
        return git_service.clone_repo(request.repo_url, request.local_path, request.auth_token)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to clone repository: {str(e)}")


@router.post(
    "/remove",
    response_model=str,
    summary="Remove file",
    description="Remove a file from the repository",
)
async def remove_file(request: GitRemoveFileRequest = Body(...)):
    """
    Remove a file from the repository.
    Removes the specified file from the Git repository.
    """
    try:
        return git_service.remove_file(request.repo_path, request.file_path)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to remove file: {str(e)}")


@router.post(
    "/file-content",
    response_model=str,
    summary="Get file content",
    description="Get file content at a specific Git version",
)
async def get_file_content(request: GitFileContentRequest = Body(...)):
    """
    Get file content at a specific Git version.
    Returns the content of the specified file at the given version.
    """
    try:
        return git_service.get_file_content_at_version(
            request.repo_path, request.file_path, request.version
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get file content: {str(e)}")


@router.post(
    "/lfs",
    response_model=str,
    summary="Set up Git LFS",
    description="Configure Git LFS for the repository",
)
async def setup_lfs(request: GitLFSRequest = Body(...)):
    """
    Configure Git LFS for the repository.
    Tracks the specified file patterns with Git LFS.
    """
    try:
        return git_service.setup_lfs(request.repo_path, request.file_patterns)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to set up Git LFS: {str(e)}")


@router.post(
    "/batch-commit",
    response_model=List[str],
    summary="Batch commit",
    description="Commit files in batches",
)
async def batch_commit(request: GitBatchCommitRequest = Body(...)):
    """
    Commit files in batches.
    Commits the specified file groups in batches for better performance.
    """
    try:
        return git_service.batch_commit(
            request.repo_path, request.file_groups, request.message_template
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to batch commit: {str(e)}")


@router.post(
    "/pull",
    response_model=str,
    summary="Pull changes",
    description="Pull changes from a remote repository",
)
async def pull_changes(request: GitPullRequest = Body(...)):
    """
    Pull changes from a remote repository.
    Fetches and merges changes from the specified remote and branch.
    """
    try:
        return git_service.pull(request.repo_path, request.remote, request.branch)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to pull changes: {str(e)}")


@router.post(
    "/fetch",
    response_model=str,
    summary="Fetch changes",
    description="Fetch changes from a remote repository",
)
async def fetch_changes(request: GitFetchRequest = Body(...)):
    """
    Fetch changes from a remote repository.
    Downloads objects and refs from the specified remote.
    """
    try:
        return git_service.fetch(request.repo_path, request.remote, request.all_remotes)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to fetch changes: {str(e)}")


@router.post(
    "/tag/create",
    response_model=str,
    summary="Create tag",
    description="Create a new tag in the repository",
)
async def create_tag(request: GitCreateTagRequest = Body(...)):
    """
    Create a new tag in the repository.
    Tags a specific commit with a name and optional message.
    """
    try:
        return git_service.create_tag(
            request.repo_path, request.tag_name, request.message, request.commit
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create tag: {str(e)}")


@router.post(
    "/tags",
    response_model=GitTagsResponse,
    summary="List tags",
    description="List all tags in the repository",
)
async def list_tags(request: GitRepoPath = Body(...)):
    """
    List all tags in the repository.
    Returns tag names, associated commits, and dates.
    """
    try:
        tags = git_service.list_tags(request.repo_path)
        return {"tags": tags}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list tags: {str(e)}")


================================================================================
FILE: app\api\memory.py
LANGUAGE: python
SIZE: 16632 bytes
================================================================================

from fastapi import APIRouter, Body, HTTPException, Query, Path
from typing import Dict, Any, List, Optional, Union
from pydantic import BaseModel, Field
import random
import time
import logging
import json
import os
from pathlib import Path as PathLib

# Import models after defining base models
from app.models.memory import (
    Entity,
    Relation,
    KnowledgeGraph,
    AddEntitiesRequest,
    AddRelationsRequest,
)


# Define models
class ScrapeSingleUrlRequest(BaseModel):
    """Request to scrape a single URL"""

    url: str = Field(..., description="URL to scrape")
    wait_for_selector: Optional[str] = Field(None, description="CSS selector to wait for")
    wait_for_timeout: Optional[int] = Field(30000, description="Maximum wait time in ms")
    extract_tables: bool = Field(True, description="Extract tables from content")
    store_as_document: bool = Field(False, description="Store result as a document")
    document_tags: Optional[List[str]] = Field(None, description="Tags for document if stored")


class UrlList(BaseModel):
    """Request to scrape multiple URLs"""

    urls: List[str] = Field(..., description="List of URLs to scrape")
    recursion_depth: int = Field(0, ge=0, le=3, description="How many links deep to follow (0-3)")
    store_as_documents: bool = Field(False, description="Save results as documents")
    document_tags: Optional[List[str]] = Field(None, description="Tags for documents if stored")


class ScrapeCrawlRequest(BaseModel):
    """Request to crawl a website"""

    start_url: str = Field(..., description="Starting URL for crawl")
    max_pages: int = Field(100, ge=1, description="Maximum number of pages to crawl")
    recursion_depth: int = Field(1, ge=1, description="How many links deep to follow")
    allowed_domains: Optional[List[str]] = Field(
        None, description="Restrict crawling to these domains"
    )
    create_documents: bool = Field(True, description="Create documents from scraped content")
    document_tags: Optional[List[str]] = Field(None, description="Tags for documents if created")
    verification_pass: bool = Field(False, description="Run verification pass after initial crawl")


class SearchAndScrapeRequest(BaseModel):
    """Request to search and scrape results"""

    query: str = Field(..., description="Search query")
    max_results: int = Field(10, ge=1, le=50, description="Maximum search results to process")
    create_documents: bool = Field(False, description="Create documents from scraped content")
    document_tags: Optional[List[str]] = Field(None, description="Tags for documents if created")


class SitemapScrapeRequest(BaseModel):
    """Request to scrape URLs from a sitemap"""

    sitemap_url: str = Field(..., description="URL of the sitemap")
    max_urls: int = Field(50, ge=1, description="Maximum number of URLs to scrape")
    create_documents: bool = Field(True, description="Create documents from scraped content")
    document_tags: Optional[List[str]] = Field(None, description="Tags for documents if created")


class TableData(BaseModel):
    headers: List[str] = Field(default_factory=list, description="Table headers")
    rows: List[List[str]] = Field(default_factory=list, description="Table rows")


class ScraperResponse(BaseModel):
    """Response from scraper"""

    url: str = Field(..., description="Scraped URL")
    title: str = Field(..., description="Page title")
    content: str = Field(..., description="Cleaned content in Markdown format")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Extracted metadata")
    scraped_at: int = Field(..., description="Timestamp when scraped")
    success: bool = Field(True, description="Whether scraping was successful")
    links: List[str] = Field(default_factory=list, description="Links extracted from content")
    document_id: Optional[str] = Field(None, description="Document ID if saved as document")
    error: Optional[str] = Field(None, description="Error message if scraping failed")


# Import services after the model definitions to avoid circular imports
from app.core.scraper_service import ScraperService
from app.core.documents_service import DocumentsService
from app.core.memory_service import MemoryService
from app.models.documents import DocumentType
from app.utils.config import get_config

# Create router and services
router = APIRouter()
scraper_service = ScraperService()
documents_service = DocumentsService()
memory_service = MemoryService()


# Define routes
@router.post(
    "/url",
    response_model=ScraperResponse,
    summary="Scrape a single URL",
    description="Extract content from a web page and convert to Markdown",
)
async def scrape_url(request: ScrapeSingleUrlRequest = Body(...)):
    """
    Scrape a single URL and return structured data.
    Extracts content, converts to Markdown, and optionally stores as a document.
    """
    try:
        result = await scraper_service.scrape_url(
            request.url, request.wait_for_selector, request.wait_for_timeout
        )
        # If requested, store as document
        if request.store_as_document and result["success"]:
            doc = documents_service.create_document(
                title=result["title"],
                content=result["content"],
                document_type=DocumentType.WEBPAGE,
                metadata=result["metadata"],
                tags=request.document_tags,
                source_url=result["url"],
            )
            result["document_id"] = doc["id"]
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Scraping failed: {str(e)}")


@router.post(
    "/urls",
    response_model=List[ScraperResponse],
    summary="Scrape multiple URLs",
    description="Scrape multiple URLs in parallel",
)
async def scrape_multiple_urls(request: UrlList = Body(...)):
    """
    Scrape multiple URLs in parallel.
    Processes a list of URLs and returns the scraped content for each.
    """
    try:
        results = await scraper_service.scrape_urls(request.urls)
        # If requested, store results as documents
        if request.store_as_documents:
            for i, result in enumerate(results):
                if result["success"]:
                    try:
                        doc = documents_service.create_document(
                            title=result["title"],
                            content=result["content"],
                            document_type=DocumentType.WEBPAGE,
                            metadata=result["metadata"],
                            tags=request.document_tags,
                            source_url=result["url"],
                        )
                        results[i]["document_id"] = doc["id"]
                    except Exception as e:
                        results[i]["error"] = f"Document creation failed: {str(e)}"
        return results
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Scraping failed: {str(e)}")


@router.post(
    "/crawl",
    response_model=Dict[str, Any],
    summary="Crawl website",
    description="Crawl a website starting from a URL",
)
async def crawl_website(request: ScrapeCrawlRequest = Body(...)):
    """
    Crawl a website starting from a URL.
    Follows links up to a specified depth and processes each page.
    Optional verification pass ensures content stability.
    """
    try:
        results = await scraper_service.crawl_website(
            request.start_url,
            request.max_pages,
            request.recursion_depth,
            request.allowed_domains,
            request.verification_pass,
        )
        response = {
            "pages_crawled": results.get("pages_crawled", 0),
            "start_url": request.start_url,
            "success_count": results.get("success_count", 0),
            "failed_count": results.get("failed_count", 0),
        }
        # Include verification results if available
        if "verification_results" in results:
            response["verification_results"] = results["verification_results"]
            response["verification_success_rate"] = results["verification_success_rate"]
        # If requested, create documents
        if request.create_documents:
            document_ids = []
            for result in results.get("results", []):
                if result.get("success", False):
                    try:
                        doc = documents_service.create_document(
                            title=result["title"],
                            content=result["content"],
                            document_type=DocumentType.WEBPAGE,
                            metadata=result["metadata"],
                            tags=request.document_tags,
                            source_url=result["url"],
                        )
                        document_ids.append(doc["id"])
                    except Exception:
                        pass
            response["documents_created"] = len(document_ids)
            response["document_ids"] = document_ids
        return response
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Crawling failed: {str(e)}")


@router.post(
    "/search",
    response_model=List[ScraperResponse],
    summary="Search and scrape",
    description="Search for content and scrape the results",
)
async def search_and_scrape(request: SearchAndScrapeRequest = Body(...)):
    """
    Search for content and scrape the results.
    Performs a web search and scrapes the top results.
    """
    try:
        results = await scraper_service.search_and_scrape(request.query, request.max_results)
        # If requested, create documents
        if request.create_documents:
            for i, result in enumerate(results):
                if result.get("success", False):
                    try:
                        doc = documents_service.create_document(
                            title=result["title"],
                            content=result["content"],
                            document_type=DocumentType.WEBPAGE,
                            metadata=result["metadata"],
                            tags=request.document_tags,
                            source_url=result["url"],
                        )
                        results[i]["document_id"] = doc["id"]
                    except Exception:
                        pass
        return results
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Search and scrape failed: {str(e)}")


@router.post(
    "/sitemap",
    response_model=Dict[str, Any],
    summary="Scrape sitemap",
    description="Extract URLs from sitemap and scrape them",
)
async def scrape_sitemap(request: SitemapScrapeRequest = Body(...)):
    """
    Extract URLs from a sitemap and scrape them.
    Processes XML sitemap files and scrapes the listed URLs.
    """
    try:
        result = await scraper_service.scrape_sitemap(request.sitemap_url, request.max_urls)
        # Handle document creation if requested
        if request.create_documents and result.get("urls_scraped", []):
            document_ids = []
            for scraped_url in result["urls_scraped"]:
                if scraped_url.get("success", False):
                    try:
                        doc = documents_service.create_document(
                            title=scraped_url["title"],
                            content=scraped_url["content"],
                            document_type=DocumentType.WEBPAGE,
                            metadata=scraped_url["metadata"],
                            tags=request.document_tags,
                            source_url=scraped_url["url"],
                        )
                        document_ids.append(doc["id"])
                    except Exception:
                        pass
            result["documents_created"] = len(document_ids)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Sitemap scraping failed: {str(e)}")


# Add knowledge graph routes
@router.post(
    "/entities",
    response_model=List[Dict[str, Any]],
    summary="Add entities",
    description="Add entities to the knowledge graph",
)
async def add_entities(request: AddEntitiesRequest = Body(...)):
    """Add new entities to the knowledge graph"""
    try:
        created_entities = memory_service.create_entities(request.entities)
        return created_entities
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create entities: {str(e)}")


@router.post(
    "/relations",
    response_model=List[Dict[str, Any]],
    summary="Add relations",
    description="Add relationships to the knowledge graph",
)
async def add_relations(request: AddRelationsRequest = Body(...)):
    """Add new relations to the knowledge graph"""
    try:
        created_relations = memory_service.create_relations(request.relations)
        return created_relations
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create relations: {str(e)}")


@router.get(
    "/graph",
    response_model=KnowledgeGraph,
    summary="Get knowledge graph",
    description="Get the full knowledge graph",
)
async def get_graph():
    """Get the entire knowledge graph."""
    try:
        return memory_service.get_full_graph()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get graph: {str(e)}")


@router.get(
    "/entity/{entity_name}/related",
    response_model=Dict[str, Any],
    summary="Get related entities",
    description="Get entities related to a specific entity",
)
async def get_related_entities(
    entity_name: str = Path(..., description="Entity name"),
    max_depth: int = Query(1, description="Maximum relationship depth"),
):
    """Get entities related to a specific entity up to a maximum depth."""
    try:
        return memory_service.get_related_entities(entity_name, max_depth)
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get related entities: {str(e)}")


@router.get(
    "/entity/{entity_name}/connections",
    response_model=Dict[str, Any],
    summary="Get entity connections",
    description="Get direct connections for an entity",
)
async def get_entity_connections(entity_name: str = Path(..., description="Entity name")):
    """Get direct connections for a specific entity."""
    try:
        return memory_service.get_entity_connections(entity_name)
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get entity connections: {str(e)}")


@router.post(
    "/find-paths",
    response_model=List[List[Dict[str, Any]]],
    summary="Find paths",
    description="Find paths between entities in the knowledge graph",
)
async def find_paths(
    start_entity: str = Body(..., embed=True),
    end_entity: str = Body(..., embed=True),
    max_length: int = Body(3, embed=True),
):
    """Find paths between two entities in the knowledge graph, up to max_length."""
    try:
        paths = memory_service.find_paths(start_entity, end_entity, max_length)
        return paths
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to find paths: {str(e)}")


@router.post(
    "/similar-entities",
    response_model=List[Dict[str, Any]],
    summary="Find similar entities",
    description="Find entities with similar names",
)
async def find_similar_entities(
    entity_name: str = Body(..., embed=True), threshold: float = Body(0.6, embed=True)
):
    """Find entities with similar names to the provided entity name."""
    try:
        return memory_service.get_similar_entities(entity_name, threshold)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to find similar entities: {str(e)}")


================================================================================
FILE: app\api\scraper.py
LANGUAGE: python
SIZE: 7622 bytes
================================================================================

from fastapi import APIRouter, Body, HTTPException, Query, Path
from typing import Dict, Any, List, Optional
from app.models.scraper import (
    ScrapeSingleUrlRequest,
    UrlList,
    ScrapeCrawlRequest,
    SearchAndScrapeRequest,
    ScraperResponse,
)
from app.core.scraper_service import ScraperService
from app.core.documents_service import DocumentsService
from app.models.documents import DocumentType

router = APIRouter(
    responses={400: {"description": "Bad request"}, 500: {"description": "Scraping failed"}}
)
scraper_service = ScraperService()
documents_service = DocumentsService()


@router.post(
    "/url",
    response_model=ScraperResponse,
    summary="Scrape a single URL",
    description="Extract content from a web page and convert to Markdown",
)
async def scrape_url(request: ScrapeSingleUrlRequest = Body(...)):
    """
    Scrape a single URL and return structured data.
    Extracts content, converts to Markdown, and optionally stores as a document.
    """
    try:
        result = await scraper_service.scrape_url(
            request.url, request.wait_for_selector, request.wait_for_timeout
        )
        # If requested, store as document
        if request.store_as_document and result["success"]:
            doc = documents_service.create_document(
                title=result["title"],
                content=result["content"],
                document_type=DocumentType.WEBPAGE,
                metadata=result["metadata"],
                tags=request.document_tags,
                source_url=result["url"],
            )
            result["document_id"] = doc["id"]
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Scraping failed: {str(e)}")


@router.post(
    "/urls",
    response_model=List[ScraperResponse],
    summary="Scrape multiple URLs",
    description="Scrape multiple URLs in parallel",
)
async def scrape_multiple_urls(request: UrlList = Body(...)):
    """
    Scrape multiple URLs in parallel.
    Processes a list of URLs and returns the scraped content for each.
    """
    try:
        results = await scraper_service.scrape_urls(request.urls)
        # If requested, store results as documents
        if request.store_as_documents:
            for i, result in enumerate(results):
                if result["success"]:
                    try:
                        doc = documents_service.create_document(
                            title=result["title"],
                            content=result["content"],
                            document_type=DocumentType.WEBPAGE,
                            metadata=result["metadata"],
                            tags=request.document_tags,
                            source_url=result["url"],
                        )
                        results[i]["document_id"] = doc["id"]
                    except Exception as e:
                        results[i]["error"] = f"Document creation failed: {str(e)}"
        return results
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Scraping failed: {str(e)}")


@router.post(
    "/crawl",
    response_model=Dict[str, Any],
    summary="Crawl website",
    description="Crawl a website starting from a URL",
)
async def crawl_website(request: ScrapeCrawlRequest = Body(...)):
    """
    Crawl a website starting from a URL.
    Follows links up to a specified depth and processes each page.
    Optional verification pass ensures content stability.
    """
    try:
        results = await scraper_service.crawl_website(
            request.start_url,
            request.max_pages,
            request.recursion_depth,
            request.allowed_domains,
            request.verification_pass,  # Pass the verification_pass parameter
        )
        response = {
            "pages_crawled": results.get("pages_crawled", 0),
            "start_url": request.start_url,
            "success_count": results.get("success_count", 0),
            "failed_count": results.get("failed_count", 0),
        }
        # Include verification results if available
        if "verification_results" in results:
            response["verification_results"] = results["verification_results"]
            response["verification_success_rate"] = results["verification_success_rate"]
        # If requested, create documents
        if request.create_documents:
            document_ids = []
            for result in results.get("results", []):
                if result.get("success", False):
                    try:
                        doc = documents_service.create_document(
                            title=result["title"],
                            content=result["content"],
                            document_type=DocumentType.WEBPAGE,
                            metadata=result["metadata"],
                            tags=request.document_tags,
                            source_url=result["url"],
                        )
                        document_ids.append(doc["id"])
                    except Exception:
                        pass
            response["documents_created"] = len(document_ids)
            response["document_ids"] = document_ids
        return response
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Crawling failed: {str(e)}")


@router.post(
    "/search",
    response_model=List[ScraperResponse],
    summary="Search and scrape",
    description="Search for content and scrape the results",
)
async def search_and_scrape(request: SearchAndScrapeRequest = Body(...)):
    """
    Search for content and scrape the results.
    Performs a web search and scrapes the top results.
    """
    try:
        results = await scraper_service.search_and_scrape(request.query, request.max_results)
        # If requested, create documents
        if request.create_documents:
            for i, result in enumerate(results):
                if result.get("success", False):
                    try:
                        doc = documents_service.create_document(
                            title=result["title"],
                            content=result["content"],
                            document_type=DocumentType.WEBPAGE,
                            metadata=result["metadata"],
                            tags=request.document_tags,
                            source_url=result["url"],
                        )
                        results[i]["document_id"] = doc["id"]
                    except Exception:
                        pass
        return results
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Search and scrape failed: {str(e)}")


@router.post(
    "/screenshot",
    response_model=Dict[str, Any],
    summary="Capture screenshot",
    description="Capture screenshot of a webpage",
)
async def capture_screenshot(
    url: str = Body(..., embed=True), full_page: bool = Body(True, embed=True)
):
    """
    Capture a screenshot of a URL.
    Returns the path to the saved screenshot file.
    """
    try:
        result = await scraper_service.capture_screenshot(url, full_page)
        if not result["success"]:
            raise HTTPException(status_code=500, detail=result["error"])
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Screenshot failed: {str(e)}")


================================================================================
FILE: app\core\__init__.py
LANGUAGE: python
SIZE: 411 bytes
================================================================================

"""
Core Services for the Unified Tools Server

This package contains the core service implementations:
- FilesystemService: File operations for local and S3 storage
- GitService: Git repository operations and version control
- MemoryService: Knowledge graph and entity relation management
- DocumentsService: Document storage and retrieval
- ScraperService: Web content extraction and processing
"""


================================================================================
FILE: app\core\documents_service.py
LANGUAGE: python
SIZE: 25788 bytes
================================================================================

import os
import io
import hashlib
import json
import tempfile
import time
import threading
import random
import uuid
import re
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Union, Set, Tuple
from fastapi import HTTPException
from git import Repo
from pydantic import BaseModel, Field
from app.models.documents import (
    DocumentType,
    CreateDocumentRequest,
    UpdateDocumentRequest,
    DocumentResponse,
    DocumentVersionResponse,
    DocumentContentResponse,
)
from app.core.filesystem_service import FilesystemService
from app.core.memory_service import MemoryService
from app.core.git_service import GitService
from app.utils.config import get_config

logger = logging.getLogger(__name__)

try:
    import markdown
except ImportError:
    markdown = None
    logger.warning("Markdown package not installed. Markdown to HTML conversion will be limited.")


class DocumentsService:
    def __init__(self, base_path: str = None, large_content_threshold: int = 100000):
        """Initialize the document service"""
        config = get_config()

        self.base_path = Path(base_path or os.path.join(os.getcwd(), "data", "documents"))
        self.base_path.mkdir(parents=True, exist_ok=True)

        self.repo_path = str(self.base_path)
        self.git_service = GitService()

        for doc_type in DocumentType:
            (self.base_path / doc_type.value).mkdir(exist_ok=True)

        readme_path = self.base_path / "README.md"
        if not readme_path.exists():
            with open(readme_path, "w", encoding="utf-8") as f:
                f.write("# Document Storage\n\n")
                f.write(
                    "This directory contains documents managed by the Tools Server Document Service.\n"
                )
                f.write(
                    "It supports various document types including manuscripts (stories, novels),\n"
                )
                f.write("documentation (technical, research), and datasets.")
                f.write(
                    "\n\nLarge documents (70,000+ words) are automatically chunked for better performance.\n"
                )

            self.git_service.add_files(self.repo_path, ["README.md"])
            self.git_service.commit_changes(self.repo_path, "Initialize document repository")

        self.large_content_threshold = large_content_threshold

        self.index_dir = self.base_path / ".index"
        self.index_dir.mkdir(exist_ok=True)

        self.vector_search_enabled = False
        self.vector_model = None

        try:
            import numpy as np
            from sentence_transformers import SentenceTransformer

            self.np = np
            self.vector_model = SentenceTransformer("all-MiniLM-L6-v2")
            self.vector_search_enabled = True
            self.vector_index_path = self.base_path / ".vectors"
            self.vector_index_path.mkdir(exist_ok=True)
        except ImportError:
            logger.warning("Vector search dependencies not installed. Semantic search disabled.")

        self.index_lock = threading.Lock()
        self.file_locks = {}

    def _get_file_lock(self, doc_id):
        """Get a lock for a specific file to prevent concurrent modifications"""
        if doc_id not in self.file_locks:
            self.file_locks[doc_id] = threading.Lock()
        return self.file_locks[doc_id]

    def create_document(
        self,
        title: str,
        content: str,
        document_type: DocumentType,
        metadata: Dict[str, Any] = None,
        tags: List[str] = None,
        source_url: Optional[str] = None,
        storage_type: str = "local",
    ) -> Dict[str, Any]:
        """Create a new document with Git versioning"""
        doc_id = f"doc_{int(time.time())}_{uuid.uuid4().hex[:8]}"
        now = int(time.time())

        metadata = metadata or {}
        tags = tags or []

        doc_dir = self.base_path / document_type.value
        doc_path = doc_dir / f"{doc_id}.md"

        frontmatter = f"---\ntitle: {title}\ncreated_at: {now}\nupdated_at: {now}\nid: {doc_id}\ndocument_type: {document_type.value}\n"

        if tags:
            frontmatter += f"tags: {', '.join(tags)}\n"

        if source_url:
            frontmatter += f"source_url: {source_url}\n"

        for key, value in metadata.items():
            if isinstance(value, (str, int, float, bool)):
                frontmatter += f"{key}: {value}\n"

        frontmatter += "---\n\n"
        full_content = frontmatter + content

        doc_path.write_text(full_content, encoding="utf-8")

        self._update_index(
            doc_id,
            {
                "id": doc_id,
                "title": title,
                "document_type": document_type.value,
                "created_at": now,
                "updated_at": now,
                "tags": tags,
                "metadata": metadata,
                "size_bytes": len(full_content.encode("utf-8")),
                "source_url": source_url,
                "path": str(doc_path.relative_to(self.base_path)),
            },
        )

        rel_path = doc_path.relative_to(self.base_path)
        self.git_service.add_files(self.repo_path, [str(rel_path)])
        self.git_service.commit_changes(self.repo_path, f"Created document: {title}")

        self._update_memory_graph(doc_id, title, document_type.value, tags, metadata, source_url)

        self.generate_embeddings(doc_id, content)

        return self.get_document(doc_id)

    def update_document(
        self,
        doc_id: str,
        title: Optional[str] = None,
        content: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
        tags: Optional[List[str]] = None,
        commit_message: str = "Updated document",
        expected_version: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Update an existing document with version control"""
        with self._get_file_lock(doc_id):
            doc_info = self._get_document_index(doc_id)
            if not doc_info:
                raise ValueError(f"Document with ID {doc_id} not found")

            if expected_version:
                current_version = None
                try:
                    log = self.git_service.get_log(
                        self.repo_path, max_count=1, file_path=doc_info["path"]
                    )
                    if log.get("commits"):
                        current_version = log["commits"][0]["hash"]
                    if current_version and current_version != expected_version:
                        raise ValueError(
                            "Document has been modified since you loaded it. Please refresh and try again."
                        )
                except Exception as e:
                    logger.warning(f"Version check failed: {e}")

            doc_path = self.base_path / doc_info["path"]
            if not doc_path.exists():
                raise ValueError(f"Document file not found at {doc_path}")

            current_content = doc_path.read_text(encoding="utf-8")

            frontmatter_match = re.match(r"---(.*?)---\n\n", current_content, re.DOTALL)
            if not frontmatter_match:
                raise ValueError("Invalid document format: missing frontmatter")

            frontmatter = frontmatter_match.group(1)
            existing_content = current_content[frontmatter_match.end() :]

            frontmatter_dict = {}
            for line in frontmatter.strip().split("\n"):
                if ": " in line:
                    key, value = line.split(": ", 1)
                    frontmatter_dict[key] = value

            now = int(time.time())
            frontmatter_dict["updated_at"] = str(now)

            if title:
                frontmatter_dict["title"] = title

            if tags is not None:
                if tags:
                    frontmatter_dict["tags"] = ", ".join(tags)
                else:
                    if "tags" in frontmatter_dict:
                        del frontmatter_dict["tags"]

            if metadata:
                for key, value in metadata.items():
                    if isinstance(value, (str, int, float, bool)):
                        frontmatter_dict[key] = str(value)

            new_frontmatter = "---\n"
            for key, value in frontmatter_dict.items():
                new_frontmatter += f"{key}: {value}\n"
            new_frontmatter += "---\n\n"

            final_content = content if content is not None else existing_content
            full_content = new_frontmatter + final_content

            doc_path.write_text(full_content, encoding="utf-8")

            doc_info_update = {
                "updated_at": now,
                "size_bytes": len(full_content.encode("utf-8")),
            }

            if title:
                doc_info_update["title"] = title

            if tags is not None:
                doc_info_update["tags"] = tags

            if metadata:
                doc_info_update["metadata"] = {**doc_info.get("metadata", {}), **metadata}

            with self.index_lock:
                self._update_index(doc_id, doc_info_update)

            rel_path = doc_path.relative_to(self.base_path)
            self.git_service.add_files(self.repo_path, [str(rel_path)])
            self.git_service.commit_changes(self.repo_path, commit_message)

            self._update_memory_graph(
                doc_id,
                title or doc_info.get("title"),
                doc_info.get("document_type"),
                tags if tags is not None else doc_info.get("tags", []),
                {**doc_info.get("metadata", {}), **(metadata or {})},
                doc_info.get("source_url"),
            )

            if content is not None:
                self.generate_embeddings(doc_id, content)

        return self.get_document(doc_id)

    def get_document(self, doc_id: str) -> Dict[str, Any]:
        """Get document metadata and preview"""
        doc_info = self._get_document_index(doc_id)
        if not doc_info:
            return None

        doc_path = self.base_path / doc_info["path"]
        if not doc_path.exists():
            return None

        content = doc_path.read_text(encoding="utf-8")

        content_without_frontmatter = re.sub(r"^---.*?---\n\n", "", content, flags=re.DOTALL)
        preview = content_without_frontmatter[:500] + (
            "..." if len(content_without_frontmatter) > 500 else ""
        )

        version_count = 1
        try:
            log = self.git_service.get_log(
                self.repo_path, max_count=100, file_path=doc_info["path"]
            )
            version_count = len(log.get("commits", []))
        except Exception:
            pass

        return {
            "id": doc_id,
            "title": doc_info.get("title", "Untitled"),
            "document_type": doc_info.get("document_type", DocumentType.GENERIC.value),
            "created_at": doc_info.get("created_at", 0),
            "updated_at": doc_info.get("updated_at", 0),
            "tags": doc_info.get("tags", []),
            "metadata": doc_info.get("metadata", {}),
            "content_preview": preview,
            "size_bytes": doc_info.get("size_bytes", 0),
            "version_count": version_count,
            "content_available": True,
            "source_url": doc_info.get("source_url"),
        }

    def get_document_content(self, doc_id: str, version: Optional[str] = None) -> Dict[str, Any]:
        """Get full document content, optionally from a specific version"""
        doc_info = self._get_document_index(doc_id)
        if not doc_info:
            return None

        doc_path = self.base_path / doc_info["path"]

        content = ""
        if version:
            try:
                content = self.git_service.get_file_content_at_version(
                    self.repo_path, doc_info["path"], version
                )
            except Exception:
                return None
        else:
            if not doc_path.exists():
                return None
            content = doc_path.read_text(encoding="utf-8")

        content_without_frontmatter = re.sub(r"^---.*?---\n\n", "", content, flags=re.DOTALL)
        return {
            "id": doc_id,
            "title": doc_info.get("title", "Untitled"),
            "content": content_without_frontmatter,
            "version": version,
        }

    def get_document_versions(self, doc_id: str, max_versions: int = 10) -> List[Dict[str, Any]]:
        """Get version history for a document"""
        doc_info = self._get_document_index(doc_id)
        if not doc_info:
            return []

        try:
            log = self.git_service.get_log(
                self.repo_path, max_count=max_versions, file_path=doc_info["path"]
            )
            versions = []
            for commit in log.get("commits", []):
                versions.append(
                    {
                        "version_hash": commit["hash"],
                        "commit_message": commit["message"],
                        "author": commit["author"],
                        "timestamp": int(
                            time.mktime(time.strptime(commit["date"], "%Y-%m-%d %H:%M:%S %z"))
                        ),
                    }
                )
            return versions
        except Exception as e:
            logger.error(f"Error getting versions: {e}", exc_info=True)
            return []

    def delete_document(self, doc_id: str) -> bool:
        """Delete a document"""
        doc_info = self._get_document_index(doc_id)
        if not doc_info:
            return False

        doc_path = self.base_path / doc_info["path"]
        if not doc_path.exists():
            return False

        try:
            doc_path.unlink()

            rel_path = doc_path.relative_to(self.base_path)
            self.git_service.remove_file(self.repo_path, str(rel_path))
            self.git_service.commit_changes(
                self.repo_path, f"Deleted document: {doc_info.get('title', doc_id)}"
            )

            self._remove_index(doc_id)

            self._remove_from_memory_graph(doc_id)

            return True
        except Exception as e:
            logger.error(f"Error deleting document: {e}", exc_info=True)
            return False

    def search_documents(
        self,
        query: str,
        doc_type: Optional[str] = None,
        tags: Optional[List[str]] = None,
        limit: int = 10,
    ) -> List[Dict[str, Any]]:
        """Search documents by query, type, and tags"""
        results = []

        index_files = list(self.index_dir.glob("*.json"))

        for index_file in index_files:
            try:
                doc_info = json.loads(index_file.read_text(encoding="utf-8"))

                if doc_type and doc_info.get("document_type") != doc_type:
                    continue

                if tags:
                    doc_tags = set(doc_info.get("tags", []))
                    if not all(tag in doc_tags for tag in tags):
                        continue

                if query:
                    query_lower = query.lower()
                    title = doc_info.get("title", "").lower()
                    doc_content = ""

                    if query and not (query_lower in title):
                        try:
                            doc_path = self.base_path / doc_info["path"]
                            content = doc_path.read_text(encoding="utf-8")
                            doc_content = re.sub(
                                r"^---.*?---\n\n", "", content, flags=re.DOTALL
                            ).lower()
                        except Exception:
                            pass

                    if not (query_lower in title or query_lower in doc_content):
                        continue

                results.append(
                    {
                        "id": doc_info.get("id"),
                        "title": doc_info.get("title", "Untitled"),
                        "document_type": doc_info.get("document_type", DocumentType.GENERIC.value),
                        "created_at": doc_info.get("created_at", 0),
                        "updated_at": doc_info.get("updated_at", 0),
                        "tags": doc_info.get("tags", []),
                        "metadata": doc_info.get("metadata", {}),
                        "size_bytes": doc_info.get("size_bytes", 0),
                        "source_url": doc_info.get("source_url"),
                    }
                )

                if len(results) >= limit:
                    break

            except Exception as e:
                logger.error(f"Error processing document index {index_file}: {e}", exc_info=True)

        return results

    def _update_index(self, doc_id: str, doc_info: Dict[str, Any]) -> None:
        """Update the document index"""
        with self.index_lock:
            index_path = self.index_dir / f"{doc_id}.json"

            if index_path.exists():
                current_info = json.loads(index_path.read_text(encoding="utf-8"))
                current_info.update(doc_info)
                index_path.write_text(json.dumps(current_info, indent=2), encoding="utf-8")
            else:
                index_path.write_text(json.dumps(doc_info, indent=2), encoding="utf-8")

    def _get_document_index(self, doc_id: str) -> Optional[Dict[str, Any]]:
        """Get document index by ID"""
        index_path = self.index_dir / f"{doc_id}.json"
        if not index_path.exists():
            return None

        try:
            return json.loads(index_path.read_text(encoding="utf-8"))
        except Exception:
            return None

    def _remove_index(self, doc_id: str) -> None:
        """Remove document index"""
        index_path = self.index_dir / f"{doc_id}.json"
        if index_path.exists():
            index_path.unlink()

    def _update_memory_graph(
        self,
        doc_id: str,
        title: str,
        doc_type: str,
        tags: List[str],
        metadata: Dict[str, Any],
        source_url: Optional[str] = None,
    ) -> None:
        """Update the knowledge graph with document references"""
        try:
            doc_entity_name = f"document:{doc_id}"

            observations = [
                f"Title: {title}",
                f"Type: {doc_type}",
            ]

            if tags:
                observations.append(f"Tags: {', '.join(tags)}")

            if source_url:
                observations.append(f"Source URL: {source_url}")

            for key, value in metadata.items():
                if isinstance(value, (str, int, float, bool)):
                    observations.append(f"{key}: {value}")

            self.memory_service.create_entities(
                [{"name": doc_entity_name, "entity_type": "document", "observations": observations}]
            )

            relations = []
            for tag in tags:
                tag_entity_name = f"tag:{tag}"
                self.memory_service.create_entities(
                    [
                        {
                            "name": tag_entity_name,
                            "entity_type": "tag",
                            "observations": [f"Document tag: {tag}"],
                        }
                    ]
                )

                relations.append(
                    {"from": doc_entity_name, "to": tag_entity_name, "relation_type": "tagged_with"}
                )

            if source_url:
                source_entity_name = f"source:{source_url.replace('://', '_').replace('/', '_')}"
                self.memory_service.create_entities(
                    [
                        {
                            "name": source_entity_name,
                            "entity_type": "source",
                            "observations": [f"URL: {source_url}"],
                        }
                    ]
                )

                relations.append(
                    {
                        "from": doc_entity_name,
                        "to": source_entity_name,
                        "relation_type": "sourced_from",
                    }
                )

            if relations:
                self.memory_service.create_relations(relations)

        except Exception as e:
            logger.error(f"Error updating memory graph: {e}", exc_info=True)

    def _remove_from_memory_graph(self, doc_id: str) -> None:
        """Remove document from knowledge graph"""
        try:
            doc_entity_name = f"document:{doc_id}"
            self.memory_service.delete_entities([doc_entity_name])
        except Exception as e:
            logger.error(f"Error removing document from memory graph: {e}", exc_info=True)

    def generate_embeddings(self, doc_id: str, content: str) -> None:
        """Generate and store embeddings for a document"""
        if not self.vector_search_enabled:
            return

        try:
            embedding = self.vector_model.encode(content[:10000])

            embedding_path = self.vector_index_path / f"{doc_id}.npy"
            self.np.save(embedding_path, embedding)
        except Exception as e:
            logger.error(f"Error generating embeddings for document {doc_id}: {str(e)}")

    def semantic_search(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """Search documents semantically using vector similarity"""
        if not self.vector_search_enabled:
            raise ValueError("Vector search not enabled")

        try:
            query_embedding = self.vector_model.encode(query)

            results = []
            for embedding_file in self.vector_index_path.glob("*.npy"):
                doc_id = embedding_file.stem
                doc_embedding = self.np.load(embedding_file)

                similarity = self.np.dot(query_embedding, doc_embedding) / (
                    self.np.linalg.norm(query_embedding) * self.np.linalg.norm(doc_embedding)
                )

                results.append((doc_id, float(similarity)))

            results.sort(key=lambda x: x[1], reverse=True)
            top_results = results[:limit]

            return [self.get_document(doc_id) for doc_id, _ in top_results]
        except Exception as e:
            logger.error(f"Error during semantic search: {str(e)}")
            return []

    def convert_document_format(self, doc_id: str, target_format: str) -> bytes:
        """Convert document to different formats (PDF, DOCX, etc.)"""
        doc_content = self.get_document_content(doc_id)
        if not doc_content:
            raise ValueError(f"Document {doc_id} not found")

        content = doc_content["content"]
        title = doc_content["title"]

        if target_format.lower() == "pdf":
            try:
                if markdown is None:
                    raise ImportError("markdown package is required for PDF conversion")

                try:
                    import weasyprint
                except ImportError:
                    raise ImportError("weasyprint package is required for PDF conversion")

                html_content = f"<h1>{title}</h1>{markdown.markdown(content)}"
                pdf_bytes = weasyprint.HTML(string=html_content).write_pdf()
                return pdf_bytes
            except ImportError as e:
                logger.error(f"{e}")
                raise ValueError(str(e))

        elif target_format.lower() == "docx":
            try:
                try:
                    from docx import Document
                except ImportError:
                    raise ImportError("python-docx package is required for DOCX conversion")

                doc = Document()
                doc.add_heading(title, 0)
                doc.add_paragraph(content)

                buffer = io.BytesIO()
                doc.save(buffer)
                buffer.seek(0)
                return buffer.read()
            except ImportError as e:
                logger.error(f"{e}")
                raise ValueError(str(e))
        else:
            raise ValueError(f"Unsupported format: {target_format}")

    def get_document_diff(
        self, doc_id: str, from_version: str, to_version: str = "HEAD"
    ) -> Dict[str, Any]:
        """Get differences between document versions"""
        doc_info = self._get_document_index(doc_id)
        if not doc_info:
            raise ValueError(f"Document with ID {doc_id} not found")

        try:
            diff = self.git_service.get_diff(
                self.repo_path, doc_info["path"], from_version, to_version
            )

            return {
                "id": doc_id,
                "title": doc_info.get("title", "Untitled"),
                "from_version": from_version,
                "to_version": to_version,
                "diff": diff,
            }
        except Exception as e:
            logger.error(f"Error getting document diff: {e}", exc_info=True)
            raise ValueError(f"Error getting document diff: {e}")


================================================================================
FILE: app\core\filesystem_service.py
LANGUAGE: python
SIZE: 15825 bytes
================================================================================

import os
import pathlib
import logging
import hashlib
from typing import Optional, List, Dict, Any
import time
import threading
import glob
import shutil
import fnmatch
from app.utils.config import get_config

# Third-party imports in try/except for graceful handling
try:
    import boto3
    HAS_BOTO3 = True
except ImportError:
    boto3 = None
    HAS_BOTO3 = False

logger = logging.getLogger(__name__)

class FilesystemService:
    def __init__(self):
        config = get_config()
        self.allowed_directories = [str(pathlib.Path(os.path.expanduser(d)).resolve())
                                   for d in config.allowed_directories]
        self.s3_client = None
        self.s3_resource = None
        
        if config.s3_access_key and config.s3_secret_key:
            try:
                if not HAS_BOTO3:
                    logger.warning("boto3 is not installed. S3 functionality will be disabled.")
                else:
                    self.s3_client = boto3.client(
                        's3',
                        aws_access_key_id=config.s3_access_key,
                        aws_secret_access_key=config.s3_secret_key,
                        region_name=config.s3_region
                    )
                    self.s3_resource = boto3.resource(
                        's3',
                        aws_access_key_id=config.s3_access_key,
                        aws_secret_access_key=config.s3_secret_key,
                        region_name=config.s3_region
                    )
                    logger.info("S3 client initialized successfully")
            except Exception as e:
                logger.error("Failed to initialize S3 client: %s", e)
                
        self.cache_enabled = hasattr(config, 'file_cache_enabled') and config.file_cache_enabled
        self.cache_dir = pathlib.Path("./cache")
        self.cache_max_age = 3600
        self.cache = {}
        self.cache_lock = threading.Lock()
        
        if self.cache_enabled:
            self.cache_dir.mkdir(exist_ok=True)
            logger.info("File caching enabled")

    def normalize_path(self, requested_path: str) -> pathlib.Path:
        if not requested_path:
            raise ValueError("Empty path not allowed")
        requested = pathlib.Path(os.path.expanduser(requested_path)).resolve()
        
        for allowed in self.allowed_directories:
            if str(requested).startswith(allowed):
                return requested
        raise ValueError(f"Access denied: {requested} is outside allowed directories.")

    def _cache_key(self, path: str, storage: str = "local", bucket: Optional[str] = None) -> str:
        key_parts = [storage, path]
        if bucket:
            key_parts.append(bucket)
        key_string = ":".join(key_parts)
        return hashlib.md5(key_string.encode()).hexdigest()

    def read_file(self, path: str, storage: str = "local", bucket: Optional[str] = None) -> str:
        if storage == "local":
            file_path = self.normalize_path(path)
            return file_path.read_text(encoding="utf-8")
        elif storage == "s3":
            if not bucket:
                raise ValueError("S3 bucket name is required for S3 storage")
            if not self.s3_client:
                raise ValueError("S3 client not configured")
            response = self.s3_client.get_object(Bucket=bucket, Key=path)
            return response['Body'].read().decode('utf-8')
        else:
            raise ValueError(f"Unsupported storage type: {storage}")

    def read_file_cached(self, path: str, max_age: Optional[int] = None,
                        storage: str = "local", bucket: Optional[str] = None) -> str:
        if not self.cache_enabled:
            return self.read_file(path, storage, bucket)
        cache_key = self._cache_key(path, storage, bucket)
        cache_max_age = self.cache_max_age if max_age is None else max_age
        
        with self.cache_lock:
            if cache_key in self.cache:
                entry = self.cache[cache_key]
                if time.time() - entry["timestamp"] < cache_max_age:
                    logger.debug("Cache hit for %s", path)
                    return entry["content"]
        
        content = self.read_file(path, storage, bucket)
        
        with self.cache_lock:
            self.cache[cache_key] = {
                "content": content,
                "timestamp": time.time()
            }
            cache_file = self.cache_dir / cache_key
            try:
                with open(cache_file, 'w', encoding='utf-8') as f:
                    f.write(content)
            except Exception as e:
                logger.warning("Failed to write to disk cache: %s", e)
        return content

    def read_file_binary(self, path: str, storage: str = "local", bucket: Optional[str] = None) -> bytes:
        if storage == "local":
            file_path = self.normalize_path(path)
            if not file_path.exists():
                raise ValueError(f"File not found: {path}")
            if file_path.is_dir():
                raise ValueError(f"Path is a directory, not a file: {path}")
            return file_path.read_bytes()
        elif storage == "s3":
            if not bucket:
                raise ValueError("S3 bucket name is required for S3 storage")
            if not self.s3_client:
                raise ValueError("S3 client not configured")
            response = self.s3_client.get_object(Bucket=bucket, Key=path)
            return response['Body'].read()
        else:
            raise ValueError(f"Unsupported storage type: {storage}")

    def write_file(self, path: str, content: str, storage: str = "local", bucket: Optional[str] = None) -> str:
        if storage == "local":
            file_path = self.normalize_path(path)
            file_path.parent.mkdir(parents=True, exist_ok=True)
            file_path.write_text(content, encoding="utf-8")
            return f"Successfully wrote to {path}"
        elif storage == "s3":
            if not bucket:
                raise ValueError("S3 bucket name is required for S3 storage")
            if not self.s3_client:
                raise ValueError("S3 client not configured")
            self.s3_client.put_object(
                Bucket=bucket,
                Key=path,
                Body=content.encode('utf-8'),
                ContentType='text/plain'
            )
            return f"Successfully wrote to s3://{bucket}/{path}"
        else:
            raise ValueError(f"Unsupported storage type: {storage}")

    def write_file_binary(self, path: str, content: bytes,
                         storage: str = "local", bucket: Optional[str] = None) -> str:
        if storage == "local":
            file_path = self.normalize_path(path)
            file_path.parent.mkdir(parents=True, exist_ok=True)
            with open(file_path, "wb") as f:
                f.write(content)
            return f"Successfully wrote to {path}"
        elif storage == "s3":
            if not bucket:
                raise ValueError("S3 bucket name is required for S3 storage")
            if not self.s3_client:
                raise ValueError("S3 client not configured")
            self.s3_client.put_object(Bucket=bucket, Key=path, Body=content)
            return f"Successfully wrote to s3://{bucket}/{path}"
        else:
            raise ValueError(f"Unsupported storage type: {storage}")

    def create_directory(self, path: str, storage: str = "local", bucket: Optional[str] = None) -> str:
        if storage == "local":
            dir_path = self.normalize_path(path)
            dir_path.mkdir(parents=True, exist_ok=True)
            return f"Successfully created directory {path}"
        elif storage == "s3":
            if not bucket:
                raise ValueError("S3 bucket name is required for S3 storage")
            if not self.s3_client:
                raise ValueError("S3 client not configured")
            # S3 doesn't need explicit directory creation, but we'll add an empty marker
            self.s3_client.put_object(Bucket=bucket, Key=f"{path}/", Body=b'')
            return f"Successfully created directory s3://{bucket}/{path}/"
        else:
            raise ValueError(f"Unsupported storage type: {storage}")

    def delete_file(self, path: str, storage: str = "local", bucket: Optional[str] = None) -> str:
        if storage == "local":
            file_path = self.normalize_path(path)
            if file_path.exists():
                if file_path.is_file():
                    file_path.unlink()
                else:
                    shutil.rmtree(file_path)
                return f"Successfully deleted {path}"
            else:
                return f"File or directory does not exist: {path}"
        elif storage == "s3":
            if not bucket:
                raise ValueError("S3 bucket name is required for S3 storage")
            if not self.s3_client:
                raise ValueError("S3 client not configured")
            self.s3_client.delete_object(Bucket=bucket, Key=path)
            return f"Successfully deleted s3://{bucket}/{path}"
        else:
            raise ValueError(f"Unsupported storage type: {storage}")

    def search_files(self, directory: str, pattern: str, storage: str = "local", bucket: Optional[str] = None) -> List[str]:
        if storage == "local":
            dir_path = self.normalize_path(directory)
            if not dir_path.exists() or not dir_path.is_dir():
                raise ValueError(f"Directory does not exist: {directory}")
            
            matches = []
            for root, _, _ in os.walk(dir_path):
                for file in glob.glob(os.path.join(root, pattern)):
                    matches.append(file)
            return matches
        elif storage == "s3":
            if not bucket:
                raise ValueError("S3 bucket name is required for S3 storage")
            if not self.s3_client:
                raise ValueError("S3 client not configured")
            
            # Convert glob pattern to a prefix for S3
            prefix = directory.rstrip('/') + '/' if directory else ''
            response = self.s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)
            
            matches = []
            if 'Contents' in response:
                for obj in response['Contents']:
                    key = obj['Key']
                    if fnmatch.fnmatch(os.path.basename(key), pattern):
                        matches.append(key)
            return matches
        else:
            raise ValueError(f"Unsupported storage type: {storage}")

    def list_directory(self, directory: str, storage: str = "local", bucket: Optional[str] = None) -> Dict[str, Any]:
        if storage == "local":
            dir_path = self.normalize_path(directory)
            if not dir_path.exists():
                raise ValueError(f"Directory does not exist: {directory}")
            if not dir_path.is_dir():
                raise ValueError(f"Path is not a directory: {directory}")
            
            items = []
            for item in dir_path.iterdir():
                item_type = "directory" if item.is_dir() else "file"
                size = 0
                if item.is_file():
                    size = item.stat().st_size
                items.append({
                    "name": item.name,
                    "type": item_type,
                    "size": size,
                    "modified": item.stat().st_mtime
                })
            
            return {
                "path": str(dir_path),
                "items": items
            }
        elif storage == "s3":
            if not bucket:
                raise ValueError("S3 bucket name is required for S3 storage")
            if not self.s3_client:
                raise ValueError("S3 client not configured")
            
            prefix = directory.rstrip('/') + '/' if directory else ''
            response = self.s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix, Delimiter='/')
            
            items = []
            
            # Add directories (CommonPrefixes)
            if 'CommonPrefixes' in response:
                for prefix_obj in response['CommonPrefixes']:
                    prefix_name = prefix_obj['Prefix']
                    name = os.path.basename(prefix_name.rstrip('/'))
                    items.append({
                        "name": name,
                        "type": "directory",
                        "size": 0,
                        "modified": 0
                    })
            
            # Add files (Contents)
            if 'Contents' in response:
                for obj in response['Contents']:
                    key = obj['Key']
                    # Skip the directory itself or empty directory markers
                    if key == prefix or key.endswith('/'):
                        continue
                    name = os.path.basename(key)
                    items.append({
                        "name": name,
                        "type": "file",
                        "size": obj['Size'],
                        "modified": obj['LastModified'].timestamp() if hasattr(obj['LastModified'], 'timestamp') else 0
                    })
            
            return {
                "path": f"s3://{bucket}/{directory}",
                "items": items
            }
        else:
            raise ValueError(f"Unsupported storage type: {storage}")

    def file_exists(self, path: str, storage: str = "local", bucket: Optional[str] = None) -> bool:
        if storage == "local":
            file_path = self.normalize_path(path)
            return file_path.exists()
        elif storage == "s3":
            if not bucket:
                raise ValueError("S3 bucket name is required for S3 storage")
            if not self.s3_client:
                raise ValueError("S3 client not configured")
            
            try:
                self.s3_client.head_object(Bucket=bucket, Key=path)
                return True
            except Exception:
                # Check if it might be a directory by looking for objects with this prefix
                response = self.s3_client.list_objects_v2(
                    Bucket=bucket,
                    Prefix=path.rstrip('/') + '/',
                    MaxKeys=1
                )
                return 'Contents' in response and len(response['Contents']) > 0
        else:
            raise ValueError(f"Unsupported storage type: {storage}")

    def invalidate_cache(self, path: Optional[str] = None,
                        storage: str = "local", bucket: Optional[str] = None):
        if not self.cache_enabled:
            return
        with self.cache_lock:
            if path is not None:
                cache_key = self._cache_key(path, storage, bucket)
                if cache_key in self.cache:
                    del self.cache[cache_key]
                cache_file = self.cache_dir / cache_key
                if cache_file.exists():
                    cache_file.unlink()
            else:
                self.cache.clear()
                for cache_file in self.cache_dir.glob("*"):
                    try:
                        cache_file.unlink()
                    except Exception as e:
                        logger.warning("Failed to delete cache file %s: %s", cache_file, e)


================================================================================
FILE: app\core\git_service.py
LANGUAGE: python
SIZE: 13447 bytes
================================================================================

import os
import logging
import threading
from typing import Dict, List, Any, Optional

# Try to import git module at top level
try:
    import git
    HAS_GIT = True
except ImportError:
    git = None
    HAS_GIT = False

logger = logging.getLogger(__name__)


class GitService:
    """Service for Git operations with thread-safe repository access"""

    def __init__(
        self,
        default_author_name: str = "OtherTales",
        default_author_email: str = "system@othertales.com",
    ):
        """Initialize the Git service"""
        self.repo_locks = {}
        self.default_author_name = default_author_name
        self.default_author_email = default_author_email

        if not HAS_GIT:
            logger.error("GitPython is not installed. Git functionality will be limited.")
        self.git = git

    def _get_repo_lock(self, repo_path: str):
        """Get or create a lock for a specific repository path"""
        if repo_path not in self.repo_locks:
            self.repo_locks[repo_path] = threading.Lock()
        return self.repo_locks[repo_path]

    def _get_repo(self, repo_path: str):
        """Get Git repository, creating it if it doesn't exist"""
        if not self.git:
            raise ValueError("GitPython is not installed")
        if not os.path.exists(repo_path):
            os.makedirs(repo_path, exist_ok=True)
        try:
            repo = self.git.Repo(repo_path)
            return repo
        except self.git.InvalidGitRepositoryError:
            repo = self.git.Repo.init(repo_path)
            with repo.config_writer() as config:
                config.set_value("user", "name", self.default_author_name)
                config.set_value("user", "email", self.default_author_email)

            if not os.path.exists(os.path.join(repo_path, ".gitignore")):
                with open(os.path.join(repo_path, ".gitignore"), "w", encoding="utf-8") as f:
                    f.write("*.swp\n*.bak\n*.tmp\n*.orig\n*~\n")
                repo.git.add(".gitignore")
                repo.git.commit("-m", "Initial commit: Add .gitignore")
            return repo
        except Exception as e:
            logger.error("Error initializing repository: %s", e)
            raise ValueError(f"Failed to initialize repository: {e}") from e

    def get_status(self, repo_path: str) -> Dict[str, Any]:
        """Get the status of the git repository."""
        try:
            repo = self._get_repo(repo_path)
            
            status = {
                "branch": repo.active_branch.name,  # This field is needed for tests
                "current_branch": repo.active_branch.name,
                "clean": not repo.is_dirty(),
                "staged_files": [],
                "unstaged_files": [],
                "untracked_files": repo.untracked_files,
                "untracked": repo.untracked_files  # This field is needed for tests
            }
            
            # Get staged and unstaged changes
            for item in repo.index.diff(None):
                status["unstaged_files"].append(item.a_path)
                
            for item in repo.index.diff("HEAD"):
                status["staged_files"].append(item.a_path)
                
            return status
        except Exception as e:
            logger.error("Error getting git status: %s", e)
            raise

    def get_diff(
        self, repo_path: str, file_path: Optional[str] = None, target: Optional[str] = None
    ) -> str:
        """Get diff of changes"""
        repo = self._get_repo(repo_path)
        try:
            if file_path and target:
                return repo.git.diff(target, file_path)
            elif file_path:
                return repo.git.diff("HEAD", file_path)
            elif target:
                return repo.git.diff(target)
            else:
                return repo.git.diff()
        except Exception as e:
            logger.error("Error getting diff: %s", e)
            raise ValueError(f"Failed to get diff: {e}") from e

    def add_files(self, repo_path: str, files: List[str]) -> str:
        """Stage files for commit"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            try:
                repo.git.add(files)
                return "Files staged successfully"
            except Exception as e:
                logger.error("Error adding files: %s", e)
                raise ValueError(f"Failed to add files: {e}") from e

    def commit_changes(
        self,
        repo_path: str,
        message: str,
        author_name: Optional[str] = None,
        author_email: Optional[str] = None,
    ) -> str:
        """Commit staged changes"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            author_name = author_name or self.default_author_name
            author_email = author_email or self.default_author_email

            try:
                # Set author for this commit
                with repo.config_writer() as config:
                    config.set_value("user", "name", author_name)
                    config.set_value("user", "email", author_email)

                # Commit changes
                commit = repo.index.commit(message)
                return f"Committed changes with hash {commit.hexsha}"
            except Exception as e:
                logger.error("Error committing changes: %s", e)
                raise ValueError(f"Failed to commit changes: {e}") from e

    def reset_changes(self, repo_path: str) -> str:
        """Reset staged changes"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            try:
                repo.git.reset()
                return "All staged changes reset"
            except Exception as e:
                logger.error("Error resetting changes: %s", e)
                raise ValueError("Failed to reset changes: {}".format(e)) from e

    def get_log(
        self, repo_path: str, max_count: int = 10, file_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """Get commit log"""
        repo = self._get_repo(repo_path)
        try:
            commits_data = []
            if file_path:
                # Get log for specific file
                commits = list(repo.iter_commits(paths=file_path, max_count=max_count))
            else:
                # Get log for entire repo
                commits = list(repo.iter_commits(max_count=max_count))

            for commit in commits:
                commits_data.append(
                    {
                        "hash": commit.hexsha,
                        "author": f"{commit.author.name} <{commit.author.email}>",
                        "date": commit.committed_datetime.strftime("%Y-%m-%d %H:%M:%S %z"),
                        "message": commit.message.strip(),
                    }
                )

            return {"commits": commits_data}
        except Exception as e:
            logger.error("Error getting log: %s", e)
            raise ValueError(f"Failed to get log: {e}") from e

    def create_branch(
        self, repo_path: str, branch_name: str, base_branch: Optional[str] = None
    ) -> str:
        """Create a new branch"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            try:
                # Use the specified base branch or current branch
                if base_branch:
                    # Check if base branch exists
                    if base_branch not in repo.refs:
                        raise ValueError(f"Base branch '{base_branch}' does not exist")
                    base = repo.refs[base_branch]
                else:
                    base = repo.active_branch

                # Create new branch
                repo.create_head(branch_name, base)
                return f"Created branch '{branch_name}'"
            except Exception as e:
                logger.error("Error creating branch: %s", e)
                raise ValueError(f"Failed to create branch: {e}") from e

    def checkout_branch(self, repo_path: str, branch_name: str, create: bool = False) -> str:
        """Checkout a branch"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            try:
                # Check if branch exists
                branch_exists = branch_name in repo.refs

                # Create branch if it doesn't exist
                if create and not branch_exists:
                    repo.create_head(branch_name)
                elif not branch_exists:
                    raise ValueError(f"Branch '{branch_name}' does not exist")

                # Checkout the branch
                repo.git.checkout(branch_name)
                return f"Switched to branch '{branch_name}'"
            except Exception as e:
                logger.error("Error checking out branch: %s", e)
                raise ValueError(f"Failed to checkout branch: {e}") from e

    def clone_repo(self, repo_url: str, local_path: str, auth_token: Optional[str] = None) -> str:
        """Clone a Git repository"""
        if not self.git:
            raise ValueError("GitPython is not installed")

        try:
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(os.path.abspath(local_path)), exist_ok=True)

            # If auth token is provided, modify the URL
            if auth_token:
                if repo_url.startswith("https://"):
                    parsed_url = repo_url.replace(
                        "https://", f"https://x-access-token:{auth_token}@"
                    )
                    self.git.Repo.clone_from(parsed_url, local_path)
                else:
                    # For SSH or other protocols, use standard clone
                    self.git.Repo.clone_from(repo_url, local_path)
            else:
                self.git.Repo.clone_from(repo_url, local_path)

            return f"Successfully cloned repository to '{local_path}'"
        except Exception as e:
            logger.error("Error cloning repository: %s", e)
            raise ValueError(f"Failed to clone repository: {e}") from e

    def get_file_content_at_version(self, repo_path: str, file_path: str, version: str) -> str:
        """Get file content at a specific Git version"""
        repo = self._get_repo(repo_path)
        try:
            return repo.git.show(f"{version}:{file_path}")
        except Exception as e:
            logger.error("Error getting file content at version: %s", e)
            raise ValueError(f"Failed to get file content at version {version}: {e}") from e

    def get_diff_between_versions(
        self, repo_path: str, file_path: str, from_version: str, to_version: str = "HEAD"
    ) -> str:
        """Get the differences between two versions of a file"""
        repo = self._get_repo(repo_path)
        try:
            return repo.git.diff(from_version, to_version, "--", file_path)
        except Exception as e:
            logger.error("Error getting diff between versions: %s", e)
            raise ValueError(f"Failed to get diff between versions: {e}") from e

    def remove_file(self, repo_path: str, file_path: str) -> str:
        """Remove a file from the repository"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            try:
                # Check if file exists
                full_path = os.path.join(repo_path, file_path)
                if not os.path.exists(full_path):
                    return f"File does not exist: {file_path}"
                
                # Remove from git index
                repo.index.remove([file_path])
                
                # Optionally commit the removal
                # repo.index.commit(f"Removed {file_path}")
                
                return f"Successfully removed {file_path} from index"
            except Exception as e:
                logger.error("Error removing file: %s", e)
                raise ValueError(f"Failed to remove file: {e}") from e
                
    def batch_commit(
        self, repo_path: str, file_groups: List[List[str]], message_template: str
    ) -> List[str]:
        """Commit files in batches for better performance"""
        with self._get_repo_lock(repo_path):
            repo = self._get_repo(repo_path)
            try:
                commit_hashes = []
                for i, file_group in enumerate(file_groups):
                    # Add files in this group
                    repo.git.add(file_group)
                    
                    # Commit with a templated message
                    commit_message = f"{message_template} (batch {i+1}/{len(file_groups)})"
                    commit = repo.index.commit(commit_message)
                    commit_hashes.append(commit.hexsha)
                
                return commit_hashes
            except Exception as e:
                logger.error("Error in batch commit: %s", e)
                raise ValueError(f"Failed to perform batch commit: {e}") from e


================================================================================
FILE: app\core\memory_service.py
LANGUAGE: python
SIZE: 34647 bytes
================================================================================

import json
import os
import threading
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
import logging
import difflib

import networkx as nx

from app.models.memory import Entity, Relation, KnowledgeGraph
from app.utils.config import get_config

logger = logging.getLogger(__name__)

class MemoryService:
    def __init__(self, memory_file_path: Optional[str] = None):
        config = get_config()
        
        memory_file_path = memory_file_path or config.memory_file_path
        self.memory_file_path = Path(
            memory_file_path
            if Path(memory_file_path).is_absolute()
            else Path(os.getcwd()) / memory_file_path
        )
        
        self.memory_file_path.parent.mkdir(parents=True, exist_ok=True)
        
        self.user_prefs_dir = self.memory_file_path.parent / "user_preferences"
        self.user_prefs_dir.mkdir(exist_ok=True)
        
        self.use_graph_db = config.use_graph_db
        if self.use_graph_db:
            try:
                self.graph = nx.DiGraph()
                self._load_graph_from_file()
            except ImportError:
                logger.warning("NetworkX not installed. Graph database functionality will be limited.")
                self.use_graph_db = False

        self.entities = {}
        self.relations = []
        self.lock = threading.Lock()
        
        # Load existing data if available
        self._load_memory()

    def _read_graph_file(self) -> KnowledgeGraph:
        """Read the knowledge graph from disk"""
        if not self.memory_file_path.exists():
            return KnowledgeGraph(entities=[], relations=[])
        try:
            with open(self.memory_file_path, "r", encoding="utf-8") as f:
                lines = [line for line in f if line.strip()]
                entities = []
                relations = []
                for line in lines:
                    item = json.loads(line)
                    if item.get("type") == "entity":
                        entities.append(Entity(
                            name=item["name"],
                            entity_type=item["entity_type"],
                            observations=item.get("observations", [])
                        ))
                    elif item.get("type") == "relation":
                        relations.append(Relation(
                            **{k: v for k, v in item.items() if k != "type"}
                        ))
                return KnowledgeGraph(entities=entities, relations=relations)
        except Exception as e:
            print(f"Error reading graph file: {e}")
            return KnowledgeGraph(entities=[], relations=[])

    def _save_graph(self, graph: KnowledgeGraph):
        """Save the knowledge graph to disk"""
        lines = []
        # Save entities
        for e in graph.entities:
            entity_dict = e.dict()
            entity_dict["type"] = "entity"
            lines.append(json.dumps(entity_dict))
        # Save relations
        for r in graph.relations:
            relation_dict = r.dict(by_alias=True)
            relation_dict["type"] = "relation"
            lines.append(json.dumps(relation_dict))
        # Write to file
        with open(self.memory_file_path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))

    def create_entities(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Create new entities in the graph"""
        # Process entities through the legacy file-based system
        graph = self._read_graph_file()
        existing_names = {e.name for e in graph.entities}
        # Convert input dictionaries to Entity objects
        entity_objects = []
        for entity_dict in entities:
            if isinstance(entity_dict, dict):
                entity_objects.append(Entity(**entity_dict))
            else:
                entity_objects.append(entity_dict)
        # Filter out existing entities
        new_entities = [e for e in entity_objects if e.name not in existing_names]
        # Add new entities to graph
        graph.entities.extend(new_entities)
        self._save_graph(graph)
        
        # Also update the in-memory data
        with self.lock:
            for entity in entities:
                entity_name = entity.get('name')
                if not entity_name:
                    continue
                    
                entity_type = entity.get('entity_type', 'unknown')
                properties = entity.get('properties', {})
                
                # Add entity to memory
                self.entities[entity_name] = {
                    'entity_type': entity_type,
                    'properties': properties,
                    'created_at': time.time()
                }
                
                # Add to graph if using graph DB
                if self.use_graph_db:
                    self.graph.add_node(entity_name, 
                                      entity_type=entity_type, 
                                      **properties)
            
            # Save changes
            self._save_memory()
        
        # Return the added entities
        return [e.dict() for e in new_entities]

    def create_relations(self, relations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Create new relations in the graph"""
        # Process relations through the legacy file-based system
        graph = self._read_graph_file()
        # Get existing relations for deduplication
        existing_relations = {(r.from_, r.to, r.relation_type) for r in graph.relations}
        # Convert input dictionaries to Relation objects
        relation_objects = []
        for relation_dict in relations:
            if isinstance(relation_dict, dict):
                # Handle inconsistent field naming in input
                if "relation_type" in relation_dict and "relationType" not in relation_dict:
                    relation_dict["relationType"] = relation_dict["relation_type"]
                if "from_" in relation_dict and "from" not in relation_dict:
                    relation_dict["from"] = relation_dict["from_"]
                relation_objects.append(Relation(**relation_dict))
            else:
                relation_objects.append(relation_dict)
        # Filter out existing relations
        new_relations = [r for r in relation_objects
                        if (r.from_, r.to, r.relation_type) not in existing_relations]
        # Add new relations to graph
        graph.relations.extend(new_relations)
        self._save_graph(graph)
        
        # Also update the in-memory data
        created_relations = []
        
        with self.lock:
            for relation in relations:
                from_entity = relation.get('from')
                to_entity = relation.get('to')
                relation_type = relation.get('relation_type', 'related_to')
                properties = relation.get('properties', {})
                
                # Check if entities exist
                if from_entity not in self.entities or to_entity not in self.entities:
                    logger.warning(f"Cannot create relation: one or both entities don't exist ({from_entity}, {to_entity})")
                    continue
                
                # Create relation object
                relation_obj = {
                    'from': from_entity,
                    'to': to_entity,
                    'relation_type': relation_type,
                    'properties': properties,
                    'created_at': time.time()
                }
                
                # Add to relations list
                self.relations.append(relation_obj)
                
                # Add to graph if using graph DB
                if self.use_graph_db:
                    self.graph.add_edge(from_entity, to_entity, 
                                      relation_type=relation_type, 
                                      **properties)
                
                # Add to created list - use from_ to match test expectations
                created_relations.append({
                    'from_': from_entity,
                    'to': to_entity,
                    'relation_type': relation_type,
                    'properties': properties
                })
            
            # Save changes
            self._save_memory()
        
        # Return the results in the expected format for tests
        if not created_relations and new_relations:
            # If we have file-based results but no in-memory results, convert them
            return [r.dict(by_alias=True) for r in new_relations]
        return created_relations

    def add_observations(self, observations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Add observations to entities"""
        graph = self._read_graph_file()
        results = []
        # Process each observation item
        for obs_item in observations:
            entity_name = obs_item["entity_name"]
            contents = obs_item["contents"]
            # Find the entity
            entity = next((e for e in graph.entities if e.name == entity_name), None)
            if entity:
                # Get observations that are not already in the entity
                new_observations = [c for c in contents if c not in entity.observations]
                # Add new observations
                entity.observations.extend(new_observations)
                # Record result
                results.append({
                    "entity_name": entity_name,
                    "added_observations": new_observations
                })
        # Save the updated graph
        self._save_graph(graph)
        return results

    def delete_entities(self, entity_names: List[str]) -> Dict[str, int]:
        """Delete entities and their relations"""
        # Handle file-based graph
        graph = self._read_graph_file()
        # Remove entities
        initial_count = len(graph.entities)
        graph.entities = [e for e in graph.entities if e.name not in entity_names]
        file_entities_removed = initial_count - len(graph.entities)
        # Remove relations involving the deleted entities
        initial_relations_count = len(graph.relations)
        graph.relations = [r for r in graph.relations
                         if r.from_ not in entity_names and r.to not in entity_names]
        file_relations_removed = initial_relations_count - len(graph.relations)
        # Save the updated graph
        self._save_graph(graph)
        
        # Also handle in-memory data
        entities_removed = 0
        relations_removed = 0
        
        with self.lock:
            for name in entity_names:
                # Remove entity if it exists
                if name in self.entities:
                    del self.entities[name]
                    entities_removed += 1
                    
                    # Remove relations involving this entity
                    new_relations = []
                    for relation in self.relations:
                        if relation['from'] == name or relation['to'] == name:
                            relations_removed += 1
                        else:
                            new_relations.append(relation)
                    self.relations = new_relations
                    
                    # Remove from graph if using graph DB
                    if self.use_graph_db and name in self.graph:
                        self.graph.remove_node(name)
            
            # Save changes
            self._save_memory()
        
        # Return the results (prefer in-memory counts if available)
        return {
            "entities_removed": entities_removed or file_entities_removed,
            "relations_removed": relations_removed or file_relations_removed
        }

    def delete_relations(self, relations: List[Dict[str, Any]]) -> Dict[str, int]:
        """Delete specific relations"""
        # Handle file-based graph
        graph = self._read_graph_file()
        # Convert input dictionaries to relation tuples for comparison
        relation_tuples = []
        for relation in relations:
            from_entity = relation.get("from", relation.get("from_"))
            to_entity = relation.get("to")
            relation_type = relation.get("relation_type", relation.get("relationType"))
            if from_entity and to_entity and relation_type:
                relation_tuples.append((from_entity, to_entity, relation_type))
        # Remove matching relations
        initial_count = len(graph.relations)
        graph.relations = [r for r in graph.relations
                         if (r.from_, r.to, r.relation_type) not in relation_tuples]
        file_relations_removed = initial_count - len(graph.relations)
        # Save the updated graph
        self._save_graph(graph)
        
        # Also handle in-memory data
        relations_removed = 0
        
        with self.lock:
            for rel_to_delete in relations:
                from_entity = rel_to_delete.get('from')
                to_entity = rel_to_delete.get('to')
                relation_type = rel_to_delete.get('relation_type')
                
                # Filter out matching relations
                new_relations = []
                for existing_rel in self.relations:
                    if (existing_rel['from'] == from_entity and 
                        existing_rel['to'] == to_entity and 
                        existing_rel['relation_type'] == relation_type):
                        relations_removed += 1
                        
                        # Remove from graph if using graph DB
                        if self.use_graph_db:
                            # Check if edge exists before removing
                            if self.graph.has_edge(from_entity, to_entity):
                                self.graph.remove_edge(from_entity, to_entity)
                    else:
                        new_relations.append(existing_rel)
                
                self.relations = new_relations
            
            # Save changes
            self._save_memory()
        
        # Return the results (prefer in-memory count if available)
        return {
            "relations_removed": relations_removed or file_relations_removed
        }

    def search_nodes(self, query: str) -> KnowledgeGraph:
        """Search for nodes matching the query"""
        graph = self._read_graph_file()
        # Convert query to lowercase for case-insensitive search
        query_lower = query.lower()
        # Find entities matching the query
        matching_entities = []
        for entity in graph.entities:
            # Check name
            if query_lower in entity.name.lower():
                matching_entities.append(entity)
                continue
            # Check type
            if query_lower in entity.entity_type.lower():
                matching_entities.append(entity)
                continue
            # Check observations
            if any(query_lower in observation.lower() for observation in entity.observations):
                matching_entities.append(entity)
                continue
        # Get names of matching entities
        matching_names = {entity.name for entity in matching_entities}
        # Find relations between matching entities
        matching_relations = [r for r in graph.relations
                            if r.from_ in matching_names and r.to in matching_names]
        return KnowledgeGraph(entities=matching_entities, relations=matching_relations)

    def open_nodes(self, names: List[str]) -> KnowledgeGraph:
        """Retrieve specific nodes by name"""
        graph = self._read_graph_file()
        # Find the specified entities
        entities = [e for e in graph.entities if e.name in names]
        # Get entity names
        entity_names = {e.name for e in entities}
        # Find relations between these entities
        relations = [r for r in graph.relations
                   if r.from_ in entity_names and r.to in entity_names]
        return KnowledgeGraph(entities=entities, relations=relations)

    def get_user_preference(self, user_id: str) -> Dict[str, Any]:
        """Retrieve user preferences"""
        try:
            pref_file = self.user_prefs_dir / f"{user_id}.json"
            if not pref_file.exists():
                logger.debug("No preferences found for user %s", user_id)
                return {}
            with open(pref_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error("Error reading user preferences for %s: %s", user_id, e, exc_info=True)
            return {}

    def set_user_preference(self, user_id: str, preferences: Dict[str, Any]) -> Dict[str, Any]:
        """Store user preferences"""
        try:
            # Validate user_id to prevent path traversal
            if not user_id or '/' in user_id or '\\' in user_id or '..' in user_id:
                logger.error("Invalid user ID: %s", user_id)
                raise ValueError("Invalid user ID")
            pref_file = self.user_prefs_dir / f"{user_id}.json"
            # Merge with existing preferences if present
            existing_prefs = {}
            if pref_file.exists():
                try:
                    with open(pref_file, "r", encoding="utf-8") as f:
                        existing_prefs = json.load(f)
                except Exception as e:
                    logger.warning("Could not read existing preferences for %s: %s", user_id, e)
            # Update with new preferences
            existing_prefs.update(preferences)
            # Write back to file
            with open(pref_file, "w", encoding="utf-8") as f:
                json.dump(existing_prefs, f, ensure_ascii=False, indent=2)
            logger.info("Updated preferences for user %s", user_id)
            return existing_prefs
        except Exception as e:
            logger.error("Failed to set preferences for %s: %s", user_id, e, exc_info=True)
            raise

    def get_full_graph(self) -> KnowledgeGraph:
        """Get the entire knowledge graph"""
        return self._read_graph_file()

    def find_similar_entities(self, entity_name: str, threshold: float = 0.8) -> List[str]:
        """Find entities with similar names"""
        graph = self._read_graph_file()
        similar = []
        # Use difflib for fuzzy matching
        for entity in graph.entities:
            similarity = difflib.SequenceMatcher(None, entity_name.lower(), entity.name.lower()).ratio()
            if similarity >= threshold and entity_name != entity.name:
                similar.append(entity.name)
        return similar

    def _load_graph_from_file(self):
        """Load graph data from file into networkx graph"""
        try:
            knowledge_graph = self._read_graph_file()
            # Clear existing graph
            self.graph.clear()
            # Add all entities as nodes
            for entity in knowledge_graph.entities:
                self.graph.add_node(
                    entity.name,
                    entity_type=entity.entity_type,
                    observations=entity.observations
                )
            # Add all relations as edges
            for relation in knowledge_graph.relations:
                self.graph.add_edge(
                    relation.from_,
                    relation.to,
                    relation_type=relation.relation_type
                )
            logger.info("Loaded %d entities and %d relations into graph", 
                       len(knowledge_graph.entities), len(knowledge_graph.relations))
            return True
        except Exception as e:
            logger.error("Error loading graph from file: %s", e, exc_info=True)
            return False

    def find_paths(self, start_entity: str, end_entity: str, max_length: int = 3) -> List[List[Dict[str, Any]]]:
        """Find paths between two entities in the graph"""
        if not self.use_graph_db:
            raise ValueError("Graph database not enabled")
        
        try:
            # Check if entities exist
            if start_entity not in self.graph.nodes:
                raise ValueError(f"Entity '{start_entity}' not found in graph")
            if end_entity not in self.graph.nodes:
                raise ValueError(f"Entity '{end_entity}' not found in graph")
            
            # Find all simple paths up to max_length
            paths = list(nx.all_simple_paths(self.graph, start_entity, end_entity, cutoff=max_length))
            
            # Format results
            result_paths = []
            for path in paths:
                path_info = []
                # Add nodes and edges to path
                for i, node in enumerate(path):
                    # Add node
                    node_data = self.graph.nodes[node]
                    path_info.append({
                        "type": "entity",
                        "name": node,
                        "entity_type": node_data.get("entity_type", "unknown"),
                    })
                    # Add edge if not last node
                    if i < len(path) - 1:
                        next_node = path[i+1]
                        edge_data = self.graph.get_edge_data(node, next_node)
                        path_info.append({
                            "type": "relation",
                            "from": node,
                            "to": next_node,
                            "relation_type": edge_data.get("relation_type", "unknown"),
                        })
                result_paths.append(path_info)
            return result_paths
        except ValueError:
            # Re-raise ValueError for specific error handling
            raise
        except Exception as e:
            logger.error("Error finding paths: %s", e, exc_info=True)
            return []

    def get_similar_entities(self, entity_name: str, threshold: float = 0.6) -> List[Dict[str, Any]]:
        """Find entities with similar names"""
        try:
            if self.use_graph_db:
                # If graph DB is enabled, use graph nodes
                all_entities = list(self.graph.nodes)
            else:
                # Otherwise use entities from knowledge graph
                knowledge_graph = self._read_graph_file()
                all_entities = [entity.name for entity in knowledge_graph.entities]
            
            # Calculate similarity scores
            similarities = []
            for name in all_entities:
                score = difflib.SequenceMatcher(None, entity_name.lower(), name.lower()).ratio()
                if score >= threshold:
                    similarities.append({
                        "name": name,
                        "similarity": score
                    })
            
            # Sort by similarity (highest first)
            similarities.sort(key=lambda x: x["similarity"], reverse=True)
            return similarities
        except Exception as exc:
            logger.error("Error finding similar entities: %s", exc, exc_info=True)
            return []

    def get_entity_connections(self, entity_name: str) -> Dict[str, Any]:
        """Get all connections for a specific entity"""
        if not self.use_graph_db:
            # Use an in-memory approach since graph DB is disabled
            incoming = []
            outgoing = []
            
            with self.lock:
                # Find entity
                if entity_name not in self.entities:
                    return {'incoming': [], 'outgoing': []}
                
                # Find relations
                for relation in self.relations:
                    if relation['from'] == entity_name:
                        outgoing.append({
                            'entity': relation['to'],
                            'relation_type': relation['relation_type'],
                            'properties': relation['properties']
                        })
                    elif relation['to'] == entity_name:
                        incoming.append({
                            'entity': relation['from'],
                            'relation_type': relation['relation_type'],
                            'properties': relation['properties']
                        })
            
            return {
                'entity': entity_name,
                'incoming': incoming,
                'outgoing': outgoing
            }
        else:
            # Use the graph database for faster querying
            with self.lock:
                if entity_name not in self.graph:
                    return {'incoming': [], 'outgoing': []}
                
                # Get incoming edges
                incoming = []
                for pred in self.graph.predecessors(entity_name):
                    edge_data = self.graph.get_edge_data(pred, entity_name)
                    incoming.append({
                        'entity': pred,
                        'relation_type': edge_data.get('relation_type', 'related_to'),
                        'properties': {k: v for k, v in edge_data.items() if k != 'relation_type'}
                    })
                
                # Get outgoing edges
                outgoing = []
                for succ in self.graph.successors(entity_name):
                    edge_data = self.graph.get_edge_data(entity_name, succ)
                    outgoing.append({
                        'entity': succ,
                        'relation_type': edge_data.get('relation_type', 'related_to'),
                        'properties': {k: v for k, v in edge_data.items() if k != 'relation_type'}
                    })
                
                return {
                    'entity': entity_name,
                    'incoming': incoming,
                    'outgoing': outgoing
                }

    def get_related_entities(self, entity_name: str, max_depth: int = 1) -> Dict[str, Any]:
        """Get entities related to a specific entity up to a maximum depth"""
        if not self.use_graph_db:
            raise ValueError("Graph database not enabled")
        try:
            if entity_name not in self.graph:
                raise ValueError(f"Entity '{entity_name}' does not exist in the graph")
                
            # Get entities within max_depth
            related_entities = set()
            explore_queue = [(entity_name, 0)]  # (node, depth)
            visited = set([entity_name])
            
            while explore_queue:
                node, depth = explore_queue.pop(0)
                
                if depth <= max_depth:
                    # Add all neighbors at this depth
                    neighbor_nodes = set(self.graph.successors(node)) | set(self.graph.predecessors(node))
                    for neighbor in neighbor_nodes:
                        if neighbor not in visited:
                            visited.add(neighbor)
                            related_entities.add(neighbor)
                            if depth < max_depth:
                                explore_queue.append((neighbor, depth + 1))
            
            # Get entity details
            entities = []
            for name in related_entities:
                node_data = self.graph.nodes[name]
                entities.append({
                    "name": name,
                    "entity_type": node_data.get("entity_type", "unknown"),
                    "observations": node_data.get("observations", [])[:3]  # First 3 observations
                })
                
            return {
                "source_entity": entity_name,
                "max_depth": max_depth,
                "related_entities_count": len(entities),
                "entities": entities
            }
            
        except ValueError:
            # Re-raise validation errors
            raise
        except Exception as exc:
            logger.error("Error getting related entities: %s", exc, exc_info=True)
            return {"error": str(exc)}

    def _load_memory(self):
        """Load memory data from file"""
        if os.path.exists(self.memory_file_path):
            try:
                with open(self.memory_file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.entities = data.get('entities', {})
                    self.relations = data.get('relations', [])
                    
                    # Rebuild graph if using graph DB
                    if self.use_graph_db:
                        self._rebuild_graph()
                        
                logger.info(f"Loaded {len(self.entities)} entities and {len(self.relations)} relations from {self.memory_file_path}")
            except Exception as e:
                logger.error(f"Error loading memory data: {e}")
                # Initialize empty data
                self.entities = {}
                self.relations = []

    def _save_memory(self):
        """Save memory data to file"""
        try:
            with open(self.memory_file_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'entities': self.entities,
                    'relations': self.relations
                }, f, indent=2)
            logger.info(f"Saved {len(self.entities)} entities and {len(self.relations)} relations to {self.memory_file_path}")
        except Exception as e:
            logger.error(f"Error saving memory data: {e}")

    def _rebuild_graph(self):
        """Rebuild the graph database from entities and relations"""
        if not self.use_graph_db:
            return
            
        # Clear existing graph
        self.graph.clear()
        
        # Add all entities as nodes
        for entity_name, entity_data in self.entities.items():
            self.graph.add_node(entity_name, **entity_data)
            
        # Add all relations as edges
        for relation in self.relations:
            from_entity = relation['from']
            to_entity = relation['to']
            relation_type = relation['relation_type']
            self.graph.add_edge(from_entity, to_entity, 
                               relation_type=relation_type, 
                               **relation.get('properties', {}))

    def get_entities(self, entity_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """Get all entities, optionally filtered by type"""
        result = []
        
        with self.lock:
            for name, data in self.entities.items():
                # Apply type filter if specified
                if entity_type and data['entity_type'] != entity_type:
                    continue
                    
                # Build entity object
                entity = {
                    'name': name,
                    'entity_type': data['entity_type'],
                    'properties': data.get('properties', {}),
                    'created_at': data.get('created_at')
                }
                result.append(entity)
                
        return result

    def get_relations(self, from_entity: Optional[str] = None, 
                     to_entity: Optional[str] = None,
                     relation_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """Get relations, optionally filtered by entities or type"""
        result = []
        
        with self.lock:
            for relation in self.relations:
                # Apply filters
                if from_entity and relation['from'] != from_entity:
                    continue
                if to_entity and relation['to'] != to_entity:
                    continue
                if relation_type and relation['relation_type'] != relation_type:
                    continue
                    
                # Build relation object
                relation_obj = {
                    'from_': relation['from'],  # Use from_ to match test expectations
                    'to': relation['to'],
                    'relation_type': relation['relation_type'],
                    'properties': relation.get('properties', {}),
                    'created_at': relation.get('created_at')
                }
                result.append(relation_obj)
                
        return result

    def query_entities(self, entity_type: Optional[str] = None, 
                      properties: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Query entities with optional filters"""
        result = []
        
        with self.lock:
            for name, data in self.entities.items():
                # Apply type filter if specified
                if entity_type and data['entity_type'] != entity_type:
                    continue
                    
                # Apply property filters if specified
                if properties:
                    match = True
                    entity_props = data.get('properties', {})
                    for key, value in properties.items():
                        if key not in entity_props or entity_props[key] != value:
                            match = False
                            break
                    if not match:
                        continue
                    
                # Build entity object
                entity = {
                    'name': name,
                    'entity_type': data['entity_type'],
                    'properties': data.get('properties', {}),
                    'created_at': data.get('created_at')
                }
                result.append(entity)
                
        return result


================================================================================
FILE: app\core\scraper_service.py
LANGUAGE: python
SIZE: 27485 bytes
================================================================================

import re
import time
import json
import random
import asyncio
import hashlib
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, TypeVar, cast, Protocol, TYPE_CHECKING
from urllib.parse import urljoin, urlparse

# For type checking only
if TYPE_CHECKING:
    from bs4 import BeautifulSoup, Tag
    from playwright.async_api import Page, Browser, BrowserContext, Response

# Third-party imports in try/except for graceful handling
try:
    from bs4 import BeautifulSoup, Tag
    HAS_BS4 = True
except ImportError:
    BeautifulSoup = None
    Tag = None
    HAS_BS4 = False

try:
    from playwright.async_api import async_playwright, Response
    HAS_PLAYWRIGHT = True
except ImportError:
    async_playwright = None
    Response = None
    HAS_PLAYWRIGHT = False

from app.utils.config import get_config
from app.utils.markdown import html_to_markdown

logger = logging.getLogger(__name__)

# Define type aliases for improved type checking
SoupType = TypeVar('SoupType')
TagType = TypeVar('TagType')
ResponseType = TypeVar('ResponseType')

class ScraperService:
    """Service for scraping web content and processing HTML pages"""
    def __init__(self):
        # Check if required libraries are available
        if not HAS_BS4 or not HAS_PLAYWRIGHT:
            logger.error("Required libraries (BeautifulSoup or playwright) are not installed")
            
        config = get_config()
        self.min_delay = config.scraper_min_delay
        self.max_delay = config.scraper_max_delay
        
        self.data_dir = Path(config.scraper_data_path)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        self.cache_dir = self.data_dir / "cache"
        self.cache_dir.mkdir(exist_ok=True)
        
        self.user_agent = config.user_agent
        
        self.browser = None
        self.context = None

    async def get_browser(self):
        """Initialize and return the browser instance"""
        if self.browser is None:
            if not HAS_PLAYWRIGHT:
                raise ImportError("Playwright is not installed")
            playwright = await async_playwright().start()
            self.browser = await playwright.chromium.launch(headless=True)
        return self.browser

    async def close(self):
        """Close browser instance when done"""
        if self.browser:
            await self.browser.close()
            self.browser = None

    async def get_or_scrape_url(self, url: str, max_cache_age: int = 86400) -> Dict[str, Any]:
        """Get URL from cache or scrape it if not cached or too old"""
        try:
            cache_key = hashlib.md5(url.encode()).hexdigest()
            cache_path = self.cache_dir / f"{cache_key}.json"
            
            if cache_path.exists():
                cache_age = time.time() - cache_path.stat().st_mtime
                if cache_age < max_cache_age:
                    try:
                        with open(cache_path, "r", encoding="utf-8") as f:
                            return json.load(f)
                    except Exception as e:
                        logger.warning(f"Error loading cache for {url}: {e}")
            
            result = await self.scrape_url(url)
            
            if result["success"]:
                try:
                    with open(cache_path, "w", encoding="utf-8") as f:
                        json.dump(result, f, ensure_ascii=False, indent=2)
                except Exception as e:
                    logger.warning(f"Error saving to cache for {url}: {e}")
            return result
        except Exception as e:
            logger.error(f"Error in get_or_scrape_url for {url}: {e}", exc_info=True)
            return {
                "url": url,
                "success": False,
                "error": str(e)
            }

    async def _handle_rate_limiting(self, response: Optional["ResponseType"]) -> bool:
        """Handle rate limiting based on response codes"""
        if response is not None and response.status == 429:  # Too Many Requests
            retry_after = response.headers.get('retry-after')
            wait_time = int(retry_after) if retry_after and retry_after.isdigit() else 60
            logger.info(f"Rate limited. Waiting for {wait_time} seconds")
            await asyncio.sleep(wait_time)
            return True
        return False

    async def scrape_url(self, url: str) -> Dict[str, Any]:
        """Scrape a URL and extract its content"""
        try:
            if not HAS_PLAYWRIGHT:
                raise ImportError("Playwright is not installed")
                
            browser = await self.get_browser()
            context = await browser.new_context(
                user_agent=self.user_agent
            )
            try:
                delay = random.uniform(self.min_delay, self.max_delay)
                await asyncio.sleep(delay)
                
                page = await context.new_page()
                response = await page.goto(url, wait_until="domcontentloaded", timeout=30000)
                
                if response is None or not response.ok:
                    if await self._handle_rate_limiting(response):
                        await page.close()
                        return await self.scrape_url(url)
                    else:
                        status = response.status if response else 0
                        status_text = response.status_text if response else "Unknown error"
                        return {
                            "url": url,
                            "success": False,
                            "error": f"HTTP Error: {status} {status_text}"
                        }
                
                await page.wait_for_load_state("networkidle")
                
                html_content = await page.content()
                title = await page.title()
                
                markdown_content = html_to_markdown(html_content, url, title)
                metadata = self._extract_metadata(html_content, url)
                links = self._extract_links(url, html_content)
                
                return {
                    "url": url,
                    "title": title,
                    "content": markdown_content,
                    "metadata": metadata,
                    "links": links,
                    "scraped_at": int(time.time()),
                    "success": True
                }
            finally:
                await context.close()
        except Exception as e:
            logger.error(f"Error scraping {url}: {e}", exc_info=True)
            return {
                "url": url,
                "success": False,
                "error": str(e)
            }

    def _extract_metadata(self, content: str, url: str) -> Dict[str, Any]:
        """Extract metadata from HTML content"""
        metadata: Dict[str, Any] = {
            "source_url": url,
            "extracted_at": datetime.now().isoformat()
        }
        try:
            if not HAS_BS4:
                return metadata
                
            soup = BeautifulSoup(content, "html.parser")
            # Extract Open Graph metadata
            for prop in ["og:title", "og:description", "og:image", "og:type", "og:site_name"]:
                element = soup.find("meta", property=prop)
                if element is not None:
                    element_tag = cast("Tag", element)
                    content_attr = element_tag.get("content")
                    if content_attr:
                        key = prop.rsplit(":", maxsplit=1)[-1]
                        metadata[key] = content_attr
            # Extract basic metadata
            if "title" not in metadata:
                title_tag = soup.find("title")
                if title_tag:
                    metadata["title"] = title_tag.get_text()
            # Extract description
            if "description" not in metadata:
                desc = soup.find("meta", attrs={"name": "description"})
                if desc is not None:
                    desc_tag = cast("Tag", desc)
                    content_attr = desc_tag.get("content")
                    if content_attr:
                        metadata["description"] = content_attr
            # Extract LD+JSON structured data
            structured_data = self._extract_structured_data(soup)
            if structured_data:
                metadata["structured_data"] = structured_data
        except Exception as e:
            logger.warning("Error extracting metadata from %s: %s", url, e)
        return metadata

    def _extract_structured_data(self, soup: Any) -> Dict[str, Any]:
        """Extract structured data from LD+JSON scripts"""
        try:
            if not HAS_BS4:
                return {}
                
            structured_data = []
            for script in soup.find_all("script", type="application/ld+json"):
                try:
                    script_tag = cast("Tag", script)
                    script_string = script_tag.string
                    if script_string:
                        data = json.loads(script_string)
                        structured_data.append(data)
                except (json.JSONDecodeError, TypeError):
                    continue
            return {"items": structured_data} if structured_data else {}
        except Exception as e:
            logger.warning("Error extracting structured data: %s", e)
            return {}

    def _extract_links(self, base_url: str, content: str) -> List[str]:
        """Extract links from content"""
        if not HAS_BS4:
            return []
            
        links = []
        # Extract Markdown links [text](url)
        markdown_links = re.findall(r'\[.*?\]\((https?://[^)]+)\)', content)
        links.extend(markdown_links)
        # Extract HTML links from the content
        soup = BeautifulSoup(content, "html.parser")
        for a_tag in soup.find_all('a', href=True):
            a_tag_cast = cast("Tag", a_tag)
            href = a_tag_cast.get('href')
            if href:
                # Handle relative links
                absolute_url = urljoin(base_url, str(href))
                links.append(absolute_url)
        # Remove duplicates and return
        return list(set(links))

    async def search_and_scrape(self, query: str, max_results: int = 10) -> List[Dict[str, Any]]:
        """Search for content and scrape the results"""
        # Use multiple search engines with rotation for reliability
        search_engines = [
            f"https://duckduckgo.com/html/?q={query}",
            f"https://www.bing.com/search?q={query}"
        ]
        for search_url in search_engines:
            try:
                # Get search results page
                search_result = await self.scrape_url(search_url)
                if not search_result["success"]:
                    continue
                # Extract result links from search page
                result_links = self._extract_search_result_links(
                    search_result["content"],
                    search_url
                )
                # Limit the number of results
                result_links = result_links[:max_results]
                # Scrape each result URL
                results = []
                for result_url in result_links:
                    try:
                        result = await self.get_or_scrape_url(result_url)
                        results.append(result)
                        # Add delay between requests
                        await asyncio.sleep(random.uniform(self.min_delay, self.max_delay))
                    except Exception as e:
                        logger.warning(f"Failed to scrape search result {result_url}: {e}")
                return results
            except Exception as e:
                logger.warning(f"Search failed on {search_url}: {str(e)}")
                # Try next search engine instead of failing
                continue
        # Return empty results only if all engines fail
        return []

    def _extract_search_result_links(self, content: str, search_url: str) -> List[str]:
        """Extract links from search result page"""
        if not HAS_BS4:
            return []
            
        soup = BeautifulSoup(content, "html.parser")
        links = []
        # DuckDuckGo results
        if "duckduckgo.com" in search_url:
            for result in soup.select(".result__a"):
                result_tag = cast("Tag", result)
                href = result_tag.get("href")
                if href:
                    links.append(str(href))
        # Bing results
        elif "bing.com" in search_url:
            for result in soup.select("li.b_algo h2 a"):
                result_tag = cast("Tag", result)
                href = result_tag.get("href")
                if href:
                    links.append(str(href))
        return links

    async def scrape_with_pagination(self, url: str, max_pages: int = 5) -> Dict[str, Any]:
        """Scrape a URL and follow pagination links"""
        all_content = ""
        current_url = url
        pages_scraped = 0
        try:
            while current_url and pages_scraped < max_pages:
                result = await self.scrape_url(current_url)
                if not result["success"]:
                    break
                # Accumulate content
                all_content += result["content"] + "\n\n---\n\n"
                pages_scraped += 1
                # Find next page link
                next_url = self._find_next_page_link(result["content"], current_url)
                if not next_url or next_url == current_url:
                    break
                current_url = next_url
                # Add delay between pages
                await asyncio.sleep(random.uniform(self.min_delay, self.max_delay))
            # Create combined result
            return {
                "url": url,
                "title": f"Paginated content ({pages_scraped} pages)",
                "content": all_content,
                "scraped_at": int(time.time()),
                "success": True,
                "pages_scraped": pages_scraped
            }
        except Exception as e:
            logger.error(f"Error in paginated scraping for {url}: {e}", exc_info=True)
            return {
                "url": url,
                "success": False,
                "error": str(e),
                "pages_scraped": pages_scraped
            }

    def _find_next_page_link(self, content: str, current_url: str) -> Optional[str]:
        """Find pagination link in content"""
        if not HAS_BS4:
            return None
            
        soup = BeautifulSoup(content, "html.parser")
        # Common patterns for next page links
        next_selectors = [
            '.pagination .next',
            '.pagination a[rel="next"]',
            'a.next',
            'a:contains("Next")',
            'a[aria-label="Next"]',
            '.pagination a:contains("")',
            '.pagination a:contains(">")'
        ]
        for selector in next_selectors:
            try:
                next_link = soup.select_one(selector)
                if next_link:
                    next_link_tag = cast("Tag", next_link)
                    href = next_link_tag.get('href')
                    if href:
                        return urljoin(current_url, str(href))
            except Exception:
                continue
        return None

    async def capture_screenshot(self, url: str, full_page: bool = True) -> Dict[str, Any]:
        """Capture screenshot of a webpage"""
        if not HAS_PLAYWRIGHT:
            return {"url": url, "success": False, "error": "Playwright not installed"}
            
        browser = await self.get_browser()
        context = await browser.new_context(
            user_agent=self.user_agent,
            viewport={'width': 1920, 'height': 1080}
        )
        try:
            page = await context.new_page()
            await page.goto(url, wait_until="networkidle")
            # Capture screenshot
            screenshot_path = self.data_dir / "screenshots"
            screenshot_path.mkdir(exist_ok=True)
            filename = f"{hashlib.md5(url.encode()).hexdigest()}.png"
            file_path = screenshot_path / filename
            await page.screenshot(path=str(file_path), full_page=full_page)
            return {
                "url": url,
                "screenshot_path": str(file_path),
                "timestamp": int(time.time()),
                "success": True
            }
        except Exception as e:
            logger.error(f"Screenshot failed for {url}: {e}")
            return {
                "url": url,
                "error": str(e),
                "success": False
            }
        finally:
            await context.close()

    async def scrape_sitemap(self, sitemap_url: str, max_urls: int = 50) -> Dict[str, Any]:
        """Extract URLs from sitemap and scrape them"""
        try:
            if not HAS_BS4:
                return {"success": False, "error": "BeautifulSoup not installed"}
                
            # Scrape the sitemap XML
            sitemap_result = await self.scrape_url(sitemap_url)
            if not sitemap_result["success"]:
                return {
                    "success": False,
                    "error": f"Failed to fetch sitemap: {sitemap_result.get('error')}"
                }
            # Extract URLs from sitemap
            soup = BeautifulSoup(sitemap_result["content"], "xml")
            urls = []
            # Process standard sitemap format
            for loc in soup.find_all("loc"):
                urls.append(loc.text)
            # Limit the number of URLs to scrape
            urls = urls[:max_urls]
            # Scrape each URL
            scraped_results = []
            for url in urls:
                try:
                    result = await self.get_or_scrape_url(url)
                    scraped_results.append(result)
                    # Add delay between requests
                    await asyncio.sleep(random.uniform(self.min_delay, self.max_delay))
                except Exception as e:
                    logger.warning(f"Failed to scrape URL from sitemap {url}: {e}")
            return {
                "sitemap_url": sitemap_url,
                "urls_found": len(urls),
                "urls_scraped": scraped_results,
                "success": True
            }
        except Exception as e:
            logger.error(f"Error processing sitemap {sitemap_url}: {e}", exc_info=True)
            return {
                "sitemap_url": sitemap_url,
                "success": False,
                "error": str(e)
            }

    async def crawl_website(self, start_url: str, max_pages: int = 50,
                        recursion_depth: int = 2, allowed_domains: Optional[List[str]] = None,
                        verification_pass: bool = False) -> Dict[str, Any]:
        """Crawl a website starting from a URL"""
        # Parse the start URL to get the base domain
        start_parsed = urlparse(start_url)
        start_domain = start_parsed.netloc
        
        # Set up allowed domains
        if allowed_domains is None:
            allowed_domains = [start_domain]
            
        # Track visited URLs and frontier
        visited = set()
        frontier = [(start_url, 0)]  # (url, depth)
        results = []
        
        # Get browser instance
        browser = await self.get_browser()
        context = await browser.new_context(user_agent=self.user_agent)
        
        try:
            # Process URLs in the frontier
            while frontier and len(visited) < max_pages:
                current_url, depth = frontier.pop(0)
                
                # Skip if already visited
                if current_url in visited:
                    continue
                    
                # Check domain
                current_parsed = urlparse(current_url)
                if current_parsed.netloc not in allowed_domains:
                    continue
                
                # Process the page
                logger.info(f"Crawling: {current_url} (depth {depth})")
                visited.add(current_url)
                
                try:
                    # Create a new page for each request
                    page = await context.new_page()
                    
                    try:
                        # Navigate to the page with timeout
                        response = await page.goto(current_url, wait_until="domcontentloaded", timeout=30000)
                        
                        if response is None or not response.ok:
                            status = response.status if response else 0
                            status_text = response.status_text if response else "Unknown error"
                            results.append({
                                "url": current_url,
                                "success": False,
                                "error": f"HTTP Error: {status} {status_text}"
                            })
                            await page.close()
                            continue
                            
                        # Wait for content to load
                        await page.wait_for_load_state("networkidle", timeout=30000)
                        
                        # Extract page content
                        html_content = await page.content()
                        title = await page.title()
                        
                        # Convert to markdown
                        markdown_content = html_to_markdown(html_content, current_url, title)
                        
                        # Extract metadata
                        metadata = self._extract_metadata(html_content, current_url)
                        
                        # Add result
                        result = {
                            "url": current_url,
                            "title": title,
                            "content": markdown_content,
                            "metadata": metadata,
                            "scraped_at": int(time.time()),
                            "success": True
                        }
                        results.append(result)
                        
                        # Extract links for next level if not at max depth
                        if depth < recursion_depth:
                            links = self._extract_links(current_url, html_content)
                            for link in links:
                                # Check if it's a URL we should crawl
                                link_parsed = urlparse(link)
                                if (link not in visited and
                                    link_parsed.netloc in allowed_domains and
                                    link not in [f[0] for f in frontier]):
                                    frontier.append((link, depth + 1))
                                    
                                    # Stop adding to frontier if we'll exceed max_pages
                                    if len(visited) + len(frontier) >= max_pages:
                                        break
                        
                    except Exception as page_error:
                        results.append({
                            "url": current_url,
                            "success": False,
                            "error": str(page_error)
                        })
                    finally:
                        # Close the page
                        await page.close()
                        
                    # Add delay between requests
                    await asyncio.sleep(random.uniform(self.min_delay, self.max_delay))
                    
                except Exception as e:
                    logger.error(f"Error processing {current_url}: {e}")
                    results.append({
                        "url": current_url,
                        "success": False,
                        "error": str(e)
                    })
                    
            # Prepare response
            response = {
                "pages_crawled": len(visited),
                "start_url": start_url,
                "success_count": sum(1 for r in results if r.get("success", False)),
                "failed_count": sum(1 for r in results if not r.get("success", False)),
                "results": results
            }
            
            # Perform verification pass if requested
            if verification_pass and visited:
                verification_results = await self._perform_verification_pass(list(visited)[:10], context)
                response["verification_results"] = verification_results
                response["verification_success_rate"] = (
                    sum(1 for v in verification_results if v["verified"]) / 
                    len(verification_results) if verification_results else 0
                )
                
            return response
            
        finally:
            # Clean up
            await context.close()
    
    async def _perform_verification_pass(self, urls: List[str], context: Any) -> List[Dict[str, Any]]:
        """Verification pass to check content stability"""
        verification_results = []
        
        for url in urls:
            try:
                page = await context.new_page()
                try:
                    await page.goto(url, wait_until="networkidle", timeout=30000)
                    verification_results.append({
                        "url": url,
                        "verified": True
                    })
                except Exception as e:
                    verification_results.append({
                        "url": url,
                        "verified": False,
                        "error": str(e)
                    })
                finally:
                    await page.close()
                    
                # Rate limiting
                await asyncio.sleep(random.uniform(self.min_delay, self.max_delay))
                
            except Exception as e:
                verification_results.append({
                    "url": url,
                    "verified": False,
                    "error": str(e)
                })
                
        return verification_results
    
    async def scrape_urls(self, urls: List[str]) -> List[Dict[str, Any]]:
        """Scrape multiple URLs in parallel"""
        tasks = [self.scrape_url(url) for url in urls]
        return await asyncio.gather(*tasks)

================================================================================
FILE: app\core\test_user_prefs.py
LANGUAGE: python
SIZE: 278 bytes
================================================================================

from app.core.memory_service import MemoryService

ms = MemoryService()
# Set a preference
ms.set_user_preference("test_user", {"theme": "dark", "language": "en"})
# Get the preference
prefs = ms.get_user_preference("test_user")
print(f"Retrieved preferences: {prefs}")


================================================================================
FILE: app\models\__init__.py
LANGUAGE: python
SIZE: 52 bytes
================================================================================

"""
Data Models for the Unified Tools Server
"""


================================================================================
FILE: app\models\documents.py
LANGUAGE: python
SIZE: 3414 bytes
================================================================================

from enum import Enum
from typing import Dict, Any, Optional, List, Union
from pydantic import BaseModel, Field, HttpUrl


class DocumentType(str, Enum):
    MANUSCRIPT = "manuscript"  # Novels, fiction, etc.
    DOCUMENTATION = "documentation"  # Research papers, technical docs
    DATASET = "dataset"  # Structured data for training
    WEBPAGE = "webpage"  # Scraped web content
    GENERIC = "generic"  # Other document types


class CreateDocumentRequest(BaseModel):
    """Request model for document creation"""

    title: str = Field(..., description="Document title", min_length=1)
    content: str = Field(..., description="Document content (text)")
    document_type: DocumentType = Field(DocumentType.GENERIC, description="Type of document")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Custom metadata")
    tags: List[str] = Field(default_factory=list, description="Document tags")
    source_url: Optional[str] = Field(None, description="Source URL if applicable")
    storage_type: str = Field("local", description="Storage type (local or s3)")


class UpdateDocumentRequest(BaseModel):
    """Request model for document updates"""

    title: Optional[str] = Field(None, description="Updated title")
    content: Optional[str] = Field(None, description="Updated content")
    metadata: Optional[Dict[str, Any]] = Field(None, description="Updated metadata")
    tags: Optional[List[str]] = Field(None, description="Updated tags")
    commit_message: str = Field("Updated document", description="Git commit message")


class DocumentResponse(BaseModel):
    """Response model for document operations"""

    id: str = Field(..., description="Unique document identifier")
    title: str = Field(..., description="Document title")
    document_type: DocumentType = Field(..., description="Type of document")
    created_at: int = Field(..., description="Creation timestamp")
    updated_at: int = Field(..., description="Last update timestamp")
    tags: List[str] = Field(..., description="Document tags")
    metadata: Dict[str, Any] = Field(..., description="Document metadata")
    content_preview: str = Field(..., description="Preview of document content")
    size_bytes: int = Field(..., description="Document size in bytes")
    version_count: Optional[int] = Field(1, description="Number of versions")
    content_available: bool = Field(..., description="Whether full content is available")
    source_url: Optional[str] = Field(None, description="Source URL if applicable")


class DocumentVersionResponse(BaseModel):
    """Response model for document version history"""

    version_hash: str = Field(..., description="Git commit hash")
    commit_message: str = Field(..., description="Commit message")
    author: str = Field(..., description="Author name")
    timestamp: int = Field(..., description="Commit timestamp")
    changes: Optional[Dict[str, Any]] = Field(None, description="Changes in this version")


class DocumentContentResponse(BaseModel):
    """Response model for document content"""

    id: str = Field(..., description="Document identifier")
    title: str = Field(..., description="Document title")
    content: str = Field(..., description="Full document content")
    version: Optional[str] = Field(None, description="Version hash if specific version requested")


================================================================================
FILE: app\models\filesystem.py
LANGUAGE: python
SIZE: 3611 bytes
================================================================================

from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional


class ReadFileRequest(BaseModel):
    """Request to read a file"""

    path: str = Field(..., description="Path to file")
    storage: str = Field("local", description="Storage type (local or s3)")
    bucket: Optional[str] = Field(None, description="S3 bucket name if using S3 storage")


class WriteFileRequest(BaseModel):
    """Request to write to a file"""

    path: str = Field(..., description="Path to file")
    content: str = Field(..., description="Content to write")
    storage: str = Field("local", description="Storage type (local or s3)")
    bucket: Optional[str] = Field(None, description="S3 bucket name if using S3 storage")


class ListDirectoryRequest(BaseModel):
    """Request to list directory contents"""

    path: str = Field(..., description="Directory path")
    storage: str = Field("local", description="Storage type (local or s3)")
    bucket: Optional[str] = Field(None, description="S3 bucket name if using S3 storage")
    recursive: bool = Field(False, description="Whether to list subdirectories recursively")


class SearchFilesRequest(BaseModel):
    """Request to search for files"""

    path: str = Field(..., description="Base path to search in")
    pattern: str = Field(..., description="Pattern to search for")
    storage: str = Field("local", description="Storage type (local or s3)")
    bucket: Optional[str] = Field(None, description="S3 bucket name if using S3 storage")
    exclude_patterns: Optional[List[str]] = Field(None, description="Patterns to exclude")


class CreateDirectoryRequest(BaseModel):
    """Request to create a directory"""

    path: str = Field(..., description="Directory path")
    storage: str = Field("local", description="Storage type (local or s3)")
    bucket: Optional[str] = Field(None, description="S3 bucket name if using S3 storage")


class DeleteFileRequest(BaseModel):
    """Request to delete a file"""

    path: str = Field(..., description="Path to file")
    storage: str = Field("local", description="Storage type (local or s3)")
    bucket: Optional[str] = Field(None, description="S3 bucket name if using S3 storage")


class DirectoryItem(BaseModel):
    """Information about a directory item"""

    name: str = Field(..., description="Item name")
    path: str = Field(..., description="Item path relative to listing directory")
    type: str = Field(..., description="Item type (file or directory)")
    size: Optional[int] = Field(None, description="File size in bytes")
    last_modified: Optional[int] = Field(None, description="Last modification timestamp")


class DirectoryListingResponse(BaseModel):
    """Response for directory listing"""

    path: str = Field(..., description="Listed directory path")
    items: List[DirectoryItem] = Field(..., description="Directory contents")


class InvalidateCacheRequest(BaseModel):
    """Request to invalidate file cache"""

    path: Optional[str] = Field(None, description="Path to invalidate (None for all)")
    storage: str = Field("local", description="Storage type (local or s3)")
    bucket: Optional[str] = Field(None, description="S3 bucket name if using S3 storage")


class FileExistsRequest(BaseModel):
    """Request to check if file exists"""

    path: str = Field(..., description="Path to check")
    storage: str = Field("local", description="Storage type (local or s3)")
    bucket: Optional[str] = Field(None, description="S3 bucket name if using S3 storage")


================================================================================
FILE: app\models\git.py
LANGUAGE: python
SIZE: 2827 bytes
================================================================================

from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any, Union


class GitRepoPath(BaseModel):
    repo_path: str = Field(..., description="File system path to the Git repository.")


class GitStatusRequest(GitRepoPath):
    pass


class GitDiffRequest(GitRepoPath):
    file_path: Optional[str] = Field(None, description="Specific file to show diff for")
    target: Optional[str] = Field(None, description="The branch or commit to diff against.")


class GitCommitRequest(GitRepoPath):
    message: str = Field(..., description="Commit message for recording the change.")
    author_name: Optional[str] = Field(None, description="Git author name")
    author_email: Optional[str] = Field(None, description="Git author email")


class GitAddRequest(GitRepoPath):
    files: List[str] = Field(..., description="List of file paths to add to the staging area.")


class GitLogRequest(GitRepoPath):
    max_count: int = Field(10, description="Maximum number of commits to retrieve.")
    file_path: Optional[str] = Field(None, description="Filter log by specific file")


class GitCreateBranchRequest(GitRepoPath):
    branch_name: str = Field(..., description="Name of the branch to create.")
    base_branch: Optional[str] = Field(
        None, description="Optional base branch name to create the new branch from."
    )


class GitCheckoutRequest(GitRepoPath):
    branch_name: str = Field(..., description="Branch name to checkout.")
    create: bool = Field(False, description="Whether to create the branch if it doesn't exist")


class GitInitRequest(GitRepoPath):
    bare: bool = Field(False, description="Whether to create a bare repository")


class GitCloneRequest(BaseModel):
    repo_url: str = Field(..., description="URL of the repository to clone")
    local_path: str = Field(..., description="Local path to clone to")
    auth_token: Optional[str] = Field(None, description="Authentication token if needed")


class CommitInfo(BaseModel):
    hash: str = Field(..., description="Commit hash")
    author: str = Field(..., description="Commit author")
    date: str = Field(..., description="Commit date")
    message: str = Field(..., description="Commit message")


class GitStatusResponse(BaseModel):
    clean: bool = Field(..., description="Whether the working directory is clean")
    current_branch: str = Field(..., description="Current active branch")
    staged_files: List[str] = Field(..., description="Files staged for commit")
    unstaged_files: List[str] = Field(..., description="Files with changes not staged")
    untracked_files: List[str] = Field(..., description="Untracked files")


class GitLogResponse(BaseModel):
    commits: List[CommitInfo] = Field(..., description="List of commits")


================================================================================
FILE: app\models\memory.py
LANGUAGE: python
SIZE: 2588 bytes
================================================================================

from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional, Union, Literal


class Entity(BaseModel):
    name: str = Field(..., description="The name of the entity")
    entity_type: str = Field(..., description="The type of the entity")
    observations: List[str] = Field(
        default_factory=list,
        description="An array of observation contents associated with the entity",
    )


class Relation(BaseModel):
    from_: str = Field(
        ..., alias="from", description="The name of the entity where the relation starts"
    )
    to: str = Field(..., description="The name of the entity where the relation ends")
    relation_type: str = Field(..., description="The type of the relation")


class KnowledgeGraph(BaseModel):
    entities: List[Entity] = Field(default_factory=list)
    relations: List[Relation] = Field(default_factory=list)


class CreateEntitiesRequest(BaseModel):
    entities: List[Entity] = Field(..., description="List of entities to create")


class CreateRelationsRequest(BaseModel):
    relations: List[Relation] = Field(..., description="List of relations to create")


class ObservationItem(BaseModel):
    entity_name: str = Field(..., description="The name of the entity to add the observations to")
    contents: List[str] = Field(..., description="An array of observation contents to add")


class AddObservationsRequest(BaseModel):
    observations: List[ObservationItem] = Field(..., description="A list of observation additions")


class DeleteEntitiesRequest(BaseModel):
    entity_names: List[str] = Field(..., description="An array of entity names to delete")


class DeleteRelationsRequest(BaseModel):
    relations: List[Relation] = Field(..., description="An array of relations to delete")


class SearchNodesRequest(BaseModel):
    query: str = Field(
        ..., description="The search query to match against entity names, types, and content"
    )


class OpenNodesRequest(BaseModel):
    names: List[str] = Field(..., description="An array of entity names to retrieve")


class UserPreference(BaseModel):
    user_id: str = Field(..., description="Unique user identifier")
    preferences: Dict[str, Any] = Field(default_factory=dict, description="User preferences")


class AddEntitiesRequest(BaseModel):
    entities: List[Dict[str, Any]] = Field(..., description="List of entities to create")


class AddRelationsRequest(BaseModel):
    relations: List[Dict[str, Any]] = Field(..., description="List of relations to create")


================================================================================
FILE: app\models\scraper.py
LANGUAGE: python
SIZE: 11928 bytes
================================================================================

from fastapi import APIRouter, Body, HTTPException, Query, Path
from typing import Dict, Any, List, Optional, Union
from pydantic import BaseModel, Field
import random


# Define models
class ScrapeSingleUrlRequest(BaseModel):
    """Request to scrape a single URL"""

    url: str = Field(..., description="URL to scrape")
    wait_for_selector: Optional[str] = Field(None, description="CSS selector to wait for")
    wait_for_timeout: Optional[int] = Field(30000, description="Maximum wait time in ms")
    extract_tables: bool = Field(True, description="Extract tables from content")
    store_as_document: bool = Field(False, description="Store result as a document")
    document_tags: Optional[List[str]] = Field(None, description="Tags for document if stored")


class UrlList(BaseModel):
    """Request to scrape multiple URLs"""

    urls: List[str] = Field(..., description="List of URLs to scrape")
    recursion_depth: int = Field(0, ge=0, le=3, description="How many links deep to follow (0-3)")
    store_as_documents: bool = Field(False, description="Save results as documents")
    document_tags: Optional[List[str]] = Field(None, description="Tags for documents if stored")


class ScrapeCrawlRequest(BaseModel):
    """Request to crawl a website"""

    start_url: str = Field(..., description="Starting URL for crawl")
    max_pages: int = Field(100, ge=1, description="Maximum number of pages to crawl")
    recursion_depth: int = Field(1, ge=1, description="How many links deep to follow")
    allowed_domains: Optional[List[str]] = Field(
        None, description="Restrict crawling to these domains"
    )
    create_documents: bool = Field(True, description="Create documents from scraped content")
    document_tags: Optional[List[str]] = Field(None, description="Tags for documents if created")
    verification_pass: bool = Field(False, description="Run verification pass after initial crawl")


class SearchAndScrapeRequest(BaseModel):
    """Request to search and scrape results"""

    query: str = Field(..., description="Search query")
    max_results: int = Field(10, ge=1, le=50, description="Maximum search results to process")
    create_documents: bool = Field(False, description="Create documents from scraped content")
    document_tags: Optional[List[str]] = Field(None, description="Tags for documents if created")


class SitemapScrapeRequest(BaseModel):
    """Request to scrape URLs from a sitemap"""

    sitemap_url: str = Field(..., description="URL of the sitemap")
    max_urls: int = Field(50, ge=1, description="Maximum number of URLs to scrape")
    create_documents: bool = Field(True, description="Create documents from scraped content")
    document_tags: Optional[List[str]] = Field(None, description="Tags for documents if created")


class TableData(BaseModel):
    headers: List[str] = Field(default_factory=list, description="Table headers")
    rows: List[List[str]] = Field(default_factory=list, description="Table rows")


class ScraperResponse(BaseModel):
    """Response from scraper"""

    url: str = Field(..., description="Scraped URL")
    title: str = Field(..., description="Page title")
    content: str = Field(..., description="Cleaned content in Markdown format")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Extracted metadata")
    scraped_at: int = Field(..., description="Timestamp when scraped")
    success: bool = Field(True, description="Whether scraping was successful")
    links: List[str] = Field(default_factory=list, description="Links extracted from content")
    document_id: Optional[str] = Field(None, description="Document ID if saved as document")
    error: Optional[str] = Field(None, description="Error message if scraping failed")


from app.core.scraper_service import ScraperService
from app.core.documents_service import DocumentsService
from app.models.documents import DocumentType

router = APIRouter()
scraper_service = ScraperService()
documents_service = DocumentsService()


@router.post(
    "/url",
    response_model=ScraperResponse,
    summary="Scrape a single URL",
    description="Extract content from a web page and convert to Markdown",
)
async def scrape_url(request: ScrapeSingleUrlRequest = Body(...)):
    """
    Scrape a single URL and return structured data.
    Extracts content, converts to Markdown, and optionally stores as a document.
    """
    try:
        result = await scraper_service.scrape_url(
            request.url, request.wait_for_selector, request.wait_for_timeout
        )
        # If requested, store as document
        if request.store_as_document and result["success"]:
            doc = documents_service.create_document(
                title=result["title"],
                content=result["content"],
                document_type=DocumentType.WEBPAGE,
                metadata=result["metadata"],
                tags=request.document_tags,
                source_url=result["url"],
            )
            result["document_id"] = doc["id"]
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Scraping failed: {str(e)}")


@router.post(
    "/urls",
    response_model=List[ScraperResponse],
    summary="Scrape multiple URLs",
    description="Scrape multiple URLs in parallel",
)
async def scrape_multiple_urls(request: UrlList = Body(...)):
    """
    Scrape multiple URLs in parallel.
    Processes a list of URLs and returns the scraped content for each.
    """
    try:
        results = await scraper_service.scrape_urls(request.urls)
        # If requested, store results as documents
        if request.store_as_documents:
            for i, result in enumerate(results):
                if result["success"]:
                    try:
                        doc = documents_service.create_document(
                            title=result["title"],
                            content=result["content"],
                            document_type=DocumentType.WEBPAGE,
                            metadata=result["metadata"],
                            tags=request.document_tags,
                            source_url=result["url"],
                        )
                        results[i]["document_id"] = doc["id"]
                    except Exception as e:
                        results[i]["error"] = f"Document creation failed: {str(e)}"
        return results
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Scraping failed: {str(e)}")


@router.post(
    "/crawl",
    response_model=Dict[str, Any],
    summary="Crawl website",
    description="Crawl a website starting from a URL",
)
async def crawl_website(request: ScrapeCrawlRequest = Body(...)):
    """
    Crawl a website starting from a URL.
    Follows links up to a specified depth and processes each page.
    Optional verification pass ensures content stability.
    """
    try:
        results = await scraper_service.crawl_website(
            request.start_url,
            request.max_pages,
            request.recursion_depth,
            request.allowed_domains,
            request.verification_pass,  # Pass the verification_pass parameter
        )
        response = {
            "pages_crawled": results.get("pages_crawled", 0),
            "start_url": request.start_url,
            "success_count": results.get("success_count", 0),
            "failed_count": results.get("failed_count", 0),
        }
        # Include verification results if available
        if "verification_results" in results:
            response["verification_results"] = results["verification_results"]
            response["verification_success_rate"] = results["verification_success_rate"]
        # If requested, create documents
        if request.create_documents:
            document_ids = []
            for result in results.get("results", []):
                if result.get("success", False):
                    try:
                        doc = documents_service.create_document(
                            title=result["title"],
                            content=result["content"],
                            document_type=DocumentType.WEBPAGE,
                            metadata=result["metadata"],
                            tags=request.document_tags,
                            source_url=result["url"],
                        )
                        document_ids.append(doc["id"])
                    except Exception:
                        pass
            response["documents_created"] = len(document_ids)
            response["document_ids"] = document_ids
        return response
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Crawling failed: {str(e)}")


@router.post(
    "/search",
    response_model=List[ScraperResponse],
    summary="Search and scrape",
    description="Search for content and scrape the results",
)
async def search_and_scrape(request: SearchAndScrapeRequest = Body(...)):
    """
    Search for content and scrape the results.
    Performs a web search and scrapes the top results.
    """
    try:
        results = await scraper_service.search_and_scrape(request.query, request.max_results)
        # If requested, create documents
        if request.create_documents:
            for i, result in enumerate(results):
                if result.get("success", False):
                    try:
                        doc = documents_service.create_document(
                            title=result["title"],
                            content=result["content"],
                            document_type=DocumentType.WEBPAGE,
                            metadata=result["metadata"],
                            tags=request.document_tags,
                            source_url=result["url"],
                        )
                        results[i]["document_id"] = doc["id"]
                    except Exception:
                        pass
        return results
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Search and scrape failed: {str(e)}")


@router.post(
    "/sitemap",
    response_model=Dict[str, Any],
    summary="Scrape sitemap",
    description="Extract URLs from sitemap and scrape them",
)
async def scrape_sitemap(request: SitemapScrapeRequest = Body(...)):
    """
    Extract URLs from a sitemap and scrape them.
    Processes XML sitemap files and scrapes the listed URLs.
    """
    try:
        result = await scraper_service.scrape_sitemap(request.sitemap_url, request.max_urls)
        # Handle document creation if requested
        if request.create_documents and result.get("urls_scraped", []):
            document_ids = []
            for scraped_url in result["urls_scraped"]:
                if scraped_url.get("success", False):
                    try:
                        doc = documents_service.create_document(
                            title=scraped_url["title"],
                            content=scraped_url["content"],
                            document_type=DocumentType.WEBPAGE,
                            metadata=scraped_url["metadata"],
                            tags=request.document_tags,
                            source_url=scraped_url["url"],
                        )
                        document_ids.append(doc["id"])
                    except Exception:
                        pass
            result["documents_created"] = len(document_ids)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Sitemap scraping failed: {str(e)}")


================================================================================
FILE: app\utils\__init__.py
LANGUAGE: python
SIZE: 58 bytes
================================================================================

"""
Utility Functions for the Unified Tools Server
"""


================================================================================
FILE: app\utils\config.py
LANGUAGE: python
SIZE: 2806 bytes
================================================================================

import os
from typing import List, Optional

# Third-party imports
import dotenv
from pydantic import Field as PydanticField

# Use pydantic_settings if available, otherwise fallback
try:
    from pydantic_settings import BaseSettings
except ImportError:
    from pydantic import BaseSettings  # Fallback for older pydantic versions

# Load environment variables
dotenv.load_dotenv()


class Config(BaseSettings):
    server_host: str = PydanticField(default=os.getenv("SERVER_HOST", "0.0.0.0"))
    server_port: int = PydanticField(default=int(os.getenv("SERVER_PORT", "8000")))
    dev_mode: bool = PydanticField(default=os.getenv("DEV_MODE", "False").lower() == "true")

    allowed_directories: List[str] = PydanticField(
        default_factory=lambda: os.getenv("ALLOWED_DIRS", "./data").split(",")
    )
    file_cache_enabled: bool = PydanticField(
        default=os.getenv("FILE_CACHE_ENABLED", "False").lower() == "true"
    )

    memory_file_path: str = PydanticField(
        default=os.getenv("MEMORY_FILE_PATH", "./data/memory.json")
    )
    use_graph_db: bool = PydanticField(
        default=os.getenv("USE_GRAPH_DB", "False").lower() == "true"
    )

    default_git_username: str = PydanticField(
        default=os.getenv("DEFAULT_COMMIT_USERNAME", "UnifiedTools")
    )
    default_git_email: str = PydanticField(
        default=os.getenv("DEFAULT_COMMIT_EMAIL", "tools@example.com")
    )

    s3_access_key: Optional[str] = PydanticField(default=os.getenv("S3_ACCESS_KEY"))
    s3_secret_key: Optional[str] = PydanticField(default=os.getenv("S3_SECRET_KEY"))
    s3_region: Optional[str] = PydanticField(default=os.getenv("S3_REGION"))
    s3_bucket: Optional[str] = PydanticField(default=os.getenv("S3_BUCKET"))

    scraper_min_delay: float = PydanticField(
        default=float(os.getenv("SCRAPER_MIN_DELAY", "3"))
    )
    scraper_max_delay: float = PydanticField(
        default=float(os.getenv("SCRAPER_MAX_DELAY", "7"))
    )
    user_agent: str = PydanticField(
        default=os.getenv("USER_AGENT", "Mozilla/5.0 (compatible; UnifiedToolsServer/1.0)")
    )
    scraper_data_path: str = PydanticField(
        default=os.getenv("SCRAPER_DATA_PATH", "./data/scraped")
    )

    # Special configuration for pydantic
    model_config = {
        "env_file": ".env",
        "case_sensitive": False,
    }


# Singleton instance using function attribute pattern
def get_config() -> Config:
    """
    Get the singleton configuration instance.
    
    Returns:
        Config: The configuration object with settings from environment variables
    """
    if not hasattr(get_config, "_config_instance"):
        get_config._config_instance = Config()
    return get_config._config_instance


================================================================================
FILE: app\utils\markdown.py
LANGUAGE: python
SIZE: 12367 bytes
================================================================================

from bs4 import BeautifulSoup, Comment, NavigableString, Tag
import re
import json
from typing import List, Dict, Any, Optional, Tuple
import logging

logger = logging.getLogger(__name__)


def _clean_text(text: str) -> str:
    """Clean up text by removing extra whitespace and normalizing line breaks"""
    # Replace multiple whitespaces with a single space
    text = re.sub(r"\s+", " ", text)
    # Remove leading/trailing whitespace
    text = text.strip()
    return text


def _is_navigation_element(tag):
    """Identify if an element is likely navigation or template content"""
    if not tag or not isinstance(tag, Tag):
        return False
    # Check class names
    nav_classes = ["nav", "menu", "navigation", "banner", "header", "footer", "sidebar", "cookie"]
    if tag.has_attr("class"):
        for cls in tag.get("class", []):
            if any(nav_term in cls.lower() for nav_term in nav_classes):
                return True
    # Check element ID
    if tag.has_attr("id"):
        if any(nav_term in tag.get("id", "").lower() for nav_term in nav_classes):
            return True
    # Check role attribute
    if tag.has_attr("role"):
        nav_roles = ["navigation", "banner", "menu", "menubar", "complementary"]
        if tag.get("role", "").lower() in nav_roles:
            return True
    return False


def calculate_readability_score(soup: BeautifulSoup) -> float:
    """Calculate a readability score for the content"""
    # Simple implementation - can be expanded with readability algorithms
    text = soup.get_text()
    words = len(re.findall(r"\b\w+\b", text))
    sentences = len(re.findall(r"[.!?]+", text)) or 1
    # Calculate average words per sentence (basic readability metric)
    avg_words_per_sentence = words / sentences
    # Detect if text is likely substantive content
    paragraphs = len(soup.find_all("p"))
    has_headings = len(soup.find_all(["h1", "h2", "h3"])) > 0
    # Score based on several factors (0-100)
    score = 50  # Base score
    # Adjust based on sentence length (penalize very short and very long sentences)
    if 10 <= avg_words_per_sentence <= 25:
        score += 20
    elif avg_words_per_sentence > 25:
        score -= 10
    elif avg_words_per_sentence < 5:
        score -= 10
    # Adjust based on content structure
    if paragraphs > 3:
        score += 15
    if has_headings:
        score += 15
    # Normalize to 0-1 range
    return min(100, max(0, score)) / 100


def extract_main_content(soup: BeautifulSoup) -> BeautifulSoup:
    """Extract the main content area from a page"""
    # Look for common content containers
    content_ids = ["content", "main", "article", "post", "entry"]
    content_classes = ["content", "article", "post", "entry", "main"]
    # Try to find content by ID
    for id_name in content_ids:
        content = soup.find(id=id_name)
        if content:
            return content
    # Try to find by class
    for class_name in content_classes:
        content = soup.find(class_=class_name)
        if content:
            return content
    # Look for article tag
    article = soup.find("article")
    if article:
        return article
    # Look for main tag
    main = soup.find("main")
    if main:
        return main
    # Try to find the element with the most paragraphs
    paragraphs_by_container = {}
    for container in soup.find_all(["div", "section"]):
        paragraphs = len(container.find_all("p"))
        if paragraphs > 2:  # Only consider containers with multiple paragraphs
            paragraphs_by_container[container] = paragraphs
    if paragraphs_by_container:
        main_container = max(paragraphs_by_container, key=paragraphs_by_container.get)
        return main_container
    # If no content container found, use the body
    return soup.body or soup


def html_to_markdown(html_content: str, url: str = "", title: str = "") -> str:
    """Convert HTML to clean Markdown format with improved content extraction"""
    try:
        soup = BeautifulSoup(html_content, "lxml")
        # Add content readability score
        readability_score = calculate_readability_score(soup)
        # Improved content extraction based on readability metrics
        main_content = extract_main_content(soup)
        # Clean up the HTML
        for element in soup.find_all(["script", "style", "iframe", "noscript"]):
            element.decompose()
        # Remove comments
        for comment in soup.find_all(text=lambda text: isinstance(text, Comment)):
            comment.extract()
        # Remove likely navigation elements
        for element in soup.find_all():
            if _is_navigation_element(element):
                element.decompose()
        # Extract title if not provided
        if not title and soup.title and soup.title.string:
            title = soup.title.string.strip()
        # Start with title as heading
        markdown = f"# {title}\n\n"
        if url:
            markdown += f"URL Source: {url}\n\n"
        if main_content:
            # Process headings
            for heading in main_content.find_all(["h1", "h2", "h3", "h4", "h5", "h6"]):
                level = int(heading.name[1])
                text = _clean_text(heading.get_text())
                if text:
                    markdown += f"\n{'#' * level} {text}\n\n"
            # Process paragraphs
            for p in main_content.find_all("p"):
                text = _clean_text(p.get_text())
                if text:
                    markdown += f"{text}\n\n"
            # Process unordered lists
            for ul in main_content.find_all("ul"):
                for li in ul.find_all("li", recursive=False):
                    text = _clean_text(li.get_text())
                    if text:
                        markdown += f"* {text}\n"
                markdown += "\n"
            # Process ordered lists
            for ol in main_content.find_all("ol"):
                for i, li in enumerate(ol.find_all("li", recursive=False), 1):
                    text = _clean_text(li.get_text())
                    if text:
                        markdown += f"{i}. {text}\n"
                markdown += "\n"
            # Process blockquotes
            for blockquote in main_content.find_all("blockquote"):
                text = _clean_text(blockquote.get_text())
                if text:
                    lines = text.splitlines()
                    for line in lines:
                        markdown += f"> {line}\n"
                    markdown += "\n"
            # Process tables
            for table in main_content.find_all("table"):
                markdown += _table_to_markdown(table) + "\n\n"
            # Process code blocks
            for pre in main_content.find_all("pre"):
                code = pre.get_text()
                markdown += f"```\n{code}\n```\n\n"
            # Process links
            for link in main_content.find_all("a", href=True):
                href = link.get("href")
                text = _clean_text(link.get_text())
                if (
                    text
                    and href
                    and not href.startswith("#")
                    and not href.startswith("javascript:")
                ):
                    # Use reference style links to keep the text clean
                    markdown = markdown.replace(text, f"[{text}]({href})")
        # Clean up excess whitespace and normalize line breaks
        markdown = re.sub(r"\n{3,}", "\n\n", markdown)
        # Add metadata about extraction quality
        metadata_section = (
            f"\n\n---\n*Content extracted with {int(readability_score * 100)}% confidence*"
        )
        markdown += metadata_section
        return markdown
    except Exception as e:
        logger.error(f"Error converting HTML to Markdown: {e}")
        # Return basic markdown if conversion fails
        return f"# {title}\n\nURL Source: {url}\n\n[Content extraction failed]"


def _table_to_markdown(table_tag) -> str:
    """Convert HTML table to Markdown table"""
    rows = []
    # Extract headers
    headers = []
    header_row = table_tag.find("thead")
    if header_row:
        th_elements = header_row.find_all(["th", "td"])
        headers = [_clean_text(th.get_text()) for th in th_elements]
    # If no headers found in thead, try the first tr
    if not headers:
        first_row = table_tag.find("tr")
        if first_row:
            th_elements = first_row.find_all(["th", "td"])
            headers = [_clean_text(th.get_text()) for th in th_elements]
    # Process the table body
    tbody = table_tag.find("tbody") or table_tag
    data_rows = []
    # Skip the first row if we already used it for headers
    start_idx = 1 if not table_tag.find("thead") and headers else 0
    for tr in tbody.find_all("tr")[start_idx:]:
        cells = [_clean_text(td.get_text()) for td in tr.find_all(["td", "th"])]
        if any(cells):  # Skip empty rows
            data_rows.append(cells)
    # Create the markdown table
    markdown_table = ""
    # Add headers if we have them
    if headers:
        markdown_table += "| " + " | ".join(headers) + " |\n"
        markdown_table += "|-" + "-|-".join(["-" * len(h) for h in headers]) + "-|\n"
    # Add data rows
    for row in data_rows:
        # Pad the row if necessary to match header length
        if headers and len(row) < len(headers):
            row.extend([""] * (len(headers) - len(row)))
        markdown_table += "| " + " | ".join(row) + " |\n"
    return markdown_table


def extract_structured_data(soup: BeautifulSoup) -> Dict[str, Any]:
    """Extract structured data from HTML page"""
    structured_data = {}
    # Extract JSON-LD
    json_ld_data = []
    for script in soup.find_all("script", type="application/ld+json"):
        try:
            data = json.loads(script.string)
            if isinstance(data, dict):
                json_ld_data.append(data)
            elif isinstance(data, list):
                json_ld_data.extend(data)
        except Exception as e:
            logger.error(f"Error parsing JSON-LD: {e}")
    if json_ld_data:
        structured_data["json_ld"] = json_ld_data
    # Extract microdata
    microdata = {}
    for element in soup.find_all(itemscope=True):
        if element.has_attr("itemtype"):
            item_type = element["itemtype"]
            item_props = {}
            # Extract properties
            for prop in element.find_all(itemprop=True):
                prop_name = prop["itemprop"]
                # Get property value based on tag type
                if prop.name == "meta":
                    prop_value = prop.get("content", "")
                elif prop.name == "img":
                    prop_value = prop.get("src", "")
                elif prop.name == "a":
                    prop_value = prop.get("href", "")
                elif prop.name == "time":
                    prop_value = prop.get("datetime", prop.get_text())
                else:
                    prop_value = prop.get_text().strip()
                item_props[prop_name] = prop_value
            if item_type not in microdata:
                microdata[item_type] = []
            microdata[item_type].append(item_props)
    if microdata:
        structured_data["microdata"] = microdata
    # Extract OpenGraph metadata
    og_data = {}
    for meta in soup.find_all("meta", property=re.compile(r"^og:")):
        prop = meta.get("property", "").replace("og:", "")
        content = meta.get("content", "")
        if prop and content:
            og_data[prop] = content
    if og_data:
        structured_data["opengraph"] = og_data
    # Extract Twitter card metadata
    twitter_data = {}
    for meta in soup.find_all("meta", attrs={"name": re.compile(r"^twitter:")}):
        prop = meta.get("name", "").replace("twitter:", "")
        content = meta.get("content", "")
        if prop and content:
            twitter_data[prop] = content
    if twitter_data:
        structured_data["twitter_card"] = twitter_data
    return structured_data


================================================================================
FILE: data\user_preferences\test_user.json
LANGUAGE: json
SIZE: 44 bytes
================================================================================

{
  "theme": "dark",
  "language": "en"
}

================================================================================
FILE: export.py
LANGUAGE: python
SIZE: 7281 bytes
================================================================================

import os
import sys
import argparse
from pathlib import Path
import datetime
import fnmatch
import re


def parse_arguments():
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(description="Export codebase to a single text file")
    parser.add_argument(
        "-d",
        "--directory",
        type=str,
        default=os.getcwd(),
        help="Root directory of the codebase (default: current directory)",
    )
    parser.add_argument(
        "-o",
        "--output",
        type=str,
        default=f'codebase_export_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}.txt',
        help="Output file path (default: codebase_export_<timestamp>.txt)",
    )
    parser.add_argument(
        "-e",
        "--extensions",
        type=str,
        default=".py,.js,.java,.cpp,.h,.html,.css,.md,.txt,.json,.yml,.ts,.ico,.idx,.keep,.pack,.rev,.sample",
        help="Comma-separated list of file extensions to include (default: common code files)",
    )
    parser.add_argument(
        "-x",
        "--exclude",
        type=str,
        default="node_modules,venv,.git,__pycache__,*.pyc,*.pyo,*.pyd,*.so,*.dll,*.exe",
        help="Comma-separated list of directories and file patterns to exclude",
    )
    parser.add_argument(
        "--max-size",
        type=int,
        default=1024 * 1024,
        help="Maximum file size to include in bytes (default: 1MB)",
    )
    parser.add_argument(
        "--include-line-numbers", action="store_true", help="Include line numbers in the output"
    )
    parser.add_argument(
        "--toc", action="store_true", help="Generate a table of contents at the beginning"
    )
    parser.add_argument(
        "--header",
        type=str,
        default="Codebase Export - {timestamp}",
        help="Header text for the export file. Use {timestamp} for current timestamp.",
    )
    return parser.parse_args()


def should_include_file(file_path, args):
    """Determine if a file should be included in the export."""
    # Check extension
    extensions = args.extensions.split(",")
    if not any(file_path.name.endswith(ext) for ext in extensions):
        return False
    # Check excluded patterns
    exclude_patterns = args.exclude.split(",")
    for pattern in exclude_patterns:
        if fnmatch.fnmatch(file_path.name, pattern):
            return False
    # Check if file is in excluded directory
    for pattern in exclude_patterns:
        if pattern in str(file_path):
            return False
    # Check file size
    if file_path.stat().st_size > args.max_size:
        print(f"Skipping large file: {file_path} ({file_path.stat().st_size} bytes)")
        return False
    return True


def get_file_language(file_path):
    """Determine the programming language based on file extension."""
    extension = file_path.suffix.lower()
    language_map = {
        ".py": "python",
        ".js": "javascript",
        ".jsx": "javascript",
        ".ts": "typescript",
        ".tsx": "typescript",
        ".java": "java",
        ".c": "c",
        ".cpp": "cpp",
        ".h": "cpp",
        ".hpp": "cpp",
        ".cs": "csharp",
        ".go": "go",
        ".rb": "ruby",
        ".php": "php",
        ".swift": "swift",
        ".kt": "kotlin",
        ".hs": "haskell",
        ".rs": "rust",
        ".html": "html",
        ".css": "css",
        ".scss": "scss",
        ".sass": "scss",
        ".json": "json",
        ".xml": "xml",
        ".yaml": "yaml",
        ".yml": "yaml",
        ".md": "markdown",
        ".sh": "bash",
        ".bat": "batch",
        ".ps1": "powershell",
        ".sql": "sql",
        ".r": "r",
        ".ico": "binary",
        ".idx": "binary",
        ".keep": "text",
        ".pack": "binary",
        ".rev": "text",
        ".sample": "text",
        ".txt": "text",
    }
    return language_map.get(extension, "text")


def scan_directory(root_dir, files_list, args):
    """Recursively scan directory and collect files to include."""
    root_path = Path(root_dir)
    exclude_dirs = [item for item in args.exclude.split(",") if not item.startswith("*")]
    for path in root_path.rglob("*"):
        if any(
            exclude_dir in str(path.relative_to(root_path))
            for exclude_dir in exclude_dirs
            if exclude_dir
        ):
            continue
        if path.is_file() and should_include_file(path, args):
            files_list.append(path)


def create_separator(length=80):
    """Create a separator line."""
    return "=" * length


def format_file_header(file_path, root_dir):
    """Format header for a file."""
    rel_path = os.path.relpath(file_path, root_dir)
    header = create_separator()
    header += f"\nFILE: {rel_path}\n"
    header += f"LANGUAGE: {get_file_language(file_path)}\n"
    header += f"SIZE: {file_path.stat().st_size} bytes\n"
    header += create_separator()
    return header


def generate_toc(files_list, root_dir):
    """Generate table of contents."""
    toc = "TABLE OF CONTENTS\n"
    toc += create_separator() + "\n"
    for i, file_path in enumerate(files_list, 1):
        rel_path = os.path.relpath(file_path, root_dir)
        toc += f"{i}. {rel_path}\n"
    toc += create_separator() + "\n\n"
    return toc


def export_codebase(args):
    """Export codebase files to a single text file."""
    root_dir = os.path.abspath(args.directory)
    output_file = args.output
    files_list = []
    print(f"Scanning directory: {root_dir}")
    scan_directory(root_dir, files_list, args)
    # Sort files alphabetically
    files_list.sort()
    print(f"Found {len(files_list)} files to export")
    with open(output_file, "w", encoding="utf-8") as f:
        # Write header
        header = args.header.replace(
            "{timestamp}", datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        )
        f.write(header + "\n\n")
        # Write table of contents if requested
        if args.toc:
            f.write(generate_toc(files_list, root_dir))
        # Process each file
        for file_path in files_list:
            try:
                # Write file header
                f.write(format_file_header(file_path, root_dir) + "\n\n")
                # Read and write file content with optional line numbers
                with open(file_path, "r", encoding="utf-8", errors="replace") as source_file:
                    if args.include_line_numbers:
                        for i, line in enumerate(source_file, 1):
                            f.write(f"{i:4d} | {line}")
                    else:
                        f.write(source_file.read())
                # Add newlines for spacing
                f.write("\n\n")
            except Exception as e:
                f.write(f"ERROR: Could not read file {file_path}: {str(e)}\n\n")
    print(f"Export completed successfully to: {output_file}")
    print(f"Total size: {os.path.getsize(output_file)} bytes")


def main():
    """Main entry point."""
    args = parse_arguments()
    export_codebase(args)


if __name__ == "__main__":
    main()


================================================================================
FILE: main.py
LANGUAGE: python
SIZE: 3013 bytes
================================================================================

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.openapi.utils import get_openapi
from app.api import filesystem, memory, git, scraper, documents
from app.utils.config import get_config

app = FastAPI(
    title="othertales System Tools",
    version="1.0.0",
    description="A unified server providing filesystem, memory, git, web scraping, and document management tools for LLMs via OpenWebUI.",
)
# Configure CORS specifically for Open WebUI compatibility
origins = [
    # In production, remove the wildcard "*" and list only trusted domains
    # "*",  # Too permissive for production
    "https://ai.othertales.co",
    "https://legal.othertales.co",
    "https://mixture.othertales.co",
    # Add more specific domains as needed
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "HEAD", "PATCH"],
    allow_headers=["*"],
    expose_headers=["Content-Length"],
    max_age=600,  # Cache CORS preflight requests
)
# Include routers with well-defined tags for better OpenAPI organization
app.include_router(filesystem.router, prefix="/fs", tags=["Filesystem"])
app.include_router(memory.router, prefix="/memory", tags=["Memory"])
app.include_router(git.router, prefix="/git", tags=["Git"])
app.include_router(scraper.router, prefix="/scraper", tags=["Web Scraper"])
app.include_router(documents.router, prefix="/docs", tags=["Document Management"])


# Custom OpenAPI schema generator optimized for Open WebUI
def custom_openapi():
    if app.openapi_schema:
        return app.openapi_schema
    openapi_schema = get_openapi(
        title="Unified Tools Server",
        version=app.version,
        description="Document storage and retrieval system with Git versioning, knowledge graph, and web scraping capabilities.",
        routes=app.routes,
    )
    # Add tool metadata for Open WebUI
    openapi_schema["info"]["x-logo"] = {
        "url": "https://cdn-icons-png.flaticon.com/512/8728/8728086.png"
    }
    # Add toolkit info for better Open WebUI integration
    openapi_schema["info"]["x-openwebui-toolkit"] = {
        "category": "document-management",
        "capabilities": ["document-storage", "web-scraping", "git-versioning", "memory"],
        "auth_required": False,
    }
    app.openapi_schema = openapi_schema
    return app.openapi_schema


# Set custom OpenAPI schema generator
app.openapi = custom_openapi


@app.get("/")
async def root():
    return {
        "message": "Unified Tools Server API",
        "services": ["filesystem", "memory", "git", "scraper", "documents"],
        "version": "1.0.0",
        "openapi_url": "/openapi.json",
    }


if __name__ == "__main__":
    import uvicorn

    config = get_config()
    uvicorn.run(
        "main:app", host=config.server_host, port=config.server_port, reload=config.dev_mode
    )


================================================================================
FILE: pyrightconfig.json
LANGUAGE: json
SIZE: 345 bytes
================================================================================

{
    "include": [
        "app",
        "tests"
    ],
    "exclude": [
        "**/node_modules",
        "**/__pycache__",
        "venv"
    ],
    "venvPath": ".",
    "venv": "venv",
    "reportMissingImports": "warning",
    "reportMissingTypeStubs": false,
    "pythonVersion": "3.9",
    "typeCheckingMode": "basic"
}


================================================================================
FILE: README.md
LANGUAGE: markdown
SIZE: 1578 bytes
================================================================================

# othertales unified openapi tools server
A comprehensive server that provides document storage, memory, Git versioning, and web scraping capabilities for LLMs via OpenWebUI.
## Features
- **Document Management**: Store and retrieve documents with Git versioning
- **Knowledge Graph**: Store structured data and track user preferences
- **Git Integration**: Version control for documents
- **Web Scraping**: Extract content from websites and convert to Markdown
- **OpenWebUI Integration**: Fully compatible with OpenWebUI
## Getting Started
### Installation
#### Using Docker
```bash
docker build -t unified-tools-server .
docker run -p 8000:8000 -v $(pwd)/data:/app/data unified-tools-server
```
#### Without Docker
```bash
# Install dependencies
pip install -r requirements.txt
# Install Playwright browsers
playwright install chromium
# Run the server
python main.py
```
### Configuration
Edit the `.env` file to configure the server:
```
# Server settings
SERVER_HOST=0.0.0.0
SERVER_PORT=8000
DEV_MODE=False
# Storage settings
ALLOWED_DIRS=./data,~/documents
MEMORY_FILE_PATH=./data/memory.json
# Git settings
DEFAULT_COMMIT_USERNAME=UnifiedTools
DEFAULT_COMMIT_EMAIL=tools@example.com
```
## API Documentation
When running, documentation is available at:
- OpenAPI JSON: http://localhost:8000/openapi.json
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc
## Usage with OpenWebUI
In OpenWebUI, add a new Tool with the following URL:
```
http://localhost:8000/openapi.json
```
## License
SEE EULA

================================================================================
FILE: requirements.txt
LANGUAGE: text
SIZE: 322 bytes
================================================================================

aiohttp==3.10.11
beautifulsoup4==4.13.3
boto3==1.37.23
docx==0.2.4
fastapi==0.115.12
networkx==3.4.2
numpy==2.2.4
playwright==1.51.0
pydantic==2.11.2
pydantic_settings==2.8.1
pytest==8.3.5
python-dotenv==1.1.0
Requests==2.32.3
sentence_transformers==4.0.1
uvicorn==0.34.0
weasyprint==65.0
gitpython==3.1.44

================================================================================
FILE: tests\__init__.py
LANGUAGE: python
SIZE: 80 bytes
================================================================================

# This file is intentionally left empty to make the directory a Python package


================================================================================
FILE: tests\api\__init__.py
LANGUAGE: python
SIZE: 80 bytes
================================================================================

# This file is intentionally left empty to make the directory a Python package


================================================================================
FILE: tests\api\test_filesystem_api.py
LANGUAGE: python
SIZE: 6318 bytes
================================================================================

import pytest
from unittest.mock import patch, MagicMock
from fastapi.testclient import TestClient
from fastapi import FastAPI
import os
import io

from app.api.filesystem import router
from app.models.filesystem import DirectoryListingResponse

# Create test app
app = FastAPI()
app.include_router(router, prefix="/fs")
client = TestClient(app)


class TestFilesystemAPI:
    @patch("app.api.filesystem.filesystem_service")
    def test_read_file(self, mock_fs_service):
        # Mock the service
        mock_fs_service.read_file.return_value = "File content"

        # Send request
        response = client.post("/fs/read", json={"path": "/test/file.txt", "storage": "local"})

        # Verify response
        assert response.status_code == 200
        assert response.text == "File content"
        mock_fs_service.read_file.assert_called_once()

    @patch("app.api.filesystem.filesystem_service")
    def test_write_file(self, mock_fs_service):
        # Mock the service
        mock_fs_service.write_file.return_value = "Successfully wrote to /test/file.txt"

        # Send request
        response = client.post(
            "/fs/write",
            json={"path": "/test/file.txt", "content": "New content", "storage": "local"},
        )

        # Verify response
        assert response.status_code == 200
        assert response.text == "Successfully wrote to /test/file.txt"
        mock_fs_service.write_file.assert_called_once()

    @patch("app.api.filesystem.filesystem_service")
    def test_list_directory(self, mock_fs_service):
        # Mock the service
        mock_fs_service.list_directory.return_value = {
            "path": "/test",
            "items": [
                {
                    "name": "file1.txt",
                    "path": "file1.txt",
                    "type": "file",
                    "size": 100,
                    "last_modified": 1609459200,
                },
                {
                    "name": "subdir",
                    "path": "subdir",
                    "type": "directory",
                    "size": None,
                    "last_modified": None,
                },
            ],
        }

        # Send request
        response = client.post(
            "/fs/list", json={"path": "/test", "storage": "local", "recursive": False}
        )

        # Verify response
        assert response.status_code == 200
        assert response.json()["path"] == "/test"
        assert len(response.json()["items"]) == 2
        assert response.json()["items"][0]["name"] == "file1.txt"
        mock_fs_service.list_directory.assert_called_once()

    @patch("app.api.filesystem.filesystem_service")
    def test_search_files(self, mock_fs_service):
        # Mock the service
        mock_fs_service.search_files.return_value = ["/test/file1.txt", "/test/subdir/file2.txt"]

        # Send request
        response = client.post(
            "/fs/search", json={"path": "/test", "pattern": "*.txt", "storage": "local"}
        )

        # Verify response
        assert response.status_code == 200
        assert len(response.json()) == 2
        assert "/test/file1.txt" in response.json()
        mock_fs_service.search_files.assert_called_once()

    @patch("app.api.filesystem.filesystem_service")
    def test_create_directory(self, mock_fs_service):
        # Mock the service
        mock_fs_service.create_directory.return_value = (
            "Successfully created directory /test/newdir"
        )

        # Send request
        response = client.post("/fs/mkdir", json={"path": "/test/newdir", "storage": "local"})

        # Verify response
        assert response.status_code == 200
        assert response.text == "Successfully created directory /test/newdir"
        mock_fs_service.create_directory.assert_called_once()

    @patch("app.api.filesystem.filesystem_service")
    def test_delete_file(self, mock_fs_service):
        # Mock the service
        mock_fs_service.delete_file.return_value = "Successfully deleted /test/file.txt"

        # Send request
        response = client.post("/fs/delete", json={"path": "/test/file.txt", "storage": "local"})

        # Verify response
        assert response.status_code == 200
        assert response.text == "Successfully deleted /test/file.txt"
        mock_fs_service.delete_file.assert_called_once()

    @patch("app.api.filesystem.filesystem_service")
    def test_upload_file(self, mock_fs_service):
        # Mock the service
        mock_fs_service.write_file_binary.return_value = "Successfully wrote to /test/uploaded.txt"
        mock_fs_service.invalidate_cache.return_value = None

        # Create test file
        test_file = io.BytesIO(b"Test file content")

        # Send request
        response = client.post(
            "/fs/upload",
            files={"file": ("uploaded.txt", test_file)},
            data={"path": "/test", "storage": "local"},
        )

        # Verify response
        assert response.status_code == 200
        assert "Successfully wrote to" in response.text
        mock_fs_service.write_file_binary.assert_called_once()

    @patch("app.api.filesystem.filesystem_service")
    def test_read_binary_file(self, mock_fs_service):
        # Mock the service
        mock_fs_service.read_file_binary.return_value = b"Binary content"

        # Send request
        response = client.post(
            "/fs/read-binary", json={"path": "/test/binary.bin", "storage": "local"}
        )

        # Verify response
        assert response.status_code == 200
        assert response.content == b"Binary content"
        mock_fs_service.read_file_binary.assert_called_once()

    @patch("app.api.filesystem.filesystem_service")
    def test_file_exists(self, mock_fs_service):
        # Mock the service
        mock_fs_service.file_exists.return_value = True

        # Send request
        response = client.post("/fs/exists", json={"path": "/test/file.txt", "storage": "local"})

        # Verify response
        assert response.status_code == 200
        assert response.json() is True
        mock_fs_service.file_exists.assert_called_once()


================================================================================
FILE: tests\api\test_git_api.py
LANGUAGE: python
SIZE: 6638 bytes
================================================================================

import pytest
from unittest.mock import patch, MagicMock
from fastapi.testclient import TestClient
from fastapi import FastAPI

from app.api.git import router

# Create test app
app = FastAPI()
app.include_router(router, prefix="/git")
client = TestClient(app)


class TestGitAPI:
    @patch("app.api.git.git_service")
    def test_get_status(self, mock_git_service):
        # Mock the service
        mock_git_service.get_status.return_value = {
            "branch": "main",
            "clean": True,
            "untracked": [],
            "modified": [],
            "staged": [],
        }

        # Send request
        response = client.post("/git/status", json={"repo_path": "/test/repo"})

        # Verify response
        assert response.status_code == 200
        assert response.json()["branch"] == "main"
        assert response.json()["clean"] is True
        mock_git_service.get_status.assert_called_once()

    @patch("app.api.git.git_service")
    def test_get_diff(self, mock_git_service):
        # Mock the service
        mock_git_service.get_diff.return_value = "diff --git a/file.txt b/file.txt\n+New content"

        # Send request
        response = client.post(
            "/git/diff", json={"repo_path": "/test/repo", "file_path": "file.txt"}
        )

        # Verify response
        assert response.status_code == 200
        assert "diff --git" in response.text
        mock_git_service.get_diff.assert_called_once()

    @patch("app.api.git.git_service")
    def test_add_files(self, mock_git_service):
        # Mock the service
        mock_git_service.add_files.return_value = "Files staged successfully"

        # Send request
        response = client.post(
            "/git/add", json={"repo_path": "/test/repo", "files": ["file1.txt", "file2.txt"]}
        )

        # Verify response
        assert response.status_code == 200
        assert response.text == "Files staged successfully"
        mock_git_service.add_files.assert_called_once()

    @patch("app.api.git.git_service")
    def test_commit_changes(self, mock_git_service):
        # Mock the service
        mock_git_service.commit_changes.return_value = "Committed changes with hash abc123"

        # Send request
        response = client.post(
            "/git/commit",
            json={
                "repo_path": "/test/repo",
                "message": "Test commit",
                "author_name": "Test User",
                "author_email": "test@example.com",
            },
        )

        # Verify response
        assert response.status_code == 200
        assert "Committed changes with hash" in response.text
        mock_git_service.commit_changes.assert_called_once()

    @patch("app.api.git.git_service")
    def test_reset_changes(self, mock_git_service):
        # Mock the service
        mock_git_service.reset_changes.return_value = "All staged changes reset"

        # Send request
        response = client.post("/git/reset", json={"repo_path": "/test/repo"})

        # Verify response
        assert response.status_code == 200
        assert response.text == "All staged changes reset"
        mock_git_service.reset_changes.assert_called_once()

    @patch("app.api.git.git_service")
    def test_get_log(self, mock_git_service):
        # Mock the service
        mock_git_service.get_log.return_value = {
            "commits": [
                {
                    "hash": "abc123",
                    "message": "Test commit",
                    "author": "Test User",
                    "date": "2023-01-01 10:00:00",
                }
            ]
        }

        # Send request
        response = client.post("/git/log", json={"repo_path": "/test/repo", "max_count": 10})

        # Verify response
        assert response.status_code == 200
        assert len(response.json()["commits"]) == 1
        assert response.json()["commits"][0]["hash"] == "abc123"
        mock_git_service.get_log.assert_called_once()

    @patch("app.api.git.git_service")
    def test_create_branch(self, mock_git_service):
        # Mock the service
        mock_git_service.create_branch.return_value = "Created branch 'feature'"

        # Send request
        response = client.post(
            "/git/branch",
            json={"repo_path": "/test/repo", "branch_name": "feature", "base_branch": "main"},
        )

        # Verify response
        assert response.status_code == 200
        assert response.text == "Created branch 'feature'"
        mock_git_service.create_branch.assert_called_once()

    @patch("app.api.git.git_service")
    def test_checkout_branch(self, mock_git_service):
        # Mock the service
        mock_git_service.checkout_branch.return_value = "Switched to branch 'feature'"

        # Send request
        response = client.post(
            "/git/checkout",
            json={"repo_path": "/test/repo", "branch_name": "feature", "create": False},
        )

        # Verify response
        assert response.status_code == 200
        assert response.text == "Switched to branch 'feature'"
        mock_git_service.checkout_branch.assert_called_once()

    @patch("app.api.git.git_service")
    def test_clone_repo(self, mock_git_service):
        # Mock the service
        mock_git_service.clone_repo.return_value = "Cloned repository to '/test/cloned'"

        # Send request
        response = client.post(
            "/git/clone",
            json={"repo_url": "https://github.com/example/repo.git", "local_path": "/test/cloned"},
        )

        # Verify response
        assert response.status_code == 200
        assert response.text == "Cloned repository to '/test/cloned'"
        mock_git_service.clone_repo.assert_called_once()

    @patch("app.api.git.git_service")
    def test_batch_commit(self, mock_git_service):
        # Mock the service
        mock_git_service.batch_commit.return_value = ["abc123", "def456"]

        # Send request
        response = client.post(
            "/git/batch-commit",
            json={
                "repo_path": "/test/repo",
                "file_groups": [["file1.txt", "file2.txt"], ["file3.txt"]],
                "message_template": "Batch commit",
            },
        )

        # Verify response
        assert response.status_code == 200
        assert len(response.json()) == 2
        assert response.json()[0] == "abc123"
        mock_git_service.batch_commit.assert_called_once()


================================================================================
FILE: tests\core\__init__.py
LANGUAGE: python
SIZE: 80 bytes
================================================================================

# This file is intentionally left empty to make the directory a Python package


================================================================================
FILE: tests\core\test_documents_service.py
LANGUAGE: python
SIZE: 8199 bytes
================================================================================

import pytest
import os
import shutil
import json
import tempfile
from pathlib import Path
from unittest.mock import patch, MagicMock

from app.core.documents_service import DocumentsService
from app.models.documents import DocumentType


@pytest.fixture
def temp_dir():
    # Create a temporary directory for testing
    temp_dir = tempfile.mkdtemp()
    yield temp_dir
    # Cleanup
    shutil.rmtree(temp_dir)


@pytest.fixture
def documents_service(temp_dir):
    # Mock dependencies
    with (
        patch("app.core.documents_service.get_config") as mock_config,
        patch("app.core.documents_service.GitService") as mock_git_service,
    ):

        # Set up GitService mock
        git_service_instance = MagicMock()
        mock_git_service.return_value = git_service_instance

        # Configure service
        service = DocumentsService(base_path=temp_dir)

        # Ensure get_status always returns clean repo status
        git_service_instance.get_status.return_value = {
            "branch": "main",
            "clean": True,
            "untracked": [],
            "modified": [],
            "staged": [],
        }

        yield service


class TestDocumentsService:
    def test_create_document(self, documents_service, temp_dir):
        # Test creating a document
        doc = documents_service.create_document(
            title="Test Document",
            content="This is test content.",
            document_type=DocumentType.GENERIC,
            metadata={"source": "test"},
            tags=["test", "document"],
        )

        # Verify document was created
        assert doc["title"] == "Test Document"
        assert doc["document_type"] == DocumentType.GENERIC.value
        assert "source" in doc["metadata"]
        assert "test" in doc["tags"]

        # Check that document ID was generated
        assert doc["id"] is not None

        # Verify file exists
        doc_path = Path(temp_dir) / DocumentType.GENERIC.value / f"{doc['id']}.md"
        assert doc_path.exists()

        # Verify content
        content = doc_path.read_text()
        assert "Test Document" in content
        assert "This is test content." in content

    def test_get_document(self, documents_service):
        # Create a document
        doc = documents_service.create_document(
            title="Get Test",
            content="Content for retrieval test",
            document_type=DocumentType.GENERIC,
        )

        # Get the document
        retrieved = documents_service.get_document(doc["id"])

        # Verify retrieval
        assert retrieved["id"] == doc["id"]
        assert retrieved["title"] == "Get Test"
        assert "Content for retrieval" in retrieved["content_preview"]

    def test_update_document(self, documents_service):
        # Create a document
        doc = documents_service.create_document(
            title="Original Title",
            content="Original content",
            document_type=DocumentType.GENERIC,
            tags=["original"],
        )

        # Update the document
        updated = documents_service.update_document(
            doc_id=doc["id"],
            title="Updated Title",
            content="Updated content",
            tags=["updated", "document"],
        )

        # Verify update
        assert updated["title"] == "Updated Title"
        assert "updated" in updated["tags"]

        # Check the document content
        content = documents_service.get_document_content(doc["id"])
        assert content["content"] == "Updated content"

    def test_delete_document(self, documents_service):
        # Create a document
        doc = documents_service.create_document(
            title="To Delete",
            content="This document will be deleted",
            document_type=DocumentType.GENERIC,
        )

        # Delete the document
        result = documents_service.delete_document(doc["id"])
        assert result is True

        # Verify document is gone
        assert documents_service.get_document(doc["id"]) is None

        # Verify file is removed
        doc_path = (
            Path(documents_service.base_path) / DocumentType.GENERIC.value / f"{doc['id']}.md"
        )
        assert not doc_path.exists()

    def test_search_documents(self, documents_service):
        # Create test documents
        docs = []
        # Document 1
        docs.append(
            documents_service.create_document(
                title="Search Test One",
                content="This document has specific content to find.",
                document_type=DocumentType.GENERIC,
                tags=["search", "test"],
            )
        )

        # Document 2
        docs.append(
            documents_service.create_document(
                title="Search Test Two",
                content="Another document with different content.",
                document_type=DocumentType.GENERIC,
                tags=["search", "different"],
            )
        )

        # Document 3 (with different type)
        docs.append(
            documents_service.create_document(
                title="Different Type",
                content="This has a different document type.",
                document_type=DocumentType.WEBPAGE,
                tags=["webpage"],
            )
        )

        # Search by content
        results = documents_service.search_documents("specific content")
        assert len(results) == 1
        assert results[0]["id"] == docs[0]["id"]

        # Search by tag
        results = documents_service.search_documents("", tags=["search"])
        assert len(results) == 2

        # Search by document type
        results = documents_service.search_documents("", doc_type=DocumentType.WEBPAGE.value)
        assert len(results) == 1
        assert results[0]["id"] == docs[2]["id"]

    def test_get_document_versions(self, documents_service):
        # Create a document
        doc = documents_service.create_document(
            title="Version Test", content="Initial version", document_type=DocumentType.GENERIC
        )

        # Mock git log response
        documents_service.git_service.get_log.return_value = {
            "commits": [
                {
                    "hash": "abc123",
                    "message": "Updated document",
                    "author": "Test User",
                    "date": "2023-01-02 10:00:00",
                },
                {
                    "hash": "def456",
                    "message": "Created document",
                    "author": "Test User",
                    "date": "2023-01-01 10:00:00",
                },
            ]
        }

        # Get document versions
        versions = documents_service.get_document_versions(doc["id"])

        # Verify versions
        assert len(versions) == 2
        assert versions[0]["hash"] == "abc123"
        assert versions[1]["hash"] == "def456"
        assert versions[1]["message"] == "Created document"

    @patch("app.core.documents_service.markdown")
    @patch("weasyprint.HTML")
    def test_convert_document_format(self, mock_weasyprint, mock_markdown, documents_service):
        # Create a document
        doc = documents_service.create_document(
            title="Convert Format Test",
            content="# Heading\nContent to convert",
            document_type=DocumentType.GENERIC,
        )

        # Mock markdown conversion
        mock_markdown.markdown.return_value = "<h1>Heading</h1><p>Content to convert</p>"

        # Mock PDF generation
        mock_pdf = MagicMock()
        mock_pdf.write_pdf.return_value = b"PDF content"
        mock_weasyprint.return_value = mock_pdf

        # Convert document to PDF
        result = documents_service.convert_document_format(doc["id"], "pdf")

        # Verify conversion
        assert mock_markdown.markdown.called
        assert mock_weasyprint.called
        assert result == b"PDF content"


================================================================================
FILE: tests\core\test_filesystem_service.py
LANGUAGE: python
SIZE: 6609 bytes
================================================================================

import os
import shutil
import tempfile
from pathlib import Path
from unittest.mock import patch, MagicMock

import pytest

from app.core.filesystem_service import FilesystemService


@pytest.fixture
def test_temp_dir():
    # Create a temporary directory for testing
    temp_dir = tempfile.mkdtemp()
    yield temp_dir
    # Cleanup
    shutil.rmtree(temp_dir)


@pytest.fixture
def fs_service(test_temp_dir):
    # Configure service with test paths
    with patch("app.core.filesystem_service.get_config") as mock_config:
        mock_config.return_value.allowed_directories = [test_temp_dir]
        mock_config.return_value.file_cache_enabled = False
        mock_config.return_value.s3_access_key = None
        mock_config.return_value.s3_secret_key = None
        service = FilesystemService()
        yield service


class TestFilesystemService:
    def test_normalize_path(self, fs_service, test_temp_dir):
        # Test path normalization
        test_path = os.path.join(test_temp_dir, "test_file.txt")
        normalized = fs_service.normalize_path(test_path)
        assert str(normalized) == str(Path(test_path).resolve())

    def test_read_write_file(self, fs_service, test_temp_dir):
        # Test writing and reading a file
        test_path = os.path.join(test_temp_dir, "test_file.txt")
        content = "Test content"

        # Write file
        result = fs_service.write_file(test_path, content)
        assert "Successfully wrote to" in result

        # Read file
        read_content = fs_service.read_file(test_path)
        assert read_content == content

    def test_write_read_binary_file(self, fs_service, test_temp_dir):
        # Test writing and reading a binary file
        test_path = os.path.join(test_temp_dir, "test_binary.bin")
        content = b"\x00\x01\x02\x03"

        # Write binary file
        result = fs_service.write_file_binary(test_path, content)
        assert "Successfully wrote to" in result

        # Read binary file
        read_content = fs_service.read_file_binary(test_path)
        assert read_content == content

    def test_create_list_directory(self, fs_service, test_temp_dir):
        # Test creating and listing directories
        test_dir = os.path.join(test_temp_dir, "test_dir")

        # Create directory
        result = fs_service.create_directory(test_dir)
        assert "Successfully created directory" in result
        assert os.path.isdir(test_dir)

        # Create a file in the directory
        test_file = os.path.join(test_dir, "test_file.txt")
        fs_service.write_file(test_file, "Test content")

        # List directory
        listing = fs_service.list_directory(test_dir)
        assert listing["path"] == test_dir
        assert len(listing["items"]) == 1
        assert listing["items"][0]["name"] == "test_file.txt"
        assert listing["items"][0]["type"] == "file"

    def test_delete_file(self, fs_service, test_temp_dir):
        # Test deleting a file
        test_file = os.path.join(test_temp_dir, "file_to_delete.txt")
        fs_service.write_file(test_file, "Delete me")

        # Verify file exists
        assert os.path.exists(test_file)

        # Delete file
        result = fs_service.delete_file(test_file)
        assert "Successfully deleted" in result
        assert not os.path.exists(test_file)

    def test_search_files(self, fs_service, test_temp_dir):
        # Create test files
        os.makedirs(os.path.join(test_temp_dir, "dir1"))
        os.makedirs(os.path.join(test_temp_dir, "dir2"))
        fs_service.write_file(os.path.join(test_temp_dir, "dir1", "file1.txt"), "content")
        fs_service.write_file(os.path.join(test_temp_dir, "dir2", "file2.txt"), "content")
        fs_service.write_file(os.path.join(test_temp_dir, "dir1", "file.md"), "markdown")

        # Search for txt files
        results = fs_service.search_files(test_temp_dir, "*.txt")
        assert len(results) == 2
        assert any("file1.txt" in r for r in results)
        assert any("file2.txt" in r for r in results)

        # Search with specific pattern
        results = fs_service.search_files(os.path.join(test_temp_dir, "dir1"), "*.md")
        assert len(results) == 1
        assert "file.md" in results[0]

    @patch("boto3.client")
    @patch("boto3.resource")
    def test_s3_integration(self, mock_s3_resource, mock_s3_client, test_temp_dir):
        # Mock the S3 client and resource
        mock_client = MagicMock()
        mock_resource = MagicMock()
        mock_s3_client.return_value = mock_client
        mock_s3_resource.return_value = mock_resource

        # Mock response for list_objects_v2
        mock_client.list_objects_v2.return_value = {
            "CommonPrefixes": [{"Prefix": "folder/"}],
            "Contents": [
                {
                    "Key": "test.txt",
                    "Size": 100,
                    "LastModified": MagicMock(timestamp=lambda: 1234567890),
                }
            ],
        }

        # Create service with S3 config
        with patch("app.core.filesystem_service.get_config") as mock_config:
            mock_config.return_value.allowed_directories = [test_temp_dir]
            mock_config.return_value.file_cache_enabled = False
            mock_config.return_value.s3_access_key = "test_key"
            mock_config.return_value.s3_secret_key = "test_secret"
            mock_config.return_value.s3_region = "us-east-1"

            service = FilesystemService()

            # Test S3 directory listing
            service.list_directory("", storage="s3", bucket="test-bucket")
            assert mock_client.list_objects_v2.called

            # Test S3 file writing
            service.write_file("test.txt", "content", storage="s3", bucket="test-bucket")
            mock_client.put_object.assert_called_once()

    def test_file_exists(self, fs_service, test_temp_dir):
        # Test file existence check
        test_file = os.path.join(test_temp_dir, "existing_file.txt")
        test_dir = os.path.join(test_temp_dir, "existing_dir")

        # Create file and directory
        fs_service.write_file(test_file, "I exist")
        os.makedirs(test_dir)

        # Check existing file
        assert fs_service.file_exists(test_file)
        assert fs_service.file_exists(test_dir)

        # Check non-existing file
        assert not fs_service.file_exists(os.path.join(test_temp_dir, "nonexistent.txt"))


================================================================================
FILE: tests\core\test_git_service.py
LANGUAGE: python
SIZE: 5394 bytes
================================================================================

import pytest
import os
import shutil
import tempfile
from pathlib import Path
from unittest.mock import patch, MagicMock

from app.core.git_service import GitService


@pytest.fixture
def temp_dir():
    # Create a temporary directory for testing
    temp_dir = tempfile.mkdtemp()
    yield temp_dir
    # Cleanup
    shutil.rmtree(temp_dir)


@pytest.fixture
def git_service():
    service = GitService()
    yield service


@pytest.fixture
def git_repo(temp_dir, git_service):
    # Initialize a Git repository for testing
    repo_path = os.path.join(temp_dir, "test_repo")
    os.makedirs(repo_path)

    # Initialize git repo
    from git import Repo

    Repo.init(repo_path)

    # Create a test file and commit it
    test_file = os.path.join(repo_path, "test.txt")
    with open(test_file, "w") as f:
        f.write("Initial content")

    repo = Repo(repo_path)
    repo.git.add("test.txt")
    repo.git.config("user.email", "test@example.com")
    repo.git.config("user.name", "Test User")
    repo.git.commit("-m", "Initial commit")

    yield repo_path


class TestGitService:
    def test_get_status(self, git_service, git_repo):
        # Test getting repository status
        status = git_service.get_status(git_repo)

        assert "branch" in status
        assert status["clean"] is True
        assert len(status["untracked"]) == 0

    def test_add_and_commit(self, git_service, git_repo):
        # Create a new file
        new_file = os.path.join(git_repo, "new_file.txt")
        with open(new_file, "w") as f:
            f.write("New file content")

        # Add the file
        result = git_service.add_files(git_repo, ["new_file.txt"])
        assert "staged successfully" in result

        # Commit the file
        commit_result = git_service.commit_changes(
            git_repo, "Test commit", author_name="Test Author", author_email="test@example.com"
        )

        assert "Committed changes with hash" in commit_result

        # Check status after commit
        status = git_service.get_status(git_repo)
        assert status["clean"] is True

    def test_get_log(self, git_service, git_repo):
        # Test getting commit log
        log = git_service.get_log(git_repo)

        assert "commits" in log
        assert len(log["commits"]) > 0
        assert "hash" in log["commits"][0]
        assert "message" in log["commits"][0]
        assert "Initial commit" in log["commits"][0]["message"]

    def test_create_checkout_branch(self, git_service, git_repo):
        # Create a new branch
        result = git_service.create_branch(git_repo, "test-branch")
        assert "Created branch" in result

        # Checkout the branch
        checkout_result = git_service.checkout_branch(git_repo, "test-branch")
        assert "Switched to branch" in checkout_result

        # Verify current branch
        status = git_service.get_status(git_repo)
        assert status["branch"] == "test-branch"

    def test_reset_changes(self, git_service, git_repo):
        # Create and stage a new file
        new_file = os.path.join(git_repo, "to_reset.txt")
        with open(new_file, "w") as f:
            f.write("Content to reset")

        git_service.add_files(git_repo, ["to_reset.txt"])

        # Reset changes
        result = git_service.reset_changes(git_repo)
        assert "reset" in result

        # Verify status
        status = git_service.get_status(git_repo)
        assert "to_reset.txt" in status["untracked"]

    def test_get_diff(self, git_service, git_repo):
        # Modify a file
        test_file = os.path.join(git_repo, "test.txt")
        with open(test_file, "w") as f:
            f.write("Initial content\nModified content")

        # Get diff
        diff = git_service.get_diff(git_repo)

        assert "diff" in diff
        assert "Modified content" in diff

    def test_remove_file(self, git_service, git_repo):
        # Test removing a file
        result = git_service.remove_file(git_repo, "test.txt")
        assert "removed" in result

        # Verify file is gone
        assert not os.path.exists(os.path.join(git_repo, "test.txt"))

    @patch("git.Repo.clone_from")
    def test_clone_repo(self, mock_clone, git_service, temp_dir):
        # Test cloning a repository
        clone_path = os.path.join(temp_dir, "cloned_repo")

        git_service.clone_repo("https://github.com/example/repo.git", clone_path)

        mock_clone.assert_called_once()
        assert "https://github.com/example/repo.git" in mock_clone.call_args[0]

    def test_batch_commit(self, git_service, git_repo):
        # Create multiple files
        files = []
        for i in range(3):
            file_path = os.path.join(git_repo, f"batch_file_{i}.txt")
            with open(file_path, "w") as f:
                f.write(f"Batch file {i} content")
            files.append(f"batch_file_{i}.txt")

        # Batch commit in groups
        file_groups = [files[0:2], files[2:]]
        result = git_service.batch_commit(git_repo, file_groups, "Batch commit")

        assert len(result) == 2  # Two commit hashes

        # Verify log
        log = git_service.get_log(git_repo)
        assert "Batch commit" in log["commits"][0]["message"]


================================================================================
FILE: tests\core\test_memory_service.py
LANGUAGE: python
SIZE: 7661 bytes
================================================================================

import pytest
import os
import shutil
import json
from pathlib import Path
from unittest.mock import patch, MagicMock

from app.core.memory_service import MemoryService
from app.models.memory import Entity, Relation, KnowledgeGraph


@pytest.fixture
def temp_dir():
    # Create a temporary directory for testing
    temp_dir = Path("./test_data")
    temp_dir.mkdir(exist_ok=True)
    yield temp_dir
    # Cleanup
    shutil.rmtree(temp_dir)


@pytest.fixture
def memory_service(temp_dir):
    # Create a memory service with test file
    memory_file = temp_dir / "test_memory.json"

    with patch("app.core.memory_service.get_config") as mock_config:
        mock_config.return_value.use_graph_db = False
        service = MemoryService(str(memory_file))
        yield service


class TestMemoryService:
    def test_create_entities(self, memory_service):
        # Test creating entities
        entities = [
            {"name": "Person1", "entity_type": "person", "properties": {"age": 30}},
            {"name": "Place1", "entity_type": "location", "properties": {"country": "USA"}},
        ]

        result = memory_service.create_entities(entities)
        assert len(result) == 2
        assert result[0]["name"] == "Person1"
        assert result[1]["name"] == "Place1"

        # Test retrieving entities
        all_entities = memory_service.get_entities()
        assert len(all_entities) == 2
        assert any(e["name"] == "Person1" for e in all_entities)

    def test_create_relations(self, memory_service):
        # First create some entities
        entities = [
            {"name": "Person1", "entity_type": "person"},
            {"name": "Place1", "entity_type": "location"},
        ]
        memory_service.create_entities(entities)

        # Now create a relation between them
        relations = [{"from": "Person1", "to": "Place1", "relation_type": "visited"}]

        result = memory_service.create_relations(relations)
        assert len(result) == 1
        assert result[0]["from_"] == "Person1"
        assert result[0]["to"] == "Place1"
        assert result[0]["relation_type"] == "visited"

        # Verify the relation exists
        all_relations = memory_service.get_relations()
        assert len(all_relations) == 1
        assert all_relations[0]["from_"] == "Person1"

    def test_query_graph(self, memory_service):
        # Create test data
        memory_service.create_entities(
            [
                {"name": "Alice", "entity_type": "person"},
                {"name": "Bob", "entity_type": "person"},
                {"name": "CompanyX", "entity_type": "company"},
            ]
        )

        memory_service.create_relations(
            [
                {"from": "Alice", "to": "CompanyX", "relation_type": "works_at"},
                {"from": "Bob", "to": "CompanyX", "relation_type": "works_at"},
            ]
        )

        # Query by entity type
        people = memory_service.query_entities(entity_type="person")
        assert len(people) == 2
        assert any(p["name"] == "Alice" for p in people)
        assert any(p["name"] == "Bob" for p in people)

        # Query relations
        company_relations = memory_service.query_relations(to_entity="CompanyX")
        assert len(company_relations) == 2

    def test_user_preferences(self, memory_service):
        # Test setting preferences
        user_id = "test_user"
        prefs = {"theme": "dark", "language": "en"}

        # Set preferences
        result = memory_service.set_user_preference(user_id, prefs)
        assert result == prefs

        # Get preferences
        retrieved = memory_service.get_user_preference(user_id)
        assert retrieved == prefs

        # Update preferences
        updated_prefs = {"theme": "light", "font_size": "large"}
        result = memory_service.set_user_preference(user_id, updated_prefs)
        assert result["theme"] == "light"
        assert result["language"] == "en"  # Should preserve existing values
        assert result["font_size"] == "large"  # Should add new values

    def test_delete_entities(self, memory_service):
        # Create test entities
        memory_service.create_entities(
            [
                {"name": "ToDelete1", "entity_type": "test"},
                {"name": "ToDelete2", "entity_type": "test"},
                {"name": "ToKeep", "entity_type": "test"},
            ]
        )

        # Create some relations
        memory_service.create_relations(
            [{"from": "ToDelete1", "to": "ToKeep", "relation_type": "test_relation"}]
        )

        # Delete entities
        result = memory_service.delete_entities(["ToDelete1", "ToDelete2"])
        assert result["entities_removed"] == 2
        assert result["relations_removed"] == 1

        # Verify deletion
        remaining = memory_service.get_entities()
        assert len(remaining) == 1
        assert remaining[0]["name"] == "ToKeep"

    def test_delete_relations(self, memory_service):
        # Create test data
        memory_service.create_entities(
            [
                {"name": "E1", "entity_type": "test"},
                {"name": "E2", "entity_type": "test"},
                {"name": "E3", "entity_type": "test"},
            ]
        )

        memory_service.create_relations(
            [
                {"from": "E1", "to": "E2", "relation_type": "rel1"},
                {"from": "E1", "to": "E3", "relation_type": "rel2"},
                {"from": "E2", "to": "E3", "relation_type": "rel3"},
            ]
        )

        # Delete one relation
        result = memory_service.delete_relations(
            [{"from": "E1", "to": "E2", "relation_type": "rel1"}]
        )

        assert result["relations_removed"] == 1

        # Verify remaining relations
        remaining = memory_service.get_relations()
        assert len(remaining) == 2

    def test_entity_connections(self, memory_service):
        # Create test data
        memory_service.create_entities(
            [
                {"name": "Central", "entity_type": "test"},
                {"name": "Connected1", "entity_type": "test"},
                {"name": "Connected2", "entity_type": "test"},
            ]
        )

        memory_service.create_relations(
            [
                {"from": "Central", "to": "Connected1", "relation_type": "outgoing"},
                {"from": "Connected2", "to": "Central", "relation_type": "incoming"},
            ]
        )

        # Get connections for Central
        connections = memory_service.get_entity_connections("Central")

        assert connections["name"] == "Central"
        assert len(connections["connections"]["incoming"]) == 1
        assert len(connections["connections"]["outgoing"]) == 1
        assert connections["connections"]["incoming"][0]["from"] == "Connected2"
        assert connections["connections"]["outgoing"][0]["to"] == "Connected1"

    @patch("networkx.DiGraph")
    def test_graph_db_mode(self, mock_digraph, temp_dir):
        # Test when using graph database mode
        memory_file = temp_dir / "graph_db_test.json"

        with patch("app.core.memory_service.get_config") as mock_config:
            mock_config.return_value.use_graph_db = True

            # This should initialize the networkx graph
            service = MemoryService(str(memory_file))

            # Verify DiGraph was created
            assert mock_digraph.called


================================================================================
FILE: tests\core\test_scraper_service.py
LANGUAGE: python
SIZE: 9136 bytes
================================================================================

import pytest
import os
import shutil
import json
import asyncio
from pathlib import Path
from unittest.mock import patch, MagicMock, AsyncMock

from app.core.scraper_service import ScraperService
from bs4 import BeautifulSoup


@pytest.fixture
def temp_dir():
    # Create a temporary directory for testing
    temp_dir = Path("./test_scraper_data")
    temp_dir.mkdir(exist_ok=True)
    yield temp_dir
    # Cleanup
    shutil.rmtree(temp_dir)


@pytest.fixture
def scraper_service(temp_dir):
    with patch("app.core.scraper_service.get_config") as mock_config:
        mock_config.return_value.scraper_min_delay = 0.1
        mock_config.return_value.scraper_max_delay = 0.2
        mock_config.return_value.scraper_data_path = str(temp_dir)

        service = ScraperService()
        yield service


class TestScraperService:
    @pytest.mark.asyncio
    @patch("app.core.scraper_service.aiohttp.ClientSession")
    async def test_scrape_url(self, mock_session, scraper_service):
        # Mock the aiohttp session
        mock_session_instance = AsyncMock()
        mock_session.return_value.__aenter__.return_value = mock_session_instance

        # Mock response
        mock_response = AsyncMock()
        mock_response.status = 200
        mock_response.text = AsyncMock(
            return_value="""
        <html>
            <head>
                <title>Test Page</title>
                <meta name="description" content="Test description">
            </head>
            <body>
                <h1>Test Heading</h1>
                <p>Test paragraph content.</p>
            </body>
        </html>
        """
        )
        mock_session_instance.get.return_value.__aenter__.return_value = mock_response

        # Test scraping a URL
        result = await scraper_service.scrape_url("https://example.com")

        # Verify successful scrape
        assert result["success"] is True
        assert result["url"] == "https://example.com"
        assert result["title"] == "Test Page"
        assert "Test Heading" in result["content"]
        assert "Test paragraph" in result["content"]
        assert result["metadata"]["description"] == "Test description"

    @pytest.mark.asyncio
    @patch("app.core.scraper_service.aiohttp.ClientSession")
    async def test_scrape_url_error(self, mock_session, scraper_service):
        # Mock a failed response
        mock_session_instance = AsyncMock()
        mock_session.return_value.__aenter__.return_value = mock_session_instance

        # Connection error
        mock_session_instance.get.side_effect = Exception("Connection error")

        # Test scraping with error
        result = await scraper_service.scrape_url("https://nonexistent.example.com")

        # Verify failed scrape
        assert result["success"] is False
        assert "error" in result
        assert "Connection error" in result["error"]

    @pytest.mark.asyncio
    async def test_extract_metadata(self, scraper_service):
        # Create sample HTML
        html = """
        <html>
            <head>
                <title>Metadata Test</title>
                <meta name="description" content="Meta description">
                <meta name="keywords" content="test, metadata, extraction">
                <meta property="og:title" content="OG Title">
                <meta property="og:description" content="OG Description">
                <script type="application/ld+json">
                    {"@context": "https://schema.org", "@type": "Article", "name": "Test Article"}
                </script>
            </head>
            <body>
                <h1>Test Content</h1>
            </body>
        </html>
        """

        # Extract metadata
        soup = BeautifulSoup(html, "html.parser")
        metadata = scraper_service._extract_metadata(html, "https://example.com/metadata")

        # Verify metadata extraction
        assert metadata["title"] == "Metadata Test"
        assert metadata["description"] == "Meta description"
        assert "og:title" in metadata
        assert metadata["og:title"] == "OG Title"
        assert len(metadata["structured_data"]) == 1
        assert metadata["structured_data"][0]["@type"] == "Article"

    @pytest.mark.asyncio
    @patch("app.core.scraper_service.aiohttp.ClientSession")
    async def test_scrape_with_pagination(self, mock_session, scraper_service):
        # Mock session
        mock_session_instance = AsyncMock()
        mock_session.return_value.__aenter__.return_value = mock_session_instance

        # First page with next link
        first_page = """
        <html>
            <head><title>Page 1</title></head>
            <body>
                <h1>First Page</h1>
                <p>Content on first page</p>
                <a href="https://example.com/page/2" rel="next">Next Page</a>
            </body>
        </html>
        """

        # Second page with no next link
        second_page = """
        <html>
            <head><title>Page 2</title></head>
            <body>
                <h1>Second Page</h1>
                <p>Content on second page</p>
            </body>
        </html>
        """

        # Set up mock responses for pagination
        mock_responses = [
            # First page response
            AsyncMock(status=200, text=AsyncMock(return_value=first_page)),
            # Second page response
            AsyncMock(status=200, text=AsyncMock(return_value=second_page)),
        ]

        mock_session_instance.get.return_value.__aenter__.side_effect = mock_responses

        # Test paginated scraping
        result = await scraper_service.scrape_with_pagination("https://example.com")

        # Verify results
        assert result["success"] is True
        assert result["pages_scraped"] == 2
        assert "First Page" in result["content"]
        assert "Second Page" in result["content"]

    @pytest.mark.asyncio
    @patch("playwright.async_api.async_playwright")
    async def test_capture_screenshot(self, mock_playwright, scraper_service, temp_dir):
        # Mock Playwright
        mock_playwright_instance = AsyncMock()
        mock_playwright.return_value.__aenter__.return_value = mock_playwright_instance

        # Mock browser, context, and page
        mock_browser = AsyncMock()
        mock_context = AsyncMock()
        mock_page = AsyncMock()

        mock_playwright_instance.chromium.launch = AsyncMock(return_value=mock_browser)
        mock_browser.new_context = AsyncMock(return_value=mock_context)
        mock_context.new_page = AsyncMock(return_value=mock_page)

        # Mock screenshot
        mock_page.screenshot = AsyncMock(return_value=b"fake screenshot data")

        # Test capturing screenshot
        result = await scraper_service.capture_screenshot("https://example.com")

        # Verify screenshot capture
        assert result["success"] is True
        assert "screenshot_path" in result
        assert mock_page.goto.called
        assert mock_page.screenshot.called

    @pytest.mark.asyncio
    @patch("app.core.scraper_service.aiohttp.ClientSession")
    async def test_scrape_sitemap(self, mock_session, scraper_service):
        # Mock session
        mock_session_instance = AsyncMock()
        mock_session.return_value.__aenter__.return_value = mock_session_instance

        # Mock sitemap response
        sitemap_xml = """
        <?xml version="1.0" encoding="UTF-8"?>
        <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
            <url>
                <loc>https://example.com/page1</loc>
                <lastmod>2023-01-01</lastmod>
            </url>
            <url>
                <loc>https://example.com/page2</loc>
                <lastmod>2023-01-02</lastmod>
            </url>
        </urlset>
        """

        # Mock page responses
        page_html = """
        <html>
            <head><title>Test Page</title></head>
            <body><p>Test content</p></body>
        </html>
        """

        # Set up mock responses
        sitemap_response = AsyncMock(status=200, text=AsyncMock(return_value=sitemap_xml))
        page_response = AsyncMock(status=200, text=AsyncMock(return_value=page_html))

        # Return different responses for different URLs
        def get_side_effect(url, **kwargs):
            if url.endswith(".xml"):
                return sitemap_response
            else:
                return page_response

        mock_session_instance.get.side_effect = get_side_effect

        # Test sitemap scraping
        result = await scraper_service.scrape_sitemap("https://example.com/sitemap.xml", max_urls=2)

        # Verify results
        assert result["success"] is True
        assert len(result["results"]) == 2
        assert result["results"][0]["url"] == "https://example.com/page1"
        assert result["results"][1]["url"] == "https://example.com/page2"


================================================================================
FILE: tests\test_memory_service.py
LANGUAGE: python
SIZE: 821 bytes
================================================================================

import pytest
import os
import shutil
from pathlib import Path
from app.core.memory_service import MemoryService


@pytest.fixture
def memory_service():
    # Set up a test memory file
    test_dir = Path("./test_data")
    test_dir.mkdir(exist_ok=True)
    memory_file = test_dir / "test_memory.json"

    service = MemoryService(str(memory_file))
    yield service

    # Clean up
    shutil.rmtree(test_dir)


def test_user_preferences(memory_service):
    # Test setting preferences
    user_id = "test_user"
    prefs = {"theme": "dark", "language": "en"}

    # Set preferences
    result = memory_service.set_user_preference(user_id, prefs)
    assert result == prefs

    # Get preferences
    retrieved = memory_service.get_user_preference(user_id)
    assert retrieved == prefs


